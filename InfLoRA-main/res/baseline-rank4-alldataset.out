nohup: ignoring input
logs/cifar100/20_20_sip/InfLoRA/adam/4/0.95_1.0-0.0005/1993
2024-08-11 22:16:23,124 [trainer.py] => config: ./configs/cifar100_inflora.json
2024-08-11 22:16:23,124 [trainer.py] => device: [device(type='cuda', index=0)]
2024-08-11 22:16:23,124 [trainer.py] => prefix: reproduce
2024-08-11 22:16:23,124 [trainer.py] => dataset: cifar100
2024-08-11 22:16:23,124 [trainer.py] => data_path: /mnt/mydisk/ruoheng.li/lrh/Dataset
2024-08-11 22:16:23,124 [trainer.py] => memory_size: 0
2024-08-11 22:16:23,124 [trainer.py] => memory_per_class: 0
2024-08-11 22:16:23,124 [trainer.py] => fixed_memory: True
2024-08-11 22:16:23,124 [trainer.py] => shuffle: True
2024-08-11 22:16:23,124 [trainer.py] => init_cls: 20
2024-08-11 22:16:23,124 [trainer.py] => increment: 20
2024-08-11 22:16:23,125 [trainer.py] => model_name: InfLoRA
2024-08-11 22:16:23,125 [trainer.py] => net_type: sip
2024-08-11 22:16:23,125 [trainer.py] => embd_dim: 768
2024-08-11 22:16:23,125 [trainer.py] => num_heads: 12
2024-08-11 22:16:23,125 [trainer.py] => total_sessions: 5
2024-08-11 22:16:23,125 [trainer.py] => seed: 1993
2024-08-11 22:16:23,125 [trainer.py] => EPSILON: 1e-08
2024-08-11 22:16:23,125 [trainer.py] => init_epoch: 20
2024-08-11 22:16:23,125 [trainer.py] => optim: adam
2024-08-11 22:16:23,125 [trainer.py] => init_lr: 0.0005
2024-08-11 22:16:23,125 [trainer.py] => init_lr_decay: 0.1
2024-08-11 22:16:23,125 [trainer.py] => init_weight_decay: 0.0
2024-08-11 22:16:23,125 [trainer.py] => epochs: 20
2024-08-11 22:16:23,125 [trainer.py] => lrate: 0.0005
2024-08-11 22:16:23,125 [trainer.py] => lrate_decay: 0.1
2024-08-11 22:16:23,125 [trainer.py] => batch_size: 48
2024-08-11 22:16:23,125 [trainer.py] => weight_decay: 0.0
2024-08-11 22:16:23,125 [trainer.py] => rank: 4
2024-08-11 22:16:23,125 [trainer.py] => lamb: 0.95
2024-08-11 22:16:23,125 [trainer.py] => lame: 1.0
2024-08-11 22:16:23,125 [trainer.py] => num_workers: 16
Files already downloaded and verified
Files already downloaded and verified
2024-08-11 22:16:24,579 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2024-08-11 22:16:26,764 [trainer.py] => All params: 108245531
2024-08-11 22:16:26,765 [trainer.py] => Trainable params: 108245531
2024-08-11 22:16:26,765 [inflora.py] => Learning on 0-20
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.0.weight', 'image_encoder.blocks.11.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_v.0.weight', 'image_encoder.blocks.1.attn.lora_B_v.0.weight', 'image_encoder.blocks.2.attn.lora_B_k.0.weight', 'image_encoder.blocks.10.attn.lora_B_v.0.weight', 'image_encoder.blocks.4.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_k.0.weight', 'image_encoder.blocks.7.attn.lora_B_v.0.weight', 'image_encoder.blocks.4.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_k.0.weight', 'image_encoder.blocks.6.attn.lora_B_k.0.weight', 'image_encoder.blocks.5.attn.lora_B_k.0.weight', 'image_encoder.blocks.10.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_v.0.weight', 'image_encoder.blocks.7.attn.lora_B_k.0.weight', 'image_encoder.blocks.5.attn.lora_B_v.0.weight', 'image_encoder.blocks.8.attn.lora_B_v.0.weight', 'classifier_pool.0.bias', 'image_encoder.blocks.3.attn.lora_B_k.0.weight', 'classifier_pool.0.weight', 'image_encoder.blocks.8.attn.lora_B_k.0.weight', 'image_encoder.blocks.6.attn.lora_B_v.0.weight', 'image_encoder.blocks.2.attn.lora_B_v.0.weight', 'image_encoder.blocks.11.attn.lora_B_k.0.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 0, Epoch 1/20 => Loss 0.745, Train_accy 79.09:   0%|          | 0/20 [00:34<?, ?it/s]
Task 0, Epoch 1/20 => Loss 0.745, Train_accy 79.09:   5%|▌         | 1/20 [00:34<10:47, 34.10s/it]
Task 0, Epoch 2/20 => Loss 0.352, Train_accy 88.99:   5%|▌         | 1/20 [01:08<10:47, 34.10s/it]
Task 0, Epoch 2/20 => Loss 0.352, Train_accy 88.99:  10%|█         | 2/20 [01:08<10:14, 34.12s/it]
Task 0, Epoch 3/20 => Loss 0.342, Train_accy 89.61:  10%|█         | 2/20 [01:42<10:14, 34.12s/it]
Task 0, Epoch 3/20 => Loss 0.342, Train_accy 89.61:  15%|█▌        | 3/20 [01:42<09:41, 34.18s/it]
Task 0, Epoch 4/20 => Loss 0.325, Train_accy 90.23:  15%|█▌        | 3/20 [02:16<09:41, 34.18s/it]
Task 0, Epoch 4/20 => Loss 0.325, Train_accy 90.23:  20%|██        | 4/20 [02:16<09:07, 34.20s/it]
Task 0, Epoch 5/20 => Loss 0.287, Train_accy 91.45:  20%|██        | 4/20 [02:51<09:07, 34.20s/it]
Task 0, Epoch 5/20 => Loss 0.287, Train_accy 91.45:  25%|██▌       | 5/20 [02:51<08:33, 34.26s/it]
Task 0, Epoch 6/20 => Loss 0.284, Train_accy 91.49:  25%|██▌       | 5/20 [03:25<08:33, 34.26s/it]
Task 0, Epoch 6/20 => Loss 0.284, Train_accy 91.49:  30%|███       | 6/20 [03:25<07:59, 34.26s/it]
Task 0, Epoch 7/20 => Loss 0.281, Train_accy 91.26:  30%|███       | 6/20 [03:59<07:59, 34.26s/it]
Task 0, Epoch 7/20 => Loss 0.281, Train_accy 91.26:  35%|███▌      | 7/20 [03:59<07:25, 34.27s/it]
Task 0, Epoch 8/20 => Loss 0.271, Train_accy 91.73:  35%|███▌      | 7/20 [04:33<07:25, 34.27s/it]
Task 0, Epoch 8/20 => Loss 0.271, Train_accy 91.73:  40%|████      | 8/20 [04:33<06:51, 34.25s/it]
Task 0, Epoch 9/20 => Loss 0.261, Train_accy 91.87:  40%|████      | 8/20 [05:08<06:51, 34.25s/it]
Task 0, Epoch 9/20 => Loss 0.261, Train_accy 91.87:  45%|████▌     | 9/20 [05:08<06:16, 34.24s/it]
Task 0, Epoch 10/20 => Loss 0.249, Train_accy 92.40:  45%|████▌     | 9/20 [05:42<06:16, 34.24s/it]
Task 0, Epoch 10/20 => Loss 0.249, Train_accy 92.40:  50%|█████     | 10/20 [05:42<05:42, 34.25s/it]
Task 0, Epoch 11/20 => Loss 0.238, Train_accy 92.59:  50%|█████     | 10/20 [06:16<05:42, 34.25s/it]
Task 0, Epoch 11/20 => Loss 0.238, Train_accy 92.59:  55%|█████▌    | 11/20 [06:16<05:08, 34.24s/it]
Task 0, Epoch 12/20 => Loss 0.237, Train_accy 92.72:  55%|█████▌    | 11/20 [06:50<05:08, 34.24s/it]
Task 0, Epoch 12/20 => Loss 0.237, Train_accy 92.72:  60%|██████    | 12/20 [06:50<04:33, 34.25s/it]
Task 0, Epoch 13/20 => Loss 0.230, Train_accy 92.91:  60%|██████    | 12/20 [07:25<04:33, 34.25s/it]
Task 0, Epoch 13/20 => Loss 0.230, Train_accy 92.91:  65%|██████▌   | 13/20 [07:25<03:59, 34.27s/it]
Task 0, Epoch 14/20 => Loss 0.218, Train_accy 93.24:  65%|██████▌   | 13/20 [07:59<03:59, 34.27s/it]
Task 0, Epoch 14/20 => Loss 0.218, Train_accy 93.24:  70%|███████   | 14/20 [07:59<03:25, 34.25s/it]
Task 0, Epoch 15/20 => Loss 0.206, Train_accy 93.63:  70%|███████   | 14/20 [08:33<03:25, 34.25s/it]
Task 0, Epoch 15/20 => Loss 0.206, Train_accy 93.63:  75%|███████▌  | 15/20 [08:33<02:51, 34.25s/it]
Task 0, Epoch 16/20 => Loss 0.218, Train_accy 93.38:  75%|███████▌  | 15/20 [09:07<02:51, 34.25s/it]
Task 0, Epoch 16/20 => Loss 0.218, Train_accy 93.38:  80%|████████  | 16/20 [09:07<02:16, 34.24s/it]
Task 0, Epoch 17/20 => Loss 0.204, Train_accy 93.71:  80%|████████  | 16/20 [09:42<02:16, 34.24s/it]
Task 0, Epoch 17/20 => Loss 0.204, Train_accy 93.71:  85%|████████▌ | 17/20 [09:42<01:42, 34.23s/it]
Task 0, Epoch 18/20 => Loss 0.201, Train_accy 93.52:  85%|████████▌ | 17/20 [10:16<01:42, 34.23s/it]
Task 0, Epoch 18/20 => Loss 0.201, Train_accy 93.52:  90%|█████████ | 18/20 [10:16<01:08, 34.23s/it]
Task 0, Epoch 19/20 => Loss 0.218, Train_accy 93.18:  90%|█████████ | 18/20 [10:50<01:08, 34.23s/it]
Task 0, Epoch 19/20 => Loss 0.218, Train_accy 93.18:  95%|█████████▌| 19/20 [10:50<00:34, 34.23s/it]
Task 0, Epoch 20/20 => Loss 0.208, Train_accy 93.64:  95%|█████████▌| 19/20 [11:24<00:34, 34.23s/it]
Task 0, Epoch 20/20 => Loss 0.208, Train_accy 93.64: 100%|██████████| 20/20 [11:24<00:00, 34.23s/it]
Task 0, Epoch 20/20 => Loss 0.208, Train_accy 93.64: 100%|██████████| 20/20 [11:24<00:00, 34.24s/it]
2024-08-11 22:28:19,221 [inflora.py] => Task 0, Epoch 20/20 => Loss 0.208, Train_accy 93.64
Threshold:  0.95
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 6/768 type remove
Layer 2 : 9/768 type remove
Layer 3 : 10/768 type remove
Layer 4 : 10/768 type remove
Layer 5 : 14/768 type remove
Layer 6 : 17/768 type remove
Layer 7 : 17/768 type remove
Layer 8 : 22/768 type remove
Layer 9 : 29/768 type remove
Layer 10 : 30/768 type remove
Layer 11 : 8/768 type remove
Layer 12 : 24/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 22:29:02,668 [trainer.py] => Time:755.9025950431824
2000 2000
2000 2000
2024-08-11 22:29:06,670 [trainer.py] => Time:4.00147008895874
2024-08-11 22:29:06,670 [inflora.py] => Exemplar size: 0
2024-08-11 22:29:06,670 [trainer.py] => CNN: {'total': 98.15, '00-19': 98.15, 'old': 0, 'new': 98.15}
2024-08-11 22:29:06,670 [trainer.py] => CNN top1 curve: [98.15]
2024-08-11 22:29:06,670 [trainer.py] => CNN top1 with task curve: [98.15]
2024-08-11 22:29:06,670 [trainer.py] => CNN top1 task curve: [1.0]
2024-08-11 22:29:07,105 [trainer.py] => All params: 108245531
2024-08-11 22:29:07,106 [trainer.py] => Trainable params: 89108
2024-08-11 22:29:07,106 [inflora.py] => Learning on 20-40
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.1.weight', 'classifier_pool.1.weight', 'image_encoder.blocks.8.attn.lora_B_v.1.weight', 'image_encoder.blocks.3.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.1.weight', 'image_encoder.blocks.0.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_v.1.weight', 'classifier_pool.1.bias', 'image_encoder.blocks.2.attn.lora_B_v.1.weight', 'image_encoder.blocks.4.attn.lora_B_v.1.weight', 'image_encoder.blocks.5.attn.lora_B_v.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.1.weight', 'image_encoder.blocks.1.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_k.1.weight', 'image_encoder.blocks.11.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.1.weight', 'image_encoder.blocks.11.attn.lora_B_k.1.weight', 'image_encoder.blocks.3.attn.lora_B_k.1.weight', 'image_encoder.blocks.2.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_v.1.weight', 'image_encoder.blocks.8.attn.lora_B_k.1.weight', 'image_encoder.blocks.9.attn.lora_B_k.1.weight', 'image_encoder.blocks.0.attn.lora_B_v.1.weight', 'image_encoder.blocks.9.attn.lora_B_v.1.weight', 'image_encoder.blocks.5.attn.lora_B_k.1.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 1, Epoch 1/20 => Loss 0.548, Train_accy 84.31:   0%|          | 0/20 [00:34<?, ?it/s]
Task 1, Epoch 1/20 => Loss 0.548, Train_accy 84.31:   5%|▌         | 1/20 [00:34<10:54, 34.44s/it]
Task 1, Epoch 2/20 => Loss 0.263, Train_accy 91.69:   5%|▌         | 1/20 [01:08<10:54, 34.44s/it]
Task 1, Epoch 2/20 => Loss 0.263, Train_accy 91.69:  10%|█         | 2/20 [01:08<10:20, 34.46s/it]
Task 1, Epoch 3/20 => Loss 0.237, Train_accy 92.27:  10%|█         | 2/20 [01:43<10:20, 34.46s/it]
Task 1, Epoch 3/20 => Loss 0.237, Train_accy 92.27:  15%|█▌        | 3/20 [01:43<09:45, 34.45s/it]
Task 1, Epoch 4/20 => Loss 0.239, Train_accy 92.59:  15%|█▌        | 3/20 [02:17<09:45, 34.45s/it]
Task 1, Epoch 4/20 => Loss 0.239, Train_accy 92.59:  20%|██        | 4/20 [02:17<09:11, 34.45s/it]
Task 1, Epoch 5/20 => Loss 0.214, Train_accy 92.95:  20%|██        | 4/20 [02:52<09:11, 34.45s/it]
Task 1, Epoch 5/20 => Loss 0.214, Train_accy 92.95:  25%|██▌       | 5/20 [02:52<08:36, 34.45s/it]
Task 1, Epoch 6/20 => Loss 0.206, Train_accy 93.56:  25%|██▌       | 5/20 [03:26<08:36, 34.45s/it]
Task 1, Epoch 6/20 => Loss 0.206, Train_accy 93.56:  30%|███       | 6/20 [03:26<08:02, 34.46s/it]
Task 1, Epoch 7/20 => Loss 0.201, Train_accy 93.63:  30%|███       | 6/20 [04:01<08:02, 34.46s/it]
Task 1, Epoch 7/20 => Loss 0.201, Train_accy 93.63:  35%|███▌      | 7/20 [04:01<07:27, 34.45s/it]
Task 1, Epoch 8/20 => Loss 0.195, Train_accy 93.92:  35%|███▌      | 7/20 [04:35<07:27, 34.45s/it]
Task 1, Epoch 8/20 => Loss 0.195, Train_accy 93.92:  40%|████      | 8/20 [04:35<06:53, 34.45s/it]
Task 1, Epoch 9/20 => Loss 0.205, Train_accy 93.39:  40%|████      | 8/20 [05:10<06:53, 34.45s/it]
Task 1, Epoch 9/20 => Loss 0.205, Train_accy 93.39:  45%|████▌     | 9/20 [05:10<06:18, 34.44s/it]
Task 1, Epoch 10/20 => Loss 0.190, Train_accy 93.74:  45%|████▌     | 9/20 [05:44<06:18, 34.44s/it]
Task 1, Epoch 10/20 => Loss 0.190, Train_accy 93.74:  50%|█████     | 10/20 [05:44<05:44, 34.45s/it]
Task 1, Epoch 11/20 => Loss 0.178, Train_accy 94.45:  50%|█████     | 10/20 [06:18<05:44, 34.45s/it]
Task 1, Epoch 11/20 => Loss 0.178, Train_accy 94.45:  55%|█████▌    | 11/20 [06:18<05:09, 34.44s/it]
Task 1, Epoch 12/20 => Loss 0.175, Train_accy 94.43:  55%|█████▌    | 11/20 [06:53<05:09, 34.44s/it]
Task 1, Epoch 12/20 => Loss 0.175, Train_accy 94.43:  60%|██████    | 12/20 [06:53<04:35, 34.44s/it]
Task 1, Epoch 13/20 => Loss 0.180, Train_accy 94.29:  60%|██████    | 12/20 [07:27<04:35, 34.44s/it]
Task 1, Epoch 13/20 => Loss 0.180, Train_accy 94.29:  65%|██████▌   | 13/20 [07:27<04:01, 34.48s/it]
Task 1, Epoch 14/20 => Loss 0.168, Train_accy 94.44:  65%|██████▌   | 13/20 [08:02<04:01, 34.48s/it]
Task 1, Epoch 14/20 => Loss 0.168, Train_accy 94.44:  70%|███████   | 14/20 [08:02<03:26, 34.47s/it]
Task 1, Epoch 15/20 => Loss 0.169, Train_accy 94.77:  70%|███████   | 14/20 [08:36<03:26, 34.47s/it]
Task 1, Epoch 15/20 => Loss 0.169, Train_accy 94.77:  75%|███████▌  | 15/20 [08:36<02:52, 34.48s/it]
Task 1, Epoch 16/20 => Loss 0.175, Train_accy 94.30:  75%|███████▌  | 15/20 [09:11<02:52, 34.48s/it]
Task 1, Epoch 16/20 => Loss 0.175, Train_accy 94.30:  80%|████████  | 16/20 [09:11<02:18, 34.51s/it]
Task 1, Epoch 17/20 => Loss 0.171, Train_accy 94.46:  80%|████████  | 16/20 [09:45<02:18, 34.51s/it]
Task 1, Epoch 17/20 => Loss 0.171, Train_accy 94.46:  85%|████████▌ | 17/20 [09:45<01:43, 34.52s/it]
Task 1, Epoch 18/20 => Loss 0.155, Train_accy 94.99:  85%|████████▌ | 17/20 [10:20<01:43, 34.52s/it]
Task 1, Epoch 18/20 => Loss 0.155, Train_accy 94.99:  90%|█████████ | 18/20 [10:20<01:09, 34.52s/it]
Task 1, Epoch 19/20 => Loss 0.156, Train_accy 95.09:  90%|█████████ | 18/20 [10:54<01:09, 34.52s/it]
Task 1, Epoch 19/20 => Loss 0.156, Train_accy 95.09:  95%|█████████▌| 19/20 [10:54<00:34, 34.50s/it]
Task 1, Epoch 20/20 => Loss 0.151, Train_accy 95.07:  95%|█████████▌| 19/20 [11:29<00:34, 34.50s/it]
Task 1, Epoch 20/20 => Loss 0.151, Train_accy 95.07: 100%|██████████| 20/20 [11:29<00:00, 34.48s/it]
Task 1, Epoch 20/20 => Loss 0.151, Train_accy 95.07: 100%|██████████| 20/20 [11:29<00:00, 34.47s/it]
2024-08-11 22:41:04,149 [inflora.py] => Task 1, Epoch 20/20 => Loss 0.151, Train_accy 95.07
Threshold:  0.96
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 11/768 type remove
Layer 3 : 14/768 type remove
Layer 4 : 14/768 type remove
Layer 5 : 21/768 type remove
Layer 6 : 26/768 type remove
Layer 7 : 28/768 type remove
Layer 8 : 40/768 type remove
Layer 9 : 58/768 type remove
Layer 10 : 66/768 type remove
Layer 11 : 23/768 type remove
Layer 12 : 57/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 22:41:50,354 [trainer.py] => Time:763.2480003833771
4000 4000
4000 4000
2024-08-11 22:41:57,844 [trainer.py] => Time:7.489526987075806
2024-08-11 22:41:57,844 [inflora.py] => Exemplar size: 0
2024-08-11 22:41:57,844 [trainer.py] => CNN: {'total': 95.62, '00-19': 96.0, '20-39': 95.25, 'old': 96.0, 'new': 95.25}
2024-08-11 22:41:57,844 [trainer.py] => CNN top1 curve: [98.15, 95.62]
2024-08-11 22:41:57,845 [trainer.py] => CNN top1 with task curve: [98.15, 98.22]
2024-08-11 22:41:57,845 [trainer.py] => CNN top1 task curve: [1.0, 0.968]
2024-08-11 22:41:58,292 [trainer.py] => All params: 108245531
2024-08-11 22:41:58,293 [trainer.py] => Trainable params: 89108
2024-08-11 22:41:58,293 [inflora.py] => Learning on 40-60
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_k.2.weight', 'image_encoder.blocks.6.attn.lora_B_k.2.weight', 'image_encoder.blocks.5.attn.lora_B_v.2.weight', 'classifier_pool.2.weight', 'image_encoder.blocks.1.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_v.2.weight', 'image_encoder.blocks.11.attn.lora_B_v.2.weight', 'classifier_pool.2.bias', 'image_encoder.blocks.10.attn.lora_B_k.2.weight', 'image_encoder.blocks.6.attn.lora_B_v.2.weight', 'image_encoder.blocks.10.attn.lora_B_v.2.weight', 'image_encoder.blocks.7.attn.lora_B_k.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.2.weight', 'image_encoder.blocks.8.attn.lora_B_k.2.weight', 'image_encoder.blocks.2.attn.lora_B_k.2.weight', 'image_encoder.blocks.2.attn.lora_B_v.2.weight', 'image_encoder.blocks.7.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_k.2.weight', 'image_encoder.blocks.9.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.2.weight', 'image_encoder.blocks.1.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_k.2.weight', 'image_encoder.blocks.4.attn.lora_B_v.2.weight', 'image_encoder.blocks.11.attn.lora_B_k.2.weight', 'image_encoder.blocks.4.attn.lora_B_k.2.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 2, Epoch 1/20 => Loss 0.615, Train_accy 82.44:   0%|          | 0/20 [00:34<?, ?it/s]
Task 2, Epoch 1/20 => Loss 0.615, Train_accy 82.44:   5%|▌         | 1/20 [00:34<10:55, 34.52s/it]
Task 2, Epoch 2/20 => Loss 0.309, Train_accy 90.24:   5%|▌         | 1/20 [01:09<10:55, 34.52s/it]
Task 2, Epoch 2/20 => Loss 0.309, Train_accy 90.24:  10%|█         | 2/20 [01:09<10:21, 34.53s/it]
Task 2, Epoch 3/20 => Loss 0.261, Train_accy 91.65:  10%|█         | 2/20 [01:43<10:21, 34.53s/it]
Task 2, Epoch 3/20 => Loss 0.261, Train_accy 91.65:  15%|█▌        | 3/20 [01:43<09:47, 34.59s/it]
Task 2, Epoch 4/20 => Loss 0.252, Train_accy 91.78:  15%|█▌        | 3/20 [02:18<09:47, 34.59s/it]
Task 2, Epoch 4/20 => Loss 0.252, Train_accy 91.78:  20%|██        | 4/20 [02:18<09:13, 34.59s/it]
Task 2, Epoch 5/20 => Loss 0.243, Train_accy 92.28:  20%|██        | 4/20 [02:52<09:13, 34.59s/it]
Task 2, Epoch 5/20 => Loss 0.243, Train_accy 92.28:  25%|██▌       | 5/20 [02:52<08:38, 34.59s/it]
Task 2, Epoch 6/20 => Loss 0.225, Train_accy 92.94:  25%|██▌       | 5/20 [03:27<08:38, 34.59s/it]
Task 2, Epoch 6/20 => Loss 0.225, Train_accy 92.94:  30%|███       | 6/20 [03:27<08:04, 34.59s/it]
Task 2, Epoch 7/20 => Loss 0.224, Train_accy 92.68:  30%|███       | 6/20 [04:01<08:04, 34.59s/it]
Task 2, Epoch 7/20 => Loss 0.224, Train_accy 92.68:  35%|███▌      | 7/20 [04:01<07:29, 34.56s/it]
Task 2, Epoch 8/20 => Loss 0.213, Train_accy 93.20:  35%|███▌      | 7/20 [04:36<07:29, 34.56s/it]
Task 2, Epoch 8/20 => Loss 0.213, Train_accy 93.20:  40%|████      | 8/20 [04:36<06:54, 34.57s/it]
Task 2, Epoch 9/20 => Loss 0.221, Train_accy 93.02:  40%|████      | 8/20 [05:11<06:54, 34.57s/it]
Task 2, Epoch 9/20 => Loss 0.221, Train_accy 93.02:  45%|████▌     | 9/20 [05:11<06:20, 34.55s/it]
Task 2, Epoch 10/20 => Loss 0.194, Train_accy 93.67:  45%|████▌     | 9/20 [05:45<06:20, 34.55s/it]
Task 2, Epoch 10/20 => Loss 0.194, Train_accy 93.67:  50%|█████     | 10/20 [05:45<05:45, 34.57s/it]
Task 2, Epoch 11/20 => Loss 0.202, Train_accy 93.47:  50%|█████     | 10/20 [06:20<05:45, 34.57s/it]
Task 2, Epoch 11/20 => Loss 0.202, Train_accy 93.47:  55%|█████▌    | 11/20 [06:20<05:11, 34.58s/it]
Task 2, Epoch 12/20 => Loss 0.190, Train_accy 93.82:  55%|█████▌    | 11/20 [06:54<05:11, 34.58s/it]
Task 2, Epoch 12/20 => Loss 0.190, Train_accy 93.82:  60%|██████    | 12/20 [06:54<04:36, 34.57s/it]
Task 2, Epoch 13/20 => Loss 0.190, Train_accy 93.99:  60%|██████    | 12/20 [07:29<04:36, 34.57s/it]
Task 2, Epoch 13/20 => Loss 0.190, Train_accy 93.99:  65%|██████▌   | 13/20 [07:29<04:01, 34.54s/it]
Task 2, Epoch 14/20 => Loss 0.191, Train_accy 93.98:  65%|██████▌   | 13/20 [08:04<04:01, 34.54s/it]
Task 2, Epoch 14/20 => Loss 0.191, Train_accy 93.98:  70%|███████   | 14/20 [08:04<03:27, 34.58s/it]
Task 2, Epoch 15/20 => Loss 0.182, Train_accy 94.12:  70%|███████   | 14/20 [08:38<03:27, 34.58s/it]
Task 2, Epoch 15/20 => Loss 0.182, Train_accy 94.12:  75%|███████▌  | 15/20 [08:38<02:52, 34.60s/it]
Task 2, Epoch 16/20 => Loss 0.182, Train_accy 94.12:  75%|███████▌  | 15/20 [09:13<02:52, 34.60s/it]
Task 2, Epoch 16/20 => Loss 0.182, Train_accy 94.12:  80%|████████  | 16/20 [09:13<02:18, 34.59s/it]
Task 2, Epoch 17/20 => Loss 0.182, Train_accy 94.21:  80%|████████  | 16/20 [09:47<02:18, 34.59s/it]
Task 2, Epoch 17/20 => Loss 0.182, Train_accy 94.21:  85%|████████▌ | 17/20 [09:47<01:43, 34.60s/it]
Task 2, Epoch 18/20 => Loss 0.185, Train_accy 94.25:  85%|████████▌ | 17/20 [10:22<01:43, 34.60s/it]
Task 2, Epoch 18/20 => Loss 0.185, Train_accy 94.25:  90%|█████████ | 18/20 [10:22<01:09, 34.61s/it]
Task 2, Epoch 19/20 => Loss 0.177, Train_accy 94.55:  90%|█████████ | 18/20 [10:57<01:09, 34.61s/it]
Task 2, Epoch 19/20 => Loss 0.177, Train_accy 94.55:  95%|█████████▌| 19/20 [10:57<00:34, 34.60s/it]
Task 2, Epoch 20/20 => Loss 0.178, Train_accy 94.30:  95%|█████████▌| 19/20 [11:31<00:34, 34.60s/it]
Task 2, Epoch 20/20 => Loss 0.178, Train_accy 94.30: 100%|██████████| 20/20 [11:31<00:00, 34.58s/it]
Task 2, Epoch 20/20 => Loss 0.178, Train_accy 94.30: 100%|██████████| 20/20 [11:31<00:00, 34.58s/it]
2024-08-11 22:53:58,717 [inflora.py] => Task 2, Epoch 20/20 => Loss 0.178, Train_accy 94.30
Threshold:  0.97
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 16/768 type remove
Layer 4 : 18/768 type remove
Layer 5 : 26/768 type remove
Layer 6 : 35/768 type remove
Layer 7 : 40/768 type remove
Layer 8 : 58/768 type remove
Layer 9 : 87/768 type remove
Layer 10 : 105/768 type remove
Layer 11 : 38/768 type remove
Layer 12 : 94/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 22:54:44,693 [trainer.py] => Time:766.4004616737366
6000 6000
6000 6000
2024-08-11 22:54:55,624 [trainer.py] => Time:10.929893732070923
2024-08-11 22:54:55,624 [inflora.py] => Exemplar size: 0
2024-08-11 22:54:55,624 [trainer.py] => CNN: {'total': 93.0, '00-19': 94.1, '20-39': 92.95, '40-59': 91.95, 'old': 93.52, 'new': 91.95}
2024-08-11 22:54:55,624 [trainer.py] => CNN top1 curve: [98.15, 95.62, 93.0]
2024-08-11 22:54:55,624 [trainer.py] => CNN top1 with task curve: [98.15, 98.22, 97.73]
2024-08-11 22:54:55,624 [trainer.py] => CNN top1 task curve: [1.0, 0.968, 0.9446666666666667]
2024-08-11 22:54:56,064 [trainer.py] => All params: 108245531
2024-08-11 22:54:56,065 [trainer.py] => Trainable params: 89108
2024-08-11 22:54:56,065 [inflora.py] => Learning on 60-80
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.3.weight', 'image_encoder.blocks.3.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_v.3.weight', 'image_encoder.blocks.1.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_k.3.weight', 'image_encoder.blocks.10.attn.lora_B_v.3.weight', 'image_encoder.blocks.11.attn.lora_B_v.3.weight', 'image_encoder.blocks.2.attn.lora_B_v.3.weight', 'image_encoder.blocks.3.attn.lora_B_k.3.weight', 'image_encoder.blocks.4.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.3.weight', 'image_encoder.blocks.1.attn.lora_B_k.3.weight', 'image_encoder.blocks.7.attn.lora_B_v.3.weight', 'image_encoder.blocks.7.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.3.weight', 'image_encoder.blocks.5.attn.lora_B_v.3.weight', 'image_encoder.blocks.5.attn.lora_B_k.3.weight', 'image_encoder.blocks.6.attn.lora_B_v.3.weight', 'image_encoder.blocks.0.attn.lora_B_v.3.weight', 'classifier_pool.3.weight', 'image_encoder.blocks.6.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_k.3.weight', 'image_encoder.blocks.10.attn.lora_B_k.3.weight', 'image_encoder.blocks.2.attn.lora_B_k.3.weight', 'classifier_pool.3.bias'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 3, Epoch 1/20 => Loss 0.637, Train_accy 81.36:   0%|          | 0/20 [00:34<?, ?it/s]
Task 3, Epoch 1/20 => Loss 0.637, Train_accy 81.36:   5%|▌         | 1/20 [00:34<10:57, 34.62s/it]
Task 3, Epoch 2/20 => Loss 0.295, Train_accy 90.49:   5%|▌         | 1/20 [01:09<10:57, 34.62s/it]
Task 3, Epoch 2/20 => Loss 0.295, Train_accy 90.49:  10%|█         | 2/20 [01:09<10:22, 34.61s/it]
Task 3, Epoch 3/20 => Loss 0.260, Train_accy 91.62:  10%|█         | 2/20 [01:43<10:22, 34.61s/it]
Task 3, Epoch 3/20 => Loss 0.260, Train_accy 91.62:  15%|█▌        | 3/20 [01:43<09:48, 34.61s/it]
Task 3, Epoch 4/20 => Loss 0.260, Train_accy 91.43:  15%|█▌        | 3/20 [02:18<09:48, 34.61s/it]
Task 3, Epoch 4/20 => Loss 0.260, Train_accy 91.43:  20%|██        | 4/20 [02:18<09:14, 34.63s/it]
Task 3, Epoch 5/20 => Loss 0.222, Train_accy 92.68:  20%|██        | 4/20 [02:53<09:14, 34.63s/it]
Task 3, Epoch 5/20 => Loss 0.222, Train_accy 92.68:  25%|██▌       | 5/20 [02:53<08:39, 34.64s/it]
Task 3, Epoch 6/20 => Loss 0.225, Train_accy 92.62:  25%|██▌       | 5/20 [03:27<08:39, 34.64s/it]
Task 3, Epoch 6/20 => Loss 0.225, Train_accy 92.62:  30%|███       | 6/20 [03:27<08:04, 34.63s/it]
Task 3, Epoch 7/20 => Loss 0.231, Train_accy 92.87:  30%|███       | 6/20 [04:02<08:04, 34.63s/it]
Task 3, Epoch 7/20 => Loss 0.231, Train_accy 92.87:  35%|███▌      | 7/20 [04:02<07:30, 34.65s/it]
Task 3, Epoch 8/20 => Loss 0.218, Train_accy 92.80:  35%|███▌      | 7/20 [04:37<07:30, 34.65s/it]
Task 3, Epoch 8/20 => Loss 0.218, Train_accy 92.80:  40%|████      | 8/20 [04:37<06:55, 34.63s/it]
Task 3, Epoch 9/20 => Loss 0.217, Train_accy 92.99:  40%|████      | 8/20 [05:11<06:55, 34.63s/it]
Task 3, Epoch 9/20 => Loss 0.217, Train_accy 92.99:  45%|████▌     | 9/20 [05:11<06:20, 34.63s/it]
Task 3, Epoch 10/20 => Loss 0.207, Train_accy 93.38:  45%|████▌     | 9/20 [05:46<06:20, 34.63s/it]
Task 3, Epoch 10/20 => Loss 0.207, Train_accy 93.38:  50%|█████     | 10/20 [05:46<05:46, 34.65s/it]
Task 3, Epoch 11/20 => Loss 0.190, Train_accy 93.97:  50%|█████     | 10/20 [06:20<05:46, 34.65s/it]
Task 3, Epoch 11/20 => Loss 0.190, Train_accy 93.97:  55%|█████▌    | 11/20 [06:20<05:11, 34.63s/it]
Task 3, Epoch 12/20 => Loss 0.200, Train_accy 93.79:  55%|█████▌    | 11/20 [06:55<05:11, 34.63s/it]
Task 3, Epoch 12/20 => Loss 0.200, Train_accy 93.79:  60%|██████    | 12/20 [06:55<04:37, 34.64s/it]
Task 3, Epoch 13/20 => Loss 0.189, Train_accy 93.97:  60%|██████    | 12/20 [07:30<04:37, 34.64s/it]
Task 3, Epoch 13/20 => Loss 0.189, Train_accy 93.97:  65%|██████▌   | 13/20 [07:30<04:02, 34.64s/it]
Task 3, Epoch 14/20 => Loss 0.187, Train_accy 93.76:  65%|██████▌   | 13/20 [08:04<04:02, 34.64s/it]
Task 3, Epoch 14/20 => Loss 0.187, Train_accy 93.76:  70%|███████   | 14/20 [08:04<03:27, 34.64s/it]
Task 3, Epoch 15/20 => Loss 0.181, Train_accy 94.27:  70%|███████   | 14/20 [08:39<03:27, 34.64s/it]
Task 3, Epoch 15/20 => Loss 0.181, Train_accy 94.27:  75%|███████▌  | 15/20 [08:39<02:53, 34.66s/it]
Task 3, Epoch 16/20 => Loss 0.189, Train_accy 93.84:  75%|███████▌  | 15/20 [09:14<02:53, 34.66s/it]
Task 3, Epoch 16/20 => Loss 0.189, Train_accy 93.84:  80%|████████  | 16/20 [09:14<02:18, 34.64s/it]
Task 3, Epoch 17/20 => Loss 0.175, Train_accy 94.59:  80%|████████  | 16/20 [09:48<02:18, 34.64s/it]
Task 3, Epoch 17/20 => Loss 0.175, Train_accy 94.59:  85%|████████▌ | 17/20 [09:48<01:43, 34.65s/it]
Task 3, Epoch 18/20 => Loss 0.169, Train_accy 94.66:  85%|████████▌ | 17/20 [10:23<01:43, 34.65s/it]
Task 3, Epoch 18/20 => Loss 0.169, Train_accy 94.66:  90%|█████████ | 18/20 [10:23<01:09, 34.65s/it]
Task 3, Epoch 19/20 => Loss 0.168, Train_accy 94.52:  90%|█████████ | 18/20 [10:58<01:09, 34.65s/it]
Task 3, Epoch 19/20 => Loss 0.168, Train_accy 94.52:  95%|█████████▌| 19/20 [10:58<00:34, 34.65s/it]
Task 3, Epoch 20/20 => Loss 0.170, Train_accy 94.47:  95%|█████████▌| 19/20 [11:32<00:34, 34.65s/it]
Task 3, Epoch 20/20 => Loss 0.170, Train_accy 94.47: 100%|██████████| 20/20 [11:32<00:00, 34.63s/it]
Task 3, Epoch 20/20 => Loss 0.170, Train_accy 94.47: 100%|██████████| 20/20 [11:32<00:00, 34.64s/it]
2024-08-11 23:06:57,513 [inflora.py] => Task 3, Epoch 20/20 => Loss 0.170, Train_accy 94.47
Threshold:  0.98
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 19/768 type remove
Layer 4 : 24/768 type remove
Layer 5 : 35/768 type remove
Layer 6 : 52/768 type remove
Layer 7 : 62/768 type remove
Layer 8 : 94/768 type remove
Layer 9 : 148/768 type remove
Layer 10 : 183/768 type remove
Layer 11 : 88/768 type remove
Layer 12 : 180/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 23:07:43,469 [trainer.py] => Time:767.4041686058044
8000 8000
8000 8000
2024-08-11 23:07:57,619 [trainer.py] => Time:14.14960265159607
2024-08-11 23:07:57,620 [inflora.py] => Exemplar size: 0
2024-08-11 23:07:57,620 [trainer.py] => CNN: {'total': 90.62, '00-19': 93.1, '20-39': 92.55, '40-59': 88.1, '60-79': 88.75, 'old': 91.25, 'new': 88.75}
2024-08-11 23:07:57,620 [trainer.py] => CNN top1 curve: [98.15, 95.62, 93.0, 90.62]
2024-08-11 23:07:57,620 [trainer.py] => CNN top1 with task curve: [98.15, 98.22, 97.73, 97.38]
2024-08-11 23:07:57,620 [trainer.py] => CNN top1 task curve: [1.0, 0.968, 0.9446666666666667, 0.919875]
2024-08-11 23:07:58,074 [trainer.py] => All params: 108245531
2024-08-11 23:07:58,076 [trainer.py] => Trainable params: 89108
2024-08-11 23:07:58,076 [inflora.py] => Learning on 80-100
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_k.4.weight', 'image_encoder.blocks.6.attn.lora_B_v.4.weight', 'image_encoder.blocks.11.attn.lora_B_v.4.weight', 'classifier_pool.4.bias', 'image_encoder.blocks.1.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_v.4.weight', 'image_encoder.blocks.6.attn.lora_B_k.4.weight', 'image_encoder.blocks.4.attn.lora_B_k.4.weight', 'image_encoder.blocks.11.attn.lora_B_k.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_v.4.weight', 'image_encoder.blocks.5.attn.lora_B_v.4.weight', 'image_encoder.blocks.0.attn.lora_B_k.4.weight', 'image_encoder.blocks.2.attn.lora_B_v.4.weight', 'image_encoder.blocks.5.attn.lora_B_k.4.weight', 'image_encoder.blocks.4.attn.lora_B_v.4.weight', 'image_encoder.blocks.7.attn.lora_B_k.4.weight', 'classifier_pool.4.weight', 'image_encoder.blocks.1.attn.lora_B_k.4.weight', 'image_encoder.blocks.8.attn.lora_B_k.4.weight', 'image_encoder.blocks.0.attn.lora_B_v.4.weight', 'image_encoder.blocks.10.attn.lora_B_k.4.weight', 'image_encoder.blocks.3.attn.lora_B_k.4.weight', 'image_encoder.blocks.7.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.4.weight', 'image_encoder.blocks.8.attn.lora_B_v.4.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 4, Epoch 1/20 => Loss 0.605, Train_accy 83.07:   0%|          | 0/20 [00:34<?, ?it/s]
Task 4, Epoch 1/20 => Loss 0.605, Train_accy 83.07:   5%|▌         | 1/20 [00:34<10:57, 34.60s/it]
Task 4, Epoch 2/20 => Loss 0.294, Train_accy 91.37:   5%|▌         | 1/20 [01:09<10:57, 34.60s/it]
Task 4, Epoch 2/20 => Loss 0.294, Train_accy 91.37:  10%|█         | 2/20 [01:09<10:23, 34.65s/it]
Task 4, Epoch 3/20 => Loss 0.253, Train_accy 92.33:  10%|█         | 2/20 [01:43<10:23, 34.65s/it]
Task 4, Epoch 3/20 => Loss 0.253, Train_accy 92.33:  15%|█▌        | 3/20 [01:43<09:48, 34.62s/it]
Task 4, Epoch 4/20 => Loss 0.256, Train_accy 92.48:  15%|█▌        | 3/20 [02:18<09:48, 34.62s/it]
Task 4, Epoch 4/20 => Loss 0.256, Train_accy 92.48:  20%|██        | 4/20 [02:18<09:14, 34.65s/it]
Task 4, Epoch 5/20 => Loss 0.234, Train_accy 92.68:  20%|██        | 4/20 [02:53<09:14, 34.65s/it]
Task 4, Epoch 5/20 => Loss 0.234, Train_accy 92.68:  25%|██▌       | 5/20 [02:53<08:39, 34.66s/it]
Task 4, Epoch 6/20 => Loss 0.213, Train_accy 93.29:  25%|██▌       | 5/20 [03:27<08:39, 34.66s/it]
Task 4, Epoch 6/20 => Loss 0.213, Train_accy 93.29:  30%|███       | 6/20 [03:27<08:04, 34.64s/it]
Task 4, Epoch 7/20 => Loss 0.231, Train_accy 92.72:  30%|███       | 6/20 [04:02<08:04, 34.64s/it]
Task 4, Epoch 7/20 => Loss 0.231, Train_accy 92.72:  35%|███▌      | 7/20 [04:02<07:30, 34.65s/it]
Task 4, Epoch 8/20 => Loss 0.210, Train_accy 93.49:  35%|███▌      | 7/20 [04:37<07:30, 34.65s/it]
Task 4, Epoch 8/20 => Loss 0.210, Train_accy 93.49:  40%|████      | 8/20 [04:37<06:56, 34.68s/it]
Task 4, Epoch 9/20 => Loss 0.211, Train_accy 93.46:  40%|████      | 8/20 [05:11<06:56, 34.68s/it]
Task 4, Epoch 9/20 => Loss 0.211, Train_accy 93.46:  45%|████▌     | 9/20 [05:11<06:21, 34.68s/it]
Task 4, Epoch 10/20 => Loss 0.203, Train_accy 93.81:  45%|████▌     | 9/20 [05:46<06:21, 34.68s/it]
Task 4, Epoch 10/20 => Loss 0.203, Train_accy 93.81:  50%|█████     | 10/20 [05:46<05:46, 34.68s/it]
Task 4, Epoch 11/20 => Loss 0.185, Train_accy 94.14:  50%|█████     | 10/20 [06:21<05:46, 34.68s/it]
Task 4, Epoch 11/20 => Loss 0.185, Train_accy 94.14:  55%|█████▌    | 11/20 [06:21<05:12, 34.69s/it]
Task 4, Epoch 12/20 => Loss 0.183, Train_accy 94.43:  55%|█████▌    | 11/20 [06:56<05:12, 34.69s/it]
Task 4, Epoch 12/20 => Loss 0.183, Train_accy 94.43:  60%|██████    | 12/20 [06:56<04:37, 34.69s/it]
Task 4, Epoch 13/20 => Loss 0.184, Train_accy 94.40:  60%|██████    | 12/20 [07:30<04:37, 34.69s/it]
Task 4, Epoch 13/20 => Loss 0.184, Train_accy 94.40:  65%|██████▌   | 13/20 [07:30<04:02, 34.68s/it]
Task 4, Epoch 14/20 => Loss 0.189, Train_accy 94.04:  65%|██████▌   | 13/20 [08:05<04:02, 34.68s/it]
Task 4, Epoch 14/20 => Loss 0.189, Train_accy 94.04:  70%|███████   | 14/20 [08:05<03:28, 34.68s/it]
Task 4, Epoch 15/20 => Loss 0.183, Train_accy 94.50:  70%|███████   | 14/20 [08:40<03:28, 34.68s/it]
Task 4, Epoch 15/20 => Loss 0.183, Train_accy 94.50:  75%|███████▌  | 15/20 [08:40<02:53, 34.70s/it]
Task 4, Epoch 16/20 => Loss 0.169, Train_accy 94.74:  75%|███████▌  | 15/20 [09:14<02:53, 34.70s/it]
Task 4, Epoch 16/20 => Loss 0.169, Train_accy 94.74:  80%|████████  | 16/20 [09:14<02:18, 34.68s/it]
Task 4, Epoch 17/20 => Loss 0.181, Train_accy 94.30:  80%|████████  | 16/20 [09:49<02:18, 34.68s/it]
Task 4, Epoch 17/20 => Loss 0.181, Train_accy 94.30:  85%|████████▌ | 17/20 [09:49<01:44, 34.67s/it]
Task 4, Epoch 18/20 => Loss 0.170, Train_accy 94.49:  85%|████████▌ | 17/20 [10:24<01:44, 34.67s/it]
Task 4, Epoch 18/20 => Loss 0.170, Train_accy 94.49:  90%|█████████ | 18/20 [10:24<01:09, 34.66s/it]
Task 4, Epoch 19/20 => Loss 0.171, Train_accy 94.61:  90%|█████████ | 18/20 [10:58<01:09, 34.66s/it]
Task 4, Epoch 19/20 => Loss 0.171, Train_accy 94.61:  95%|█████████▌| 19/20 [10:58<00:34, 34.66s/it]
Task 4, Epoch 20/20 => Loss 0.167, Train_accy 94.70:  95%|█████████▌| 19/20 [11:33<00:34, 34.66s/it]
Task 4, Epoch 20/20 => Loss 0.167, Train_accy 94.70: 100%|██████████| 20/20 [11:33<00:00, 34.65s/it]
Task 4, Epoch 20/20 => Loss 0.167, Train_accy 94.70: 100%|██████████| 20/20 [11:33<00:00, 34.67s/it]
2024-08-11 23:19:59,281 [inflora.py] => Task 4, Epoch 20/20 => Loss 0.167, Train_accy 94.70
Threshold:  0.99
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 27/768 type remove
Layer 4 : 37/768 type remove
Layer 5 : 54/768 type remove
Layer 6 : 89/768 type remove
Layer 7 : 113/768 type remove
Layer 8 : 170/768 type remove
Layer 9 : 253/768 type remove
Layer 10 : 306/768 type remove
Layer 11 : 192/768 type remove
Layer 12 : 292/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 23:20:45,020 [trainer.py] => Time:766.9440360069275
10000 10000
10000 10000
2024-08-11 23:21:02,317 [trainer.py] => Time:17.296865463256836
2024-08-11 23:21:02,318 [inflora.py] => Exemplar size: 0
2024-08-11 23:21:02,318 [trainer.py] => CNN: {'total': 89.32, '00-19': 92.35, '20-39': 92.1, '40-59': 86.75, '60-79': 87.95, '80-99': 87.45, 'old': 89.79, 'new': 87.45}
2024-08-11 23:21:02,318 [trainer.py] => CNN top1 curve: [98.15, 95.62, 93.0, 90.62, 89.32]
2024-08-11 23:21:02,318 [trainer.py] => CNN top1 with task curve: [98.15, 98.22, 97.73, 97.38, 97.56]
2024-08-11 23:21:02,318 [trainer.py] => CNN top1 task curve: [1.0, 0.968, 0.9446666666666667, 0.919875, 0.9048]
Traceback (most recent call last):
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/main.py", line 33, in <module>
    main()
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/main.py", line 11, in main
    train(args)
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/trainer.py", line 28, in train
    _set_random(args["seed"])
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/trainer.py", line 101, in _set_random
    torch.manual_seed(args['seed'])
TypeError: 'int' object is not subscriptable
logs/cub/20_20_sip/InfLoRA/adam/4/0.95_1.0-0.0005/1993
2024-08-11 23:21:06,035 [trainer.py] => config: ./configs/cub200_inflora.json
2024-08-11 23:21:06,035 [trainer.py] => device: [device(type='cuda', index=0)]
2024-08-11 23:21:06,035 [trainer.py] => prefix: reproduce
2024-08-11 23:21:06,042 [trainer.py] => dataset: cub
2024-08-11 23:21:06,043 [trainer.py] => data_path: /mnt/mydisk/ruoheng.li/lrh/Dataset
2024-08-11 23:21:06,043 [trainer.py] => memory_size: 0
2024-08-11 23:21:06,043 [trainer.py] => memory_per_class: 0
2024-08-11 23:21:06,043 [trainer.py] => fixed_memory: True
2024-08-11 23:21:06,043 [trainer.py] => shuffle: True
2024-08-11 23:21:06,043 [trainer.py] => init_cls: 20
2024-08-11 23:21:06,043 [trainer.py] => increment: 20
2024-08-11 23:21:06,043 [trainer.py] => model_name: InfLoRA
2024-08-11 23:21:06,043 [trainer.py] => net_type: sip
2024-08-11 23:21:06,043 [trainer.py] => embd_dim: 768
2024-08-11 23:21:06,043 [trainer.py] => num_heads: 12
2024-08-11 23:21:06,043 [trainer.py] => total_sessions: 10
2024-08-11 23:21:06,043 [trainer.py] => seed: 1993
2024-08-11 23:21:06,043 [trainer.py] => EPSILON: 1e-08
2024-08-11 23:21:06,043 [trainer.py] => init_epoch: 20
2024-08-11 23:21:06,043 [trainer.py] => optim: adam
2024-08-11 23:21:06,043 [trainer.py] => init_lr: 0.0005
2024-08-11 23:21:06,043 [trainer.py] => init_lr_decay: 0.1
2024-08-11 23:21:06,043 [trainer.py] => init_weight_decay: 0.0
2024-08-11 23:21:06,043 [trainer.py] => epochs: 20
2024-08-11 23:21:06,043 [trainer.py] => lrate: 0.0005
2024-08-11 23:21:06,043 [trainer.py] => lrate_decay: 0.1
2024-08-11 23:21:06,043 [trainer.py] => batch_size: 48
2024-08-11 23:21:06,043 [trainer.py] => weight_decay: 0.0
2024-08-11 23:21:06,043 [trainer.py] => rank: 4
2024-08-11 23:21:06,043 [trainer.py] => lamb: 0.95
2024-08-11 23:21:06,043 [trainer.py] => lame: 1.0
2024-08-11 23:21:06,044 [trainer.py] => num_workers: 16
2024-08-11 23:21:06,080 [data_manager.py] => [168, 136, 51, 9, 183, 101, 171, 99, 42, 159, 191, 70, 16, 188, 27, 10, 175, 26, 68, 187, 98, 6, 85, 35, 112, 43, 100, 0, 103, 181, 88, 59, 4, 2, 116, 174, 94, 80, 106, 1, 147, 17, 141, 131, 72, 23, 173, 54, 197, 118, 87, 32, 79, 104, 91, 19, 135, 107, 178, 36, 11, 199, 142, 8, 122, 3, 28, 57, 153, 172, 190, 56, 49, 44, 97, 62, 151, 169, 194, 55, 192, 12, 189, 78, 66, 180, 15, 137, 109, 134, 92, 119, 126, 52, 170, 40, 148, 65, 144, 64, 138, 45, 77, 89, 154, 90, 71, 193, 74, 30, 113, 143, 96, 84, 67, 50, 186, 156, 69, 21, 18, 111, 108, 58, 125, 157, 150, 110, 182, 129, 166, 83, 81, 60, 13, 165, 14, 176, 63, 117, 5, 22, 145, 121, 38, 41, 82, 127, 114, 20, 31, 53, 37, 163, 196, 130, 152, 162, 86, 76, 24, 34, 184, 149, 33, 128, 198, 155, 146, 167, 139, 120, 140, 102, 47, 25, 158, 123, 46, 164, 61, 7, 115, 75, 133, 160, 105, 132, 179, 124, 48, 73, 93, 39, 95, 195, 29, 177, 185, 161]
2024-08-11 23:21:08,017 [trainer.py] => All params: 109136611
2024-08-11 23:21:08,019 [trainer.py] => Trainable params: 109136611
2024-08-11 23:21:08,019 [inflora.py] => Learning on 0-20
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.0.weight', 'image_encoder.blocks.11.attn.lora_B_k.0.weight', 'image_encoder.blocks.11.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_v.0.weight', 'image_encoder.blocks.4.attn.lora_B_k.0.weight', 'classifier_pool.0.weight', 'image_encoder.blocks.6.attn.lora_B_v.0.weight', 'image_encoder.blocks.3.attn.lora_B_k.0.weight', 'image_encoder.blocks.6.attn.lora_B_k.0.weight', 'image_encoder.blocks.3.attn.lora_B_v.0.weight', 'image_encoder.blocks.7.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_k.0.weight', 'image_encoder.blocks.2.attn.lora_B_v.0.weight', 'image_encoder.blocks.4.attn.lora_B_v.0.weight', 'image_encoder.blocks.5.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_k.0.weight', 'classifier_pool.0.bias', 'image_encoder.blocks.5.attn.lora_B_v.0.weight', 'image_encoder.blocks.10.attn.lora_B_k.0.weight', 'image_encoder.blocks.7.attn.lora_B_v.0.weight', 'image_encoder.blocks.8.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_v.0.weight', 'image_encoder.blocks.2.attn.lora_B_k.0.weight', 'image_encoder.blocks.8.attn.lora_B_v.0.weight', 'image_encoder.blocks.10.attn.lora_B_v.0.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 0, Epoch 1/20 => Loss 1.854, Train_accy 48.94:   0%|          | 0/20 [00:03<?, ?it/s]
Task 0, Epoch 1/20 => Loss 1.854, Train_accy 48.94:   5%|▌         | 1/20 [00:03<01:13,  3.87s/it]
Task 0, Epoch 2/20 => Loss 0.458, Train_accy 86.36:   5%|▌         | 1/20 [00:07<01:13,  3.87s/it]
Task 0, Epoch 2/20 => Loss 0.458, Train_accy 86.36:  10%|█         | 2/20 [00:07<01:09,  3.84s/it]
Task 0, Epoch 3/20 => Loss 0.275, Train_accy 91.65:  10%|█         | 2/20 [00:11<01:09,  3.84s/it]
Task 0, Epoch 3/20 => Loss 0.275, Train_accy 91.65:  15%|█▌        | 3/20 [00:11<01:05,  3.83s/it]
Task 0, Epoch 4/20 => Loss 0.216, Train_accy 93.34:  15%|█▌        | 3/20 [00:15<01:05,  3.83s/it]
Task 0, Epoch 4/20 => Loss 0.216, Train_accy 93.34:  20%|██        | 4/20 [00:15<01:01,  3.83s/it]
Task 0, Epoch 5/20 => Loss 0.202, Train_accy 93.45:  20%|██        | 4/20 [00:19<01:01,  3.83s/it]
Task 0, Epoch 5/20 => Loss 0.202, Train_accy 93.45:  25%|██▌       | 5/20 [00:19<00:57,  3.82s/it]
Task 0, Epoch 6/20 => Loss 0.170, Train_accy 95.45:  25%|██▌       | 5/20 [00:22<00:57,  3.82s/it]
Task 0, Epoch 6/20 => Loss 0.170, Train_accy 95.45:  30%|███       | 6/20 [00:22<00:53,  3.83s/it]
Task 0, Epoch 7/20 => Loss 0.188, Train_accy 93.76:  30%|███       | 6/20 [00:26<00:53,  3.83s/it]
Task 0, Epoch 7/20 => Loss 0.188, Train_accy 93.76:  35%|███▌      | 7/20 [00:26<00:49,  3.82s/it]
Task 0, Epoch 8/20 => Loss 0.151, Train_accy 95.45:  35%|███▌      | 7/20 [00:30<00:49,  3.82s/it]
Task 0, Epoch 8/20 => Loss 0.151, Train_accy 95.45:  40%|████      | 8/20 [00:30<00:45,  3.83s/it]
Task 0, Epoch 9/20 => Loss 0.141, Train_accy 95.14:  40%|████      | 8/20 [00:34<00:45,  3.83s/it]
Task 0, Epoch 9/20 => Loss 0.141, Train_accy 95.14:  45%|████▌     | 9/20 [00:34<00:42,  3.84s/it]
Task 0, Epoch 10/20 => Loss 0.151, Train_accy 95.35:  45%|████▌     | 9/20 [00:38<00:42,  3.84s/it]
Task 0, Epoch 10/20 => Loss 0.151, Train_accy 95.35:  50%|█████     | 10/20 [00:38<00:38,  3.84s/it]
Task 0, Epoch 11/20 => Loss 0.166, Train_accy 94.50:  50%|█████     | 10/20 [00:42<00:38,  3.84s/it]
Task 0, Epoch 11/20 => Loss 0.166, Train_accy 94.50:  55%|█████▌    | 11/20 [00:42<00:34,  3.83s/it]
Task 0, Epoch 12/20 => Loss 0.173, Train_accy 94.50:  55%|█████▌    | 11/20 [00:46<00:34,  3.83s/it]
Task 0, Epoch 12/20 => Loss 0.173, Train_accy 94.50:  60%|██████    | 12/20 [00:46<00:30,  3.84s/it]
Task 0, Epoch 13/20 => Loss 0.144, Train_accy 95.56:  60%|██████    | 12/20 [00:49<00:30,  3.84s/it]
Task 0, Epoch 13/20 => Loss 0.144, Train_accy 95.56:  65%|██████▌   | 13/20 [00:49<00:26,  3.83s/it]
Task 0, Epoch 14/20 => Loss 0.152, Train_accy 95.67:  65%|██████▌   | 13/20 [00:53<00:26,  3.83s/it]
Task 0, Epoch 14/20 => Loss 0.152, Train_accy 95.67:  70%|███████   | 14/20 [00:53<00:22,  3.83s/it]
Task 0, Epoch 15/20 => Loss 0.152, Train_accy 95.24:  70%|███████   | 14/20 [00:57<00:22,  3.83s/it]
Task 0, Epoch 15/20 => Loss 0.152, Train_accy 95.24:  75%|███████▌  | 15/20 [00:57<00:19,  3.83s/it]
Task 0, Epoch 16/20 => Loss 0.131, Train_accy 95.88:  75%|███████▌  | 15/20 [01:01<00:19,  3.83s/it]
Task 0, Epoch 16/20 => Loss 0.131, Train_accy 95.88:  80%|████████  | 16/20 [01:01<00:15,  3.83s/it]
Task 0, Epoch 17/20 => Loss 0.126, Train_accy 95.45:  80%|████████  | 16/20 [01:05<00:15,  3.83s/it]
Task 0, Epoch 17/20 => Loss 0.126, Train_accy 95.45:  85%|████████▌ | 17/20 [01:05<00:11,  3.83s/it]
Task 0, Epoch 18/20 => Loss 0.142, Train_accy 95.88:  85%|████████▌ | 17/20 [01:09<00:11,  3.83s/it]
Task 0, Epoch 18/20 => Loss 0.142, Train_accy 95.88:  90%|█████████ | 18/20 [01:09<00:07,  3.84s/it]
Task 0, Epoch 19/20 => Loss 0.105, Train_accy 96.93:  90%|█████████ | 18/20 [01:12<00:07,  3.84s/it]
Task 0, Epoch 19/20 => Loss 0.105, Train_accy 96.93:  95%|█████████▌| 19/20 [01:12<00:03,  3.85s/it]
Task 0, Epoch 20/20 => Loss 0.116, Train_accy 96.19:  95%|█████████▌| 19/20 [01:16<00:03,  3.85s/it]
Task 0, Epoch 20/20 => Loss 0.116, Train_accy 96.19: 100%|██████████| 20/20 [01:16<00:00,  3.84s/it]
Task 0, Epoch 20/20 => Loss 0.116, Train_accy 96.19: 100%|██████████| 20/20 [01:16<00:00,  3.84s/it]
2024-08-11 23:22:29,653 [inflora.py] => Task 0, Epoch 20/20 => Loss 0.116, Train_accy 96.19
Threshold:  0.95
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 6/768 type remove
Layer 2 : 9/768 type remove
Layer 3 : 11/768 type remove
Layer 4 : 11/768 type remove
Layer 5 : 16/768 type remove
Layer 6 : 13/768 type remove
Layer 7 : 12/768 type remove
Layer 8 : 13/768 type remove
Layer 9 : 12/768 type remove
Layer 10 : 9/768 type remove
Layer 11 : 2/768 type remove
Layer 12 : 2/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 23:22:37,744 [trainer.py] => Time:89.72486877441406
247 247
247 247
2024-08-11 23:22:38,943 [trainer.py] => Time:1.1989755630493164
2024-08-11 23:22:38,943 [inflora.py] => Exemplar size: 0
2024-08-11 23:22:38,944 [trainer.py] => CNN: {'total': 97.98, '00-19': 97.98, 'old': 0, 'new': 97.98}
2024-08-11 23:22:38,944 [trainer.py] => CNN top1 curve: [97.98]
2024-08-11 23:22:38,944 [trainer.py] => CNN top1 with task curve: [97.98]
2024-08-11 23:22:38,944 [trainer.py] => CNN top1 task curve: [1.0]
2024-08-11 23:22:39,424 [trainer.py] => All params: 109136611
2024-08-11 23:22:39,426 [trainer.py] => Trainable params: 89108
2024-08-11 23:22:39,426 [inflora.py] => Learning on 20-40
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.1.weight', 'classifier_pool.1.bias', 'classifier_pool.1.weight', 'image_encoder.blocks.0.attn.lora_B_v.1.weight', 'image_encoder.blocks.8.attn.lora_B_k.1.weight', 'image_encoder.blocks.3.attn.lora_B_k.1.weight', 'image_encoder.blocks.5.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.1.weight', 'image_encoder.blocks.10.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_v.1.weight', 'image_encoder.blocks.3.attn.lora_B_v.1.weight', 'image_encoder.blocks.9.attn.lora_B_v.1.weight', 'image_encoder.blocks.9.attn.lora_B_k.1.weight', 'image_encoder.blocks.11.attn.lora_B_v.1.weight', 'image_encoder.blocks.4.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_k.1.weight', 'image_encoder.blocks.2.attn.lora_B_v.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.1.weight', 'image_encoder.blocks.11.attn.lora_B_k.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.1.weight', 'image_encoder.blocks.0.attn.lora_B_k.1.weight', 'image_encoder.blocks.5.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.1.weight', 'image_encoder.blocks.1.attn.lora_B_k.1.weight', 'image_encoder.blocks.4.attn.lora_B_v.1.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 1, Epoch 1/20 => Loss 1.524, Train_accy 57.67:   0%|          | 0/20 [00:03<?, ?it/s]
Task 1, Epoch 1/20 => Loss 1.524, Train_accy 57.67:   5%|▌         | 1/20 [00:03<01:15,  3.98s/it]
Task 1, Epoch 2/20 => Loss 0.376, Train_accy 89.95:   5%|▌         | 1/20 [00:07<01:15,  3.98s/it]
Task 1, Epoch 2/20 => Loss 0.376, Train_accy 89.95:  10%|█         | 2/20 [00:07<01:11,  3.98s/it]
Task 1, Epoch 3/20 => Loss 0.253, Train_accy 92.59:  10%|█         | 2/20 [00:11<01:11,  3.98s/it]
Task 1, Epoch 3/20 => Loss 0.253, Train_accy 92.59:  15%|█▌        | 3/20 [00:11<01:07,  3.98s/it]
Task 1, Epoch 4/20 => Loss 0.237, Train_accy 92.80:  15%|█▌        | 3/20 [00:15<01:07,  3.98s/it]
Task 1, Epoch 4/20 => Loss 0.237, Train_accy 92.80:  20%|██        | 4/20 [00:15<01:03,  3.97s/it]
Task 1, Epoch 5/20 => Loss 0.214, Train_accy 93.02:  20%|██        | 4/20 [00:19<01:03,  3.97s/it]
Task 1, Epoch 5/20 => Loss 0.214, Train_accy 93.02:  25%|██▌       | 5/20 [00:19<00:59,  3.97s/it]
Task 1, Epoch 6/20 => Loss 0.174, Train_accy 94.92:  25%|██▌       | 5/20 [00:23<00:59,  3.97s/it]
Task 1, Epoch 6/20 => Loss 0.174, Train_accy 94.92:  30%|███       | 6/20 [00:23<00:55,  3.96s/it]
Task 1, Epoch 7/20 => Loss 0.155, Train_accy 95.34:  30%|███       | 6/20 [00:27<00:55,  3.96s/it]
Task 1, Epoch 7/20 => Loss 0.155, Train_accy 95.34:  35%|███▌      | 7/20 [00:27<00:51,  3.95s/it]
Task 1, Epoch 8/20 => Loss 0.146, Train_accy 95.45:  35%|███▌      | 7/20 [00:31<00:51,  3.95s/it]
Task 1, Epoch 8/20 => Loss 0.146, Train_accy 95.45:  40%|████      | 8/20 [00:31<00:47,  3.98s/it]
Task 1, Epoch 9/20 => Loss 0.157, Train_accy 95.34:  40%|████      | 8/20 [00:35<00:47,  3.98s/it]
Task 1, Epoch 9/20 => Loss 0.157, Train_accy 95.34:  45%|████▌     | 9/20 [00:35<00:43,  3.99s/it]
Task 1, Epoch 10/20 => Loss 0.127, Train_accy 95.77:  45%|████▌     | 9/20 [00:39<00:43,  3.99s/it]
Task 1, Epoch 10/20 => Loss 0.127, Train_accy 95.77:  50%|█████     | 10/20 [00:39<00:39,  3.99s/it]
Task 1, Epoch 11/20 => Loss 0.165, Train_accy 94.92:  50%|█████     | 10/20 [00:43<00:39,  3.99s/it]
Task 1, Epoch 11/20 => Loss 0.165, Train_accy 94.92:  55%|█████▌    | 11/20 [00:43<00:35,  3.99s/it]
Task 1, Epoch 12/20 => Loss 0.134, Train_accy 96.19:  55%|█████▌    | 11/20 [00:47<00:35,  3.99s/it]
Task 1, Epoch 12/20 => Loss 0.134, Train_accy 96.19:  60%|██████    | 12/20 [00:47<00:31,  3.99s/it]
Task 1, Epoch 13/20 => Loss 0.142, Train_accy 95.66:  60%|██████    | 12/20 [00:51<00:31,  3.99s/it]
Task 1, Epoch 13/20 => Loss 0.142, Train_accy 95.66:  65%|██████▌   | 13/20 [00:51<00:27,  4.00s/it]
Task 1, Epoch 14/20 => Loss 0.158, Train_accy 95.03:  65%|██████▌   | 13/20 [00:55<00:27,  4.00s/it]
Task 1, Epoch 14/20 => Loss 0.158, Train_accy 95.03:  70%|███████   | 14/20 [00:55<00:23,  3.99s/it]
Task 1, Epoch 15/20 => Loss 0.125, Train_accy 96.51:  70%|███████   | 14/20 [00:59<00:23,  3.99s/it]
Task 1, Epoch 15/20 => Loss 0.125, Train_accy 96.51:  75%|███████▌  | 15/20 [00:59<00:20,  4.01s/it]
Task 1, Epoch 16/20 => Loss 0.107, Train_accy 97.04:  75%|███████▌  | 15/20 [01:03<00:20,  4.01s/it]
Task 1, Epoch 16/20 => Loss 0.107, Train_accy 97.04:  80%|████████  | 16/20 [01:03<00:16,  4.00s/it]
Task 1, Epoch 17/20 => Loss 0.134, Train_accy 95.66:  80%|████████  | 16/20 [01:07<00:16,  4.00s/it]
Task 1, Epoch 17/20 => Loss 0.134, Train_accy 95.66:  85%|████████▌ | 17/20 [01:07<00:12,  4.02s/it]
Task 1, Epoch 18/20 => Loss 0.112, Train_accy 96.72:  85%|████████▌ | 17/20 [01:11<00:12,  4.02s/it]
Task 1, Epoch 18/20 => Loss 0.112, Train_accy 96.72:  90%|█████████ | 18/20 [01:11<00:08,  4.01s/it]
Task 1, Epoch 19/20 => Loss 0.121, Train_accy 96.83:  90%|█████████ | 18/20 [01:15<00:08,  4.01s/it]
Task 1, Epoch 19/20 => Loss 0.121, Train_accy 96.83:  95%|█████████▌| 19/20 [01:15<00:04,  4.01s/it]
Task 1, Epoch 20/20 => Loss 0.117, Train_accy 96.08:  95%|█████████▌| 19/20 [01:19<00:04,  4.01s/it]
Task 1, Epoch 20/20 => Loss 0.117, Train_accy 96.08: 100%|██████████| 20/20 [01:19<00:00,  4.02s/it]
Task 1, Epoch 20/20 => Loss 0.117, Train_accy 96.08: 100%|██████████| 20/20 [01:19<00:00,  3.99s/it]
2024-08-11 23:24:03,990 [inflora.py] => Task 1, Epoch 20/20 => Loss 0.117, Train_accy 96.08
Threshold:  0.955
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 10/768 type remove
Layer 3 : 13/768 type remove
Layer 4 : 13/768 type remove
Layer 5 : 19/768 type remove
Layer 6 : 17/768 type remove
Layer 7 : 16/768 type remove
Layer 8 : 17/768 type remove
Layer 9 : 17/768 type remove
Layer 10 : 15/768 type remove
Layer 11 : 5/768 type remove
Layer 12 : 6/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 23:24:15,533 [trainer.py] => Time:96.10679459571838
454 454
454 454
2024-08-11 23:24:17,212 [trainer.py] => Time:1.6793911457061768
2024-08-11 23:24:17,213 [inflora.py] => Exemplar size: 0
2024-08-11 23:24:17,213 [trainer.py] => CNN: {'total': 94.93, '00-19': 97.17, '20-39': 92.27, 'old': 97.17, 'new': 92.27}
2024-08-11 23:24:17,213 [trainer.py] => CNN top1 curve: [97.98, 94.93]
2024-08-11 23:24:17,213 [trainer.py] => CNN top1 with task curve: [97.98, 97.8]
2024-08-11 23:24:17,213 [trainer.py] => CNN top1 task curve: [1.0, 0.9625550660792952]
2024-08-11 23:24:17,676 [trainer.py] => All params: 109136611
2024-08-11 23:24:17,677 [trainer.py] => Trainable params: 89108
2024-08-11 23:24:17,677 [inflora.py] => Learning on 40-60
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_k.2.weight', 'image_encoder.blocks.5.attn.lora_B_v.2.weight', 'classifier_pool.2.bias', 'image_encoder.blocks.8.attn.lora_B_v.2.weight', 'image_encoder.blocks.6.attn.lora_B_k.2.weight', 'image_encoder.blocks.7.attn.lora_B_v.2.weight', 'image_encoder.blocks.4.attn.lora_B_v.2.weight', 'image_encoder.blocks.1.attn.lora_B_v.2.weight', 'image_encoder.blocks.4.attn.lora_B_k.2.weight', 'image_encoder.blocks.10.attn.lora_B_k.2.weight', 'image_encoder.blocks.9.attn.lora_B_v.2.weight', 'image_encoder.blocks.7.attn.lora_B_k.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.2.weight', 'image_encoder.blocks.1.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_k.2.weight', 'image_encoder.blocks.2.attn.lora_B_v.2.weight', 'image_encoder.blocks.9.attn.lora_B_k.2.weight', 'image_encoder.blocks.11.attn.lora_B_k.2.weight', 'image_encoder.blocks.10.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_v.2.weight', 'image_encoder.blocks.6.attn.lora_B_v.2.weight', 'classifier_pool.2.weight', 'image_encoder.blocks.5.attn.lora_B_k.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_k.2.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 2, Epoch 1/20 => Loss 1.831, Train_accy 53.66:   0%|          | 0/20 [00:03<?, ?it/s]
Task 2, Epoch 1/20 => Loss 1.831, Train_accy 53.66:   5%|▌         | 1/20 [00:03<01:15,  3.99s/it]
Task 2, Epoch 2/20 => Loss 0.363, Train_accy 89.08:   5%|▌         | 1/20 [00:07<01:15,  3.99s/it]
Task 2, Epoch 2/20 => Loss 0.363, Train_accy 89.08:  10%|█         | 2/20 [00:07<01:11,  4.00s/it]
Task 2, Epoch 3/20 => Loss 0.259, Train_accy 92.15:  10%|█         | 2/20 [00:11<01:11,  4.00s/it]
Task 2, Epoch 3/20 => Loss 0.259, Train_accy 92.15:  15%|█▌        | 3/20 [00:11<01:07,  4.00s/it]
Task 2, Epoch 4/20 => Loss 0.217, Train_accy 93.64:  15%|█▌        | 3/20 [00:16<01:07,  4.00s/it]
Task 2, Epoch 4/20 => Loss 0.217, Train_accy 93.64:  20%|██        | 4/20 [00:16<01:04,  4.01s/it]
Task 2, Epoch 5/20 => Loss 0.199, Train_accy 94.17:  20%|██        | 4/20 [00:20<01:04,  4.01s/it]
Task 2, Epoch 5/20 => Loss 0.199, Train_accy 94.17:  25%|██▌       | 5/20 [00:20<01:00,  4.02s/it]
Task 2, Epoch 6/20 => Loss 0.189, Train_accy 93.96:  25%|██▌       | 5/20 [00:24<01:00,  4.02s/it]
Task 2, Epoch 6/20 => Loss 0.189, Train_accy 93.96:  30%|███       | 6/20 [00:24<00:56,  4.02s/it]
Task 2, Epoch 7/20 => Loss 0.209, Train_accy 93.64:  30%|███       | 6/20 [00:28<00:56,  4.02s/it]
Task 2, Epoch 7/20 => Loss 0.209, Train_accy 93.64:  35%|███▌      | 7/20 [00:28<00:52,  4.04s/it]
Task 2, Epoch 8/20 => Loss 0.210, Train_accy 93.53:  35%|███▌      | 7/20 [00:32<00:52,  4.04s/it]
Task 2, Epoch 8/20 => Loss 0.210, Train_accy 93.53:  40%|████      | 8/20 [00:32<00:48,  4.05s/it]
Task 2, Epoch 9/20 => Loss 0.150, Train_accy 96.08:  40%|████      | 8/20 [00:36<00:48,  4.05s/it]
Task 2, Epoch 9/20 => Loss 0.150, Train_accy 96.08:  45%|████▌     | 9/20 [00:36<00:44,  4.04s/it]
Task 2, Epoch 10/20 => Loss 0.168, Train_accy 94.91:  45%|████▌     | 9/20 [00:40<00:44,  4.04s/it]
Task 2, Epoch 10/20 => Loss 0.168, Train_accy 94.91:  50%|█████     | 10/20 [00:40<00:40,  4.05s/it]
Task 2, Epoch 11/20 => Loss 0.163, Train_accy 95.76:  50%|█████     | 10/20 [00:44<00:40,  4.05s/it]
Task 2, Epoch 11/20 => Loss 0.163, Train_accy 95.76:  55%|█████▌    | 11/20 [00:44<00:36,  4.06s/it]
Task 2, Epoch 12/20 => Loss 0.166, Train_accy 95.02:  55%|█████▌    | 11/20 [00:48<00:36,  4.06s/it]
Task 2, Epoch 12/20 => Loss 0.166, Train_accy 95.02:  60%|██████    | 12/20 [00:48<00:32,  4.06s/it]
Task 2, Epoch 13/20 => Loss 0.136, Train_accy 96.08:  60%|██████    | 12/20 [00:52<00:32,  4.06s/it]
Task 2, Epoch 13/20 => Loss 0.136, Train_accy 96.08:  65%|██████▌   | 13/20 [00:52<00:28,  4.05s/it]
Task 2, Epoch 14/20 => Loss 0.131, Train_accy 96.50:  65%|██████▌   | 13/20 [00:56<00:28,  4.05s/it]
Task 2, Epoch 14/20 => Loss 0.131, Train_accy 96.50:  70%|███████   | 14/20 [00:56<00:24,  4.05s/it]
Task 2, Epoch 15/20 => Loss 0.169, Train_accy 94.91:  70%|███████   | 14/20 [01:00<00:24,  4.05s/it]
Task 2, Epoch 15/20 => Loss 0.169, Train_accy 94.91:  75%|███████▌  | 15/20 [01:00<00:20,  4.06s/it]
Task 2, Epoch 16/20 => Loss 0.144, Train_accy 95.23:  75%|███████▌  | 15/20 [01:04<00:20,  4.06s/it]
Task 2, Epoch 16/20 => Loss 0.144, Train_accy 95.23:  80%|████████  | 16/20 [01:04<00:16,  4.06s/it]
Task 2, Epoch 17/20 => Loss 0.122, Train_accy 96.18:  80%|████████  | 16/20 [01:08<00:16,  4.06s/it]
Task 2, Epoch 17/20 => Loss 0.122, Train_accy 96.18:  85%|████████▌ | 17/20 [01:08<00:12,  4.05s/it]
Task 2, Epoch 18/20 => Loss 0.138, Train_accy 95.65:  85%|████████▌ | 17/20 [01:12<00:12,  4.05s/it]
Task 2, Epoch 18/20 => Loss 0.138, Train_accy 95.65:  90%|█████████ | 18/20 [01:12<00:08,  4.06s/it]
Task 2, Epoch 19/20 => Loss 0.169, Train_accy 94.91:  90%|█████████ | 18/20 [01:16<00:08,  4.06s/it]
Task 2, Epoch 19/20 => Loss 0.169, Train_accy 94.91:  95%|█████████▌| 19/20 [01:16<00:04,  4.05s/it]
Task 2, Epoch 20/20 => Loss 0.169, Train_accy 95.44:  95%|█████████▌| 19/20 [01:20<00:04,  4.05s/it]
Task 2, Epoch 20/20 => Loss 0.169, Train_accy 95.44: 100%|██████████| 20/20 [01:20<00:00,  4.04s/it]
Task 2, Epoch 20/20 => Loss 0.169, Train_accy 95.44: 100%|██████████| 20/20 [01:20<00:00,  4.04s/it]
2024-08-11 23:25:43,621 [inflora.py] => Task 2, Epoch 20/20 => Loss 0.169, Train_accy 95.44
Threshold:  0.96
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 10/768 type remove
Layer 3 : 14/768 type remove
Layer 4 : 16/768 type remove
Layer 5 : 22/768 type remove
Layer 6 : 20/768 type remove
Layer 7 : 19/768 type remove
Layer 8 : 19/768 type remove
Layer 9 : 20/768 type remove
Layer 10 : 19/768 type remove
Layer 11 : 7/768 type remove
Layer 12 : 8/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 23:25:54,023 [trainer.py] => Time:96.34514594078064
672 672
672 672
2024-08-11 23:25:56,059 [trainer.py] => Time:2.035592794418335
2024-08-11 23:25:56,059 [inflora.py] => Exemplar size: 0
2024-08-11 23:25:56,059 [trainer.py] => CNN: {'total': 92.26, '00-19': 93.52, '20-39': 90.34, '40-59': 92.66, 'old': 92.07, 'new': 92.66}
2024-08-11 23:25:56,059 [trainer.py] => CNN top1 curve: [97.98, 94.93, 92.26]
2024-08-11 23:25:56,059 [trainer.py] => CNN top1 with task curve: [97.98, 97.8, 97.92]
2024-08-11 23:25:56,059 [trainer.py] => CNN top1 task curve: [1.0, 0.9625550660792952, 0.9300595238095238]
2024-08-11 23:25:56,522 [trainer.py] => All params: 109136611
2024-08-11 23:25:56,524 [trainer.py] => Trainable params: 89108
2024-08-11 23:25:56,524 [inflora.py] => Learning on 60-80
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.3.weight', 'image_encoder.blocks.11.attn.lora_B_v.3.weight', 'image_encoder.blocks.1.attn.lora_B_k.3.weight', 'image_encoder.blocks.5.attn.lora_B_k.3.weight', 'image_encoder.blocks.3.attn.lora_B_k.3.weight', 'image_encoder.blocks.3.attn.lora_B_v.3.weight', 'image_encoder.blocks.2.attn.lora_B_v.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.3.weight', 'image_encoder.blocks.2.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_k.3.weight', 'image_encoder.blocks.5.attn.lora_B_v.3.weight', 'image_encoder.blocks.6.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_v.3.weight', 'image_encoder.blocks.1.attn.lora_B_v.3.weight', 'image_encoder.blocks.7.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.3.weight', 'image_encoder.blocks.6.attn.lora_B_k.3.weight', 'classifier_pool.3.weight', 'image_encoder.blocks.7.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_k.3.weight', 'image_encoder.blocks.10.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.3.weight', 'classifier_pool.3.bias', 'image_encoder.blocks.10.attn.lora_B_k.3.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 3, Epoch 1/20 => Loss 1.495, Train_accy 58.46:   0%|          | 0/20 [00:04<?, ?it/s]
Task 3, Epoch 1/20 => Loss 1.495, Train_accy 58.46:   5%|▌         | 1/20 [00:04<01:16,  4.02s/it]
Task 3, Epoch 2/20 => Loss 0.277, Train_accy 91.86:   5%|▌         | 1/20 [00:08<01:16,  4.02s/it]
Task 3, Epoch 2/20 => Loss 0.277, Train_accy 91.86:  10%|█         | 2/20 [00:08<01:12,  4.05s/it]
Task 3, Epoch 3/20 => Loss 0.238, Train_accy 93.13:  10%|█         | 2/20 [00:12<01:12,  4.05s/it]
Task 3, Epoch 3/20 => Loss 0.238, Train_accy 93.13:  15%|█▌        | 3/20 [00:12<01:08,  4.05s/it]
Task 3, Epoch 4/20 => Loss 0.225, Train_accy 92.71:  15%|█▌        | 3/20 [00:16<01:08,  4.05s/it]
Task 3, Epoch 4/20 => Loss 0.225, Train_accy 92.71:  20%|██        | 4/20 [00:16<01:05,  4.09s/it]
Task 3, Epoch 5/20 => Loss 0.200, Train_accy 94.29:  20%|██        | 4/20 [00:20<01:05,  4.09s/it]
Task 3, Epoch 5/20 => Loss 0.200, Train_accy 94.29:  25%|██▌       | 5/20 [00:20<01:01,  4.09s/it]
Task 3, Epoch 6/20 => Loss 0.170, Train_accy 94.40:  25%|██▌       | 5/20 [00:24<01:01,  4.09s/it]
Task 3, Epoch 6/20 => Loss 0.170, Train_accy 94.40:  30%|███       | 6/20 [00:24<00:57,  4.10s/it]
Task 3, Epoch 7/20 => Loss 0.170, Train_accy 94.61:  30%|███       | 6/20 [00:28<00:57,  4.10s/it]
Task 3, Epoch 7/20 => Loss 0.170, Train_accy 94.61:  35%|███▌      | 7/20 [00:28<00:53,  4.08s/it]
Task 3, Epoch 8/20 => Loss 0.194, Train_accy 93.97:  35%|███▌      | 7/20 [00:32<00:53,  4.08s/it]
Task 3, Epoch 8/20 => Loss 0.194, Train_accy 93.97:  40%|████      | 8/20 [00:32<00:49,  4.11s/it]
Task 3, Epoch 9/20 => Loss 0.157, Train_accy 94.40:  40%|████      | 8/20 [00:36<00:49,  4.11s/it]
Task 3, Epoch 9/20 => Loss 0.157, Train_accy 94.40:  45%|████▌     | 9/20 [00:36<00:45,  4.10s/it]
Task 3, Epoch 10/20 => Loss 0.162, Train_accy 94.82:  45%|████▌     | 9/20 [00:40<00:45,  4.10s/it]
Task 3, Epoch 10/20 => Loss 0.162, Train_accy 94.82:  50%|█████     | 10/20 [00:40<00:40,  4.10s/it]
Task 3, Epoch 11/20 => Loss 0.154, Train_accy 94.93:  50%|█████     | 10/20 [00:44<00:40,  4.10s/it]
Task 3, Epoch 11/20 => Loss 0.154, Train_accy 94.93:  55%|█████▌    | 11/20 [00:44<00:36,  4.09s/it]
Task 3, Epoch 12/20 => Loss 0.145, Train_accy 95.45:  55%|█████▌    | 11/20 [00:48<00:36,  4.09s/it]
Task 3, Epoch 12/20 => Loss 0.145, Train_accy 95.45:  60%|██████    | 12/20 [00:49<00:32,  4.07s/it]
Task 3, Epoch 13/20 => Loss 0.120, Train_accy 96.51:  60%|██████    | 12/20 [00:53<00:32,  4.07s/it]
Task 3, Epoch 13/20 => Loss 0.120, Train_accy 96.51:  65%|██████▌   | 13/20 [00:53<00:28,  4.08s/it]
Task 3, Epoch 14/20 => Loss 0.149, Train_accy 94.71:  65%|██████▌   | 13/20 [00:57<00:28,  4.08s/it]
Task 3, Epoch 14/20 => Loss 0.149, Train_accy 94.71:  70%|███████   | 14/20 [00:57<00:24,  4.09s/it]
Task 3, Epoch 15/20 => Loss 0.109, Train_accy 96.72:  70%|███████   | 14/20 [01:01<00:24,  4.09s/it]
Task 3, Epoch 15/20 => Loss 0.109, Train_accy 96.72:  75%|███████▌  | 15/20 [01:01<00:20,  4.10s/it]
Task 3, Epoch 16/20 => Loss 0.113, Train_accy 96.30:  75%|███████▌  | 15/20 [01:05<00:20,  4.10s/it]
Task 3, Epoch 16/20 => Loss 0.113, Train_accy 96.30:  80%|████████  | 16/20 [01:05<00:16,  4.11s/it]
Task 3, Epoch 17/20 => Loss 0.145, Train_accy 95.03:  80%|████████  | 16/20 [01:09<00:16,  4.11s/it]
Task 3, Epoch 17/20 => Loss 0.145, Train_accy 95.03:  85%|████████▌ | 17/20 [01:09<00:12,  4.10s/it]
Task 3, Epoch 18/20 => Loss 0.102, Train_accy 96.72:  85%|████████▌ | 17/20 [01:13<00:12,  4.10s/it]
Task 3, Epoch 18/20 => Loss 0.102, Train_accy 96.72:  90%|█████████ | 18/20 [01:13<00:08,  4.11s/it]
Task 3, Epoch 19/20 => Loss 0.161, Train_accy 94.93:  90%|█████████ | 18/20 [01:17<00:08,  4.11s/it]
Task 3, Epoch 19/20 => Loss 0.161, Train_accy 94.93:  95%|█████████▌| 19/20 [01:17<00:04,  4.10s/it]
Task 3, Epoch 20/20 => Loss 0.116, Train_accy 96.41:  95%|█████████▌| 19/20 [01:21<00:04,  4.10s/it]
Task 3, Epoch 20/20 => Loss 0.116, Train_accy 96.41: 100%|██████████| 20/20 [01:21<00:00,  4.12s/it]
Task 3, Epoch 20/20 => Loss 0.116, Train_accy 96.41: 100%|██████████| 20/20 [01:21<00:00,  4.10s/it]
2024-08-11 23:27:23,494 [inflora.py] => Task 3, Epoch 20/20 => Loss 0.116, Train_accy 96.41
Threshold:  0.965
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 10/768 type remove
Layer 3 : 16/768 type remove
Layer 4 : 18/768 type remove
Layer 5 : 26/768 type remove
Layer 6 : 23/768 type remove
Layer 7 : 22/768 type remove
Layer 8 : 22/768 type remove
Layer 9 : 23/768 type remove
Layer 10 : 21/768 type remove
Layer 11 : 9/768 type remove
Layer 12 : 11/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 23:27:33,614 [trainer.py] => Time:97.09077763557434
919 919
919 919
2024-08-11 23:27:36,181 [trainer.py] => Time:2.565962076187134
2024-08-11 23:27:36,181 [inflora.py] => Exemplar size: 0
2024-08-11 23:27:36,181 [trainer.py] => CNN: {'total': 89.12, '00-19': 91.09, '20-39': 87.92, '40-59': 90.37, '60-79': 87.04, 'old': 89.88, 'new': 87.04}
2024-08-11 23:27:36,181 [trainer.py] => CNN top1 curve: [97.98, 94.93, 92.26, 89.12]
2024-08-11 23:27:36,181 [trainer.py] => CNN top1 with task curve: [97.98, 97.8, 97.92, 97.93]
2024-08-11 23:27:36,181 [trainer.py] => CNN top1 task curve: [1.0, 0.9625550660792952, 0.9300595238095238, 0.8998911860718172]
2024-08-11 23:27:36,651 [trainer.py] => All params: 109136611
2024-08-11 23:27:36,653 [trainer.py] => Trainable params: 89108
2024-08-11 23:27:36,653 [inflora.py] => Learning on 80-100
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_v.4.weight', 'image_encoder.blocks.8.attn.lora_B_k.4.weight', 'image_encoder.blocks.4.attn.lora_B_v.4.weight', 'image_encoder.blocks.5.attn.lora_B_k.4.weight', 'classifier_pool.4.weight', 'image_encoder.blocks.1.attn.lora_B_k.4.weight', 'image_encoder.blocks.0.attn.lora_B_v.4.weight', 'image_encoder.blocks.1.attn.lora_B_v.4.weight', 'image_encoder.blocks.7.attn.lora_B_k.4.weight', 'image_encoder.blocks.10.attn.lora_B_k.4.weight', 'image_encoder.blocks.11.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_v.4.weight', 'image_encoder.blocks.2.attn.lora_B_v.4.weight', 'image_encoder.blocks.7.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_v.4.weight', 'image_encoder.blocks.11.attn.lora_B_k.4.weight', 'image_encoder.blocks.5.attn.lora_B_v.4.weight', 'image_encoder.blocks.8.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.4.weight', 'image_encoder.blocks.2.attn.lora_B_k.4.weight', 'image_encoder.blocks.0.attn.lora_B_k.4.weight', 'classifier_pool.4.bias', 'image_encoder.blocks.6.attn.lora_B_k.4.weight', 'image_encoder.blocks.4.attn.lora_B_k.4.weight', 'image_encoder.blocks.6.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_k.4.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 4, Epoch 1/20 => Loss 1.504, Train_accy 56.89:   0%|          | 0/20 [00:03<?, ?it/s]
Task 4, Epoch 1/20 => Loss 1.504, Train_accy 56.89:   5%|▌         | 1/20 [00:03<01:15,  3.99s/it]
Task 4, Epoch 2/20 => Loss 0.368, Train_accy 88.18:   5%|▌         | 1/20 [00:08<01:15,  3.99s/it]
Task 4, Epoch 2/20 => Loss 0.368, Train_accy 88.18:  10%|█         | 2/20 [00:08<01:12,  4.01s/it]
Task 4, Epoch 3/20 => Loss 0.279, Train_accy 90.48:  10%|█         | 2/20 [00:12<01:12,  4.01s/it]
Task 4, Epoch 3/20 => Loss 0.279, Train_accy 90.48:  15%|█▌        | 3/20 [00:12<01:08,  4.00s/it]
Task 4, Epoch 4/20 => Loss 0.246, Train_accy 92.01:  15%|█▌        | 3/20 [00:16<01:08,  4.00s/it]
Task 4, Epoch 4/20 => Loss 0.246, Train_accy 92.01:  20%|██        | 4/20 [00:16<01:04,  4.05s/it]
Task 4, Epoch 5/20 => Loss 0.239, Train_accy 92.56:  20%|██        | 4/20 [00:20<01:04,  4.05s/it]
Task 4, Epoch 5/20 => Loss 0.239, Train_accy 92.56:  25%|██▌       | 5/20 [00:20<01:00,  4.04s/it]
Task 4, Epoch 6/20 => Loss 0.333, Train_accy 93.98:  25%|██▌       | 5/20 [00:24<01:00,  4.04s/it]
Task 4, Epoch 6/20 => Loss 0.333, Train_accy 93.98:  30%|███       | 6/20 [00:24<00:56,  4.03s/it]
Task 4, Epoch 7/20 => Loss 0.199, Train_accy 93.33:  30%|███       | 6/20 [00:28<00:56,  4.03s/it]
Task 4, Epoch 7/20 => Loss 0.199, Train_accy 93.33:  35%|███▌      | 7/20 [00:28<00:52,  4.02s/it]
Task 4, Epoch 8/20 => Loss 0.235, Train_accy 93.00:  35%|███▌      | 7/20 [00:32<00:52,  4.02s/it]
Task 4, Epoch 8/20 => Loss 0.235, Train_accy 93.00:  40%|████      | 8/20 [00:32<00:48,  4.02s/it]
Task 4, Epoch 9/20 => Loss 0.171, Train_accy 94.42:  40%|████      | 8/20 [00:36<00:48,  4.02s/it]
Task 4, Epoch 9/20 => Loss 0.171, Train_accy 94.42:  45%|████▌     | 9/20 [00:36<00:44,  4.02s/it]
Task 4, Epoch 10/20 => Loss 0.174, Train_accy 94.09:  45%|████▌     | 9/20 [00:40<00:44,  4.02s/it]
Task 4, Epoch 10/20 => Loss 0.174, Train_accy 94.09:  50%|█████     | 10/20 [00:40<00:40,  4.00s/it]
Task 4, Epoch 11/20 => Loss 0.159, Train_accy 95.51:  50%|█████     | 10/20 [00:44<00:40,  4.00s/it]
Task 4, Epoch 11/20 => Loss 0.159, Train_accy 95.51:  55%|█████▌    | 11/20 [00:44<00:36,  4.02s/it]
Task 4, Epoch 12/20 => Loss 0.305, Train_accy 93.87:  55%|█████▌    | 11/20 [00:48<00:36,  4.02s/it]
Task 4, Epoch 12/20 => Loss 0.305, Train_accy 93.87:  60%|██████    | 12/20 [00:48<00:32,  4.01s/it]
Task 4, Epoch 13/20 => Loss 0.161, Train_accy 94.20:  60%|██████    | 12/20 [00:52<00:32,  4.01s/it]
Task 4, Epoch 13/20 => Loss 0.161, Train_accy 94.20:  65%|██████▌   | 13/20 [00:52<00:28,  4.05s/it]
Task 4, Epoch 14/20 => Loss 0.163, Train_accy 93.87:  65%|██████▌   | 13/20 [00:56<00:28,  4.05s/it]
Task 4, Epoch 14/20 => Loss 0.163, Train_accy 93.87:  70%|███████   | 14/20 [00:56<00:24,  4.05s/it]
Task 4, Epoch 15/20 => Loss 0.208, Train_accy 94.31:  70%|███████   | 14/20 [01:00<00:24,  4.05s/it]
Task 4, Epoch 15/20 => Loss 0.208, Train_accy 94.31:  75%|███████▌  | 15/20 [01:00<00:20,  4.03s/it]
Task 4, Epoch 16/20 => Loss 0.145, Train_accy 95.84:  75%|███████▌  | 15/20 [01:04<00:20,  4.03s/it]
Task 4, Epoch 16/20 => Loss 0.145, Train_accy 95.84:  80%|████████  | 16/20 [01:04<00:16,  4.04s/it]
Task 4, Epoch 17/20 => Loss 0.133, Train_accy 95.84:  80%|████████  | 16/20 [01:08<00:16,  4.04s/it]
Task 4, Epoch 17/20 => Loss 0.133, Train_accy 95.84:  85%|████████▌ | 17/20 [01:08<00:12,  4.02s/it]
Task 4, Epoch 18/20 => Loss 0.126, Train_accy 96.28:  85%|████████▌ | 17/20 [01:12<00:12,  4.02s/it]
Task 4, Epoch 18/20 => Loss 0.126, Train_accy 96.28:  90%|█████████ | 18/20 [01:12<00:08,  4.05s/it]
Task 4, Epoch 19/20 => Loss 0.138, Train_accy 94.97:  90%|█████████ | 18/20 [01:16<00:08,  4.05s/it]
Task 4, Epoch 19/20 => Loss 0.138, Train_accy 94.97:  95%|█████████▌| 19/20 [01:16<00:04,  4.03s/it]
Task 4, Epoch 20/20 => Loss 0.144, Train_accy 95.30:  95%|█████████▌| 19/20 [01:20<00:04,  4.03s/it]
Task 4, Epoch 20/20 => Loss 0.144, Train_accy 95.30: 100%|██████████| 20/20 [01:20<00:00,  4.03s/it]
Task 4, Epoch 20/20 => Loss 0.144, Train_accy 95.30: 100%|██████████| 20/20 [01:20<00:00,  4.03s/it]
2024-08-11 23:29:02,428 [inflora.py] => Task 4, Epoch 20/20 => Loss 0.144, Train_accy 95.30
Threshold:  0.97
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 10/768 type remove
Layer 3 : 18/768 type remove
Layer 4 : 21/768 type remove
Layer 5 : 29/768 type remove
Layer 6 : 26/768 type remove
Layer 7 : 25/768 type remove
Layer 8 : 24/768 type remove
Layer 9 : 26/768 type remove
Layer 10 : 25/768 type remove
Layer 11 : 12/768 type remove
Layer 12 : 16/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 23:29:12,918 [trainer.py] => Time:96.2650785446167
1188 1188
1188 1188
2024-08-11 23:29:15,898 [trainer.py] => Time:2.979918956756592
2024-08-11 23:29:15,900 [inflora.py] => Exemplar size: 0
2024-08-11 23:29:15,900 [trainer.py] => CNN: {'total': 85.19, '00-19': 90.28, '20-39': 87.44, '40-59': 88.99, '60-79': 84.62, '80-99': 76.21, 'old': 87.81, 'new': 76.21}
2024-08-11 23:29:15,900 [trainer.py] => CNN top1 curve: [97.98, 94.93, 92.26, 89.12, 85.19]
2024-08-11 23:29:15,900 [trainer.py] => CNN top1 with task curve: [97.98, 97.8, 97.92, 97.93, 98.23]
2024-08-11 23:29:15,900 [trainer.py] => CNN top1 task curve: [1.0, 0.9625550660792952, 0.9300595238095238, 0.8998911860718172, 0.8577441077441077]
2024-08-11 23:29:16,402 [trainer.py] => All params: 109136611
2024-08-11 23:29:16,404 [trainer.py] => Trainable params: 89108
2024-08-11 23:29:16,404 [inflora.py] => Learning on 100-120
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.5.weight', 'image_encoder.blocks.7.attn.lora_B_v.5.weight', 'image_encoder.blocks.10.attn.lora_B_v.5.weight', 'image_encoder.blocks.11.attn.lora_B_k.5.weight', 'image_encoder.blocks.8.attn.lora_B_v.5.weight', 'image_encoder.blocks.6.attn.lora_B_k.5.weight', 'image_encoder.blocks.9.attn.lora_B_k.5.weight', 'image_encoder.blocks.4.attn.lora_B_k.5.weight', 'image_encoder.blocks.4.attn.lora_B_v.5.weight', 'image_encoder.blocks.5.attn.lora_B_v.5.weight', 'image_encoder.blocks.7.attn.lora_B_k.5.weight', 'image_encoder.blocks.5.attn.lora_B_k.5.weight', 'image_encoder.blocks.1.attn.lora_B_v.5.weight', 'image_encoder.blocks.9.attn.lora_B_v.5.weight', 'image_encoder.blocks.0.attn.lora_B_k.5.weight', 'image_encoder.blocks.3.attn.lora_B_v.5.weight', 'image_encoder.blocks.6.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_k.5.weight', 'image_encoder.blocks.11.attn.lora_B_v.5.weight', 'classifier_pool.5.bias', 'image_encoder.blocks.3.attn.lora_B_k.5.weight', 'classifier_pool.5.weight', 'image_encoder.blocks.2.attn.lora_B_k.5.weight', 'image_encoder.blocks.1.attn.lora_B_k.5.weight', 'image_encoder.blocks.0.attn.lora_B_v.5.weight', 'image_encoder.blocks.2.attn.lora_B_v.5.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 5, Epoch 1/20 => Loss 1.563, Train_accy 58.40:   0%|          | 0/20 [00:04<?, ?it/s]
Task 5, Epoch 1/20 => Loss 1.563, Train_accy 58.40:   5%|▌         | 1/20 [00:04<01:19,  4.18s/it]
Task 5, Epoch 2/20 => Loss 0.271, Train_accy 91.81:   5%|▌         | 1/20 [00:08<01:19,  4.18s/it]
Task 5, Epoch 2/20 => Loss 0.271, Train_accy 91.81:  10%|█         | 2/20 [00:08<01:14,  4.16s/it]
Task 5, Epoch 3/20 => Loss 0.195, Train_accy 94.22:  10%|█         | 2/20 [00:12<01:14,  4.16s/it]
Task 5, Epoch 3/20 => Loss 0.195, Train_accy 94.22:  15%|█▌        | 3/20 [00:12<01:10,  4.14s/it]
Task 5, Epoch 4/20 => Loss 0.151, Train_accy 96.22:  15%|█▌        | 3/20 [00:16<01:10,  4.14s/it]
Task 5, Epoch 4/20 => Loss 0.151, Train_accy 96.22:  20%|██        | 4/20 [00:16<01:06,  4.14s/it]
Task 5, Epoch 5/20 => Loss 0.135, Train_accy 95.06:  20%|██        | 4/20 [00:20<01:06,  4.14s/it]
Task 5, Epoch 5/20 => Loss 0.135, Train_accy 95.06:  25%|██▌       | 5/20 [00:20<01:02,  4.14s/it]
Task 5, Epoch 6/20 => Loss 0.127, Train_accy 96.22:  25%|██▌       | 5/20 [00:24<01:02,  4.14s/it]
Task 5, Epoch 6/20 => Loss 0.127, Train_accy 96.22:  30%|███       | 6/20 [00:24<00:58,  4.15s/it]
Task 5, Epoch 7/20 => Loss 0.159, Train_accy 94.54:  30%|███       | 6/20 [00:29<00:58,  4.15s/it]
Task 5, Epoch 7/20 => Loss 0.159, Train_accy 94.54:  35%|███▌      | 7/20 [00:29<00:54,  4.18s/it]
Task 5, Epoch 8/20 => Loss 0.155, Train_accy 95.17:  35%|███▌      | 7/20 [00:33<00:54,  4.18s/it]
Task 5, Epoch 8/20 => Loss 0.155, Train_accy 95.17:  40%|████      | 8/20 [00:33<00:50,  4.18s/it]
Task 5, Epoch 9/20 => Loss 0.130, Train_accy 95.80:  40%|████      | 8/20 [00:37<00:50,  4.18s/it]
Task 5, Epoch 9/20 => Loss 0.130, Train_accy 95.80:  45%|████▌     | 9/20 [00:37<00:45,  4.16s/it]
Task 5, Epoch 10/20 => Loss 0.133, Train_accy 96.01:  45%|████▌     | 9/20 [00:41<00:45,  4.16s/it]
Task 5, Epoch 10/20 => Loss 0.133, Train_accy 96.01:  50%|█████     | 10/20 [00:41<00:41,  4.17s/it]
Task 5, Epoch 11/20 => Loss 0.127, Train_accy 96.32:  50%|█████     | 10/20 [00:45<00:41,  4.17s/it]
Task 5, Epoch 11/20 => Loss 0.127, Train_accy 96.32:  55%|█████▌    | 11/20 [00:45<00:37,  4.16s/it]
Task 5, Epoch 12/20 => Loss 0.143, Train_accy 95.06:  55%|█████▌    | 11/20 [00:49<00:37,  4.16s/it]
Task 5, Epoch 12/20 => Loss 0.143, Train_accy 95.06:  60%|██████    | 12/20 [00:49<00:33,  4.18s/it]
Task 5, Epoch 13/20 => Loss 0.123, Train_accy 95.90:  60%|██████    | 12/20 [00:54<00:33,  4.18s/it]
Task 5, Epoch 13/20 => Loss 0.123, Train_accy 95.90:  65%|██████▌   | 13/20 [00:54<00:29,  4.19s/it]
Task 5, Epoch 14/20 => Loss 0.089, Train_accy 97.37:  65%|██████▌   | 13/20 [00:58<00:29,  4.19s/it]
Task 5, Epoch 14/20 => Loss 0.089, Train_accy 97.37:  70%|███████   | 14/20 [00:58<00:25,  4.18s/it]
Task 5, Epoch 15/20 => Loss 0.125, Train_accy 95.59:  70%|███████   | 14/20 [01:02<00:25,  4.18s/it]
Task 5, Epoch 15/20 => Loss 0.125, Train_accy 95.59:  75%|███████▌  | 15/20 [01:02<00:20,  4.17s/it]
Task 5, Epoch 16/20 => Loss 0.111, Train_accy 96.64:  75%|███████▌  | 15/20 [01:06<00:20,  4.17s/it]
Task 5, Epoch 16/20 => Loss 0.111, Train_accy 96.64:  80%|████████  | 16/20 [01:06<00:16,  4.17s/it]
Task 5, Epoch 17/20 => Loss 0.120, Train_accy 96.32:  80%|████████  | 16/20 [01:10<00:16,  4.17s/it]
Task 5, Epoch 17/20 => Loss 0.120, Train_accy 96.32:  85%|████████▌ | 17/20 [01:10<00:12,  4.17s/it]
Task 5, Epoch 18/20 => Loss 0.132, Train_accy 95.90:  85%|████████▌ | 17/20 [01:15<00:12,  4.17s/it]
Task 5, Epoch 18/20 => Loss 0.132, Train_accy 95.90:  90%|█████████ | 18/20 [01:15<00:08,  4.17s/it]
Task 5, Epoch 19/20 => Loss 0.115, Train_accy 96.43:  90%|█████████ | 18/20 [01:19<00:08,  4.17s/it]
Task 5, Epoch 19/20 => Loss 0.115, Train_accy 96.43:  95%|█████████▌| 19/20 [01:19<00:04,  4.17s/it]
Task 5, Epoch 20/20 => Loss 0.130, Train_accy 96.11:  95%|█████████▌| 19/20 [01:23<00:04,  4.17s/it]
Task 5, Epoch 20/20 => Loss 0.130, Train_accy 96.11: 100%|██████████| 20/20 [01:23<00:00,  4.16s/it]
Task 5, Epoch 20/20 => Loss 0.130, Train_accy 96.11: 100%|██████████| 20/20 [01:23<00:00,  4.17s/it]
2024-08-11 23:30:45,129 [inflora.py] => Task 5, Epoch 20/20 => Loss 0.130, Train_accy 96.11
Threshold:  0.975
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 11/768 type remove
Layer 3 : 22/768 type remove
Layer 4 : 24/768 type remove
Layer 5 : 34/768 type remove
Layer 6 : 30/768 type remove
Layer 7 : 31/768 type remove
Layer 8 : 31/768 type remove
Layer 9 : 32/768 type remove
Layer 10 : 31/768 type remove
Layer 11 : 16/768 type remove
Layer 12 : 20/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 23:30:56,030 [trainer.py] => Time:99.62590169906616
1416 1416
1416 1416
2024-08-11 23:30:59,463 [trainer.py] => Time:3.4325673580169678
2024-08-11 23:30:59,464 [inflora.py] => Exemplar size: 0
2024-08-11 23:30:59,464 [trainer.py] => CNN: {'total': 81.29, '00-19': 90.28, '20-39': 86.47, '40-59': 85.78, '60-79': 80.16, '80-99': 73.23, '100-119': 73.25, 'old': 82.83, 'new': 73.25}
2024-08-11 23:30:59,465 [trainer.py] => CNN top1 curve: [97.98, 94.93, 92.26, 89.12, 85.19, 81.29]
2024-08-11 23:30:59,465 [trainer.py] => CNN top1 with task curve: [97.98, 97.8, 97.92, 97.93, 98.23, 98.16]
2024-08-11 23:30:59,465 [trainer.py] => CNN top1 task curve: [1.0, 0.9625550660792952, 0.9300595238095238, 0.8998911860718172, 0.8577441077441077, 0.818502824858757]
2024-08-11 23:30:59,925 [trainer.py] => All params: 109136611
2024-08-11 23:30:59,926 [trainer.py] => Trainable params: 89108
2024-08-11 23:30:59,927 [inflora.py] => Learning on 120-140
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.6.weight', 'image_encoder.blocks.0.attn.lora_B_v.6.weight', 'image_encoder.blocks.10.attn.lora_B_v.6.weight', 'image_encoder.blocks.1.attn.lora_B_k.6.weight', 'image_encoder.blocks.5.attn.lora_B_k.6.weight', 'image_encoder.blocks.11.attn.lora_B_k.6.weight', 'image_encoder.blocks.7.attn.lora_B_v.6.weight', 'classifier_pool.6.weight', 'image_encoder.blocks.9.attn.lora_B_k.6.weight', 'image_encoder.blocks.2.attn.lora_B_k.6.weight', 'image_encoder.blocks.6.attn.lora_B_v.6.weight', 'image_encoder.blocks.2.attn.lora_B_v.6.weight', 'image_encoder.blocks.10.attn.lora_B_k.6.weight', 'classifier_pool.6.bias', 'image_encoder.blocks.4.attn.lora_B_k.6.weight', 'image_encoder.blocks.8.attn.lora_B_k.6.weight', 'image_encoder.blocks.11.attn.lora_B_v.6.weight', 'image_encoder.blocks.0.attn.lora_B_k.6.weight', 'image_encoder.blocks.3.attn.lora_B_v.6.weight', 'image_encoder.blocks.7.attn.lora_B_k.6.weight', 'image_encoder.blocks.5.attn.lora_B_v.6.weight', 'image_encoder.blocks.3.attn.lora_B_k.6.weight', 'image_encoder.blocks.4.attn.lora_B_v.6.weight', 'image_encoder.blocks.8.attn.lora_B_v.6.weight', 'image_encoder.blocks.6.attn.lora_B_k.6.weight', 'image_encoder.blocks.1.attn.lora_B_v.6.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 6, Epoch 1/20 => Loss 1.781, Train_accy 48.95:   0%|          | 0/20 [00:04<?, ?it/s]
Task 6, Epoch 1/20 => Loss 1.781, Train_accy 48.95:   5%|▌         | 1/20 [00:04<01:18,  4.14s/it]
Task 6, Epoch 2/20 => Loss 0.553, Train_accy 81.55:   5%|▌         | 1/20 [00:08<01:18,  4.14s/it]
Task 6, Epoch 2/20 => Loss 0.553, Train_accy 81.55:  10%|█         | 2/20 [00:08<01:16,  4.22s/it]
Task 6, Epoch 3/20 => Loss 0.427, Train_accy 84.38:  10%|█         | 2/20 [00:12<01:16,  4.22s/it]
Task 6, Epoch 3/20 => Loss 0.427, Train_accy 84.38:  15%|█▌        | 3/20 [00:12<01:11,  4.20s/it]
Task 6, Epoch 4/20 => Loss 0.346, Train_accy 87.74:  15%|█▌        | 3/20 [00:16<01:11,  4.20s/it]
Task 6, Epoch 4/20 => Loss 0.346, Train_accy 87.74:  20%|██        | 4/20 [00:16<01:07,  4.25s/it]
Task 6, Epoch 5/20 => Loss 0.312, Train_accy 88.99:  20%|██        | 4/20 [00:21<01:07,  4.25s/it]
Task 6, Epoch 5/20 => Loss 0.312, Train_accy 88.99:  25%|██▌       | 5/20 [00:21<01:03,  4.24s/it]
Task 6, Epoch 6/20 => Loss 0.307, Train_accy 88.78:  25%|██▌       | 5/20 [00:25<01:03,  4.24s/it]
Task 6, Epoch 6/20 => Loss 0.307, Train_accy 88.78:  30%|███       | 6/20 [00:25<00:59,  4.23s/it]
Task 6, Epoch 7/20 => Loss 0.240, Train_accy 92.35:  30%|███       | 6/20 [00:29<00:59,  4.23s/it]
Task 6, Epoch 7/20 => Loss 0.240, Train_accy 92.35:  35%|███▌      | 7/20 [00:29<00:54,  4.22s/it]
Task 6, Epoch 8/20 => Loss 0.269, Train_accy 90.88:  35%|███▌      | 7/20 [00:33<00:54,  4.22s/it]
Task 6, Epoch 8/20 => Loss 0.269, Train_accy 90.88:  40%|████      | 8/20 [00:33<00:50,  4.21s/it]
Task 6, Epoch 9/20 => Loss 0.191, Train_accy 94.44:  40%|████      | 8/20 [00:37<00:50,  4.21s/it]
Task 6, Epoch 9/20 => Loss 0.191, Train_accy 94.44:  45%|████▌     | 9/20 [00:37<00:46,  4.22s/it]
Task 6, Epoch 10/20 => Loss 0.257, Train_accy 91.61:  45%|████▌     | 9/20 [00:42<00:46,  4.22s/it]
Task 6, Epoch 10/20 => Loss 0.257, Train_accy 91.61:  50%|█████     | 10/20 [00:42<00:42,  4.22s/it]
Task 6, Epoch 11/20 => Loss 0.234, Train_accy 92.77:  50%|█████     | 10/20 [00:46<00:42,  4.22s/it]
Task 6, Epoch 11/20 => Loss 0.234, Train_accy 92.77:  55%|█████▌    | 11/20 [00:46<00:37,  4.21s/it]
Task 6, Epoch 12/20 => Loss 0.228, Train_accy 92.56:  55%|█████▌    | 11/20 [00:50<00:37,  4.21s/it]
Task 6, Epoch 12/20 => Loss 0.228, Train_accy 92.56:  60%|██████    | 12/20 [00:50<00:33,  4.21s/it]
Task 6, Epoch 13/20 => Loss 0.221, Train_accy 92.87:  60%|██████    | 12/20 [00:54<00:33,  4.21s/it]
Task 6, Epoch 13/20 => Loss 0.221, Train_accy 92.87:  65%|██████▌   | 13/20 [00:54<00:29,  4.20s/it]
Task 6, Epoch 14/20 => Loss 0.193, Train_accy 94.55:  65%|██████▌   | 13/20 [00:59<00:29,  4.20s/it]
Task 6, Epoch 14/20 => Loss 0.193, Train_accy 94.55:  70%|███████   | 14/20 [00:59<00:25,  4.23s/it]
Task 6, Epoch 15/20 => Loss 0.213, Train_accy 92.56:  70%|███████   | 14/20 [01:03<00:25,  4.23s/it]
Task 6, Epoch 15/20 => Loss 0.213, Train_accy 92.56:  75%|███████▌  | 15/20 [01:03<00:21,  4.22s/it]
Task 6, Epoch 16/20 => Loss 0.223, Train_accy 92.14:  75%|███████▌  | 15/20 [01:07<00:21,  4.22s/it]
Task 6, Epoch 16/20 => Loss 0.223, Train_accy 92.14:  80%|████████  | 16/20 [01:07<00:16,  4.23s/it]
Task 6, Epoch 17/20 => Loss 0.183, Train_accy 94.34:  80%|████████  | 16/20 [01:11<00:16,  4.23s/it]
Task 6, Epoch 17/20 => Loss 0.183, Train_accy 94.34:  85%|████████▌ | 17/20 [01:11<00:12,  4.22s/it]
Task 6, Epoch 18/20 => Loss 0.181, Train_accy 94.65:  85%|████████▌ | 17/20 [01:15<00:12,  4.22s/it]
Task 6, Epoch 18/20 => Loss 0.181, Train_accy 94.65:  90%|█████████ | 18/20 [01:15<00:08,  4.21s/it]
Task 6, Epoch 19/20 => Loss 0.180, Train_accy 94.44:  90%|█████████ | 18/20 [01:20<00:08,  4.21s/it]
Task 6, Epoch 19/20 => Loss 0.180, Train_accy 94.44:  95%|█████████▌| 19/20 [01:20<00:04,  4.22s/it]
Task 6, Epoch 20/20 => Loss 0.164, Train_accy 94.65:  95%|█████████▌| 19/20 [01:24<00:04,  4.22s/it]
Task 6, Epoch 20/20 => Loss 0.164, Train_accy 94.65: 100%|██████████| 20/20 [01:24<00:00,  4.21s/it]
Task 6, Epoch 20/20 => Loss 0.164, Train_accy 94.65: 100%|██████████| 20/20 [01:24<00:00,  4.22s/it]
2024-08-11 23:32:29,504 [inflora.py] => Task 6, Epoch 20/20 => Loss 0.164, Train_accy 94.65
Threshold:  0.98
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 26/768 type remove
Layer 4 : 31/768 type remove
Layer 5 : 43/768 type remove
Layer 6 : 40/768 type remove
Layer 7 : 39/768 type remove
Layer 8 : 39/768 type remove
Layer 9 : 40/768 type remove
Layer 10 : 40/768 type remove
Layer 11 : 21/768 type remove
Layer 12 : 23/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 23:32:40,371 [trainer.py] => Time:100.44396591186523
1641 1641
1641 1641
2024-08-11 23:32:44,160 [trainer.py] => Time:3.789114236831665
2024-08-11 23:32:44,160 [inflora.py] => Exemplar size: 0
2024-08-11 23:32:44,160 [trainer.py] => CNN: {'total': 79.4, '00-19': 89.47, '20-39': 85.99, '40-59': 85.78, '60-79': 81.78, '80-99': 66.54, '100-119': 72.37, '120-139': 76.0, 'old': 79.94, 'new': 76.0}
2024-08-11 23:32:44,160 [trainer.py] => CNN top1 curve: [97.98, 94.93, 92.26, 89.12, 85.19, 81.29, 79.4]
2024-08-11 23:32:44,160 [trainer.py] => CNN top1 with task curve: [97.98, 97.8, 97.92, 97.93, 98.23, 98.16, 97.87]
2024-08-11 23:32:44,160 [trainer.py] => CNN top1 task curve: [1.0, 0.9625550660792952, 0.9300595238095238, 0.8998911860718172, 0.8577441077441077, 0.818502824858757, 0.8013406459475929]
2024-08-11 23:32:44,638 [trainer.py] => All params: 109136611
2024-08-11 23:32:44,640 [trainer.py] => Trainable params: 89108
2024-08-11 23:32:44,640 [inflora.py] => Learning on 140-160
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.7.weight', 'image_encoder.blocks.8.attn.lora_B_v.7.weight', 'image_encoder.blocks.11.attn.lora_B_k.7.weight', 'image_encoder.blocks.1.attn.lora_B_k.7.weight', 'image_encoder.blocks.4.attn.lora_B_k.7.weight', 'image_encoder.blocks.11.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_k.7.weight', 'image_encoder.blocks.4.attn.lora_B_v.7.weight', 'image_encoder.blocks.1.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_v.7.weight', 'classifier_pool.7.bias', 'image_encoder.blocks.9.attn.lora_B_k.7.weight', 'image_encoder.blocks.0.attn.lora_B_v.7.weight', 'image_encoder.blocks.2.attn.lora_B_v.7.weight', 'image_encoder.blocks.2.attn.lora_B_k.7.weight', 'image_encoder.blocks.9.attn.lora_B_v.7.weight', 'image_encoder.blocks.10.attn.lora_B_k.7.weight', 'image_encoder.blocks.7.attn.lora_B_v.7.weight', 'image_encoder.blocks.3.attn.lora_B_k.7.weight', 'image_encoder.blocks.8.attn.lora_B_k.7.weight', 'image_encoder.blocks.6.attn.lora_B_v.7.weight', 'image_encoder.blocks.10.attn.lora_B_v.7.weight', 'image_encoder.blocks.7.attn.lora_B_k.7.weight', 'image_encoder.blocks.0.attn.lora_B_k.7.weight', 'image_encoder.blocks.6.attn.lora_B_k.7.weight', 'classifier_pool.7.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 7, Epoch 1/20 => Loss 1.712, Train_accy 51.52:   0%|          | 0/20 [00:04<?, ?it/s]
Task 7, Epoch 1/20 => Loss 1.712, Train_accy 51.52:   5%|▌         | 1/20 [00:04<01:17,  4.09s/it]
Task 7, Epoch 2/20 => Loss 0.343, Train_accy 88.70:   5%|▌         | 1/20 [00:08<01:17,  4.09s/it]
Task 7, Epoch 2/20 => Loss 0.343, Train_accy 88.70:  10%|█         | 2/20 [00:08<01:13,  4.08s/it]
Task 7, Epoch 3/20 => Loss 0.331, Train_accy 89.78:  10%|█         | 2/20 [00:12<01:13,  4.08s/it]
Task 7, Epoch 3/20 => Loss 0.331, Train_accy 89.78:  15%|█▌        | 3/20 [00:12<01:09,  4.10s/it]
Task 7, Epoch 4/20 => Loss 0.303, Train_accy 91.41:  15%|█▌        | 3/20 [00:16<01:09,  4.10s/it]
Task 7, Epoch 4/20 => Loss 0.303, Train_accy 91.41:  20%|██        | 4/20 [00:16<01:06,  4.14s/it]
Task 7, Epoch 5/20 => Loss 0.285, Train_accy 91.41:  20%|██        | 4/20 [00:20<01:06,  4.14s/it]
Task 7, Epoch 5/20 => Loss 0.285, Train_accy 91.41:  25%|██▌       | 5/20 [00:20<01:01,  4.13s/it]
Task 7, Epoch 6/20 => Loss 0.265, Train_accy 91.96:  25%|██▌       | 5/20 [00:24<01:01,  4.13s/it]
Task 7, Epoch 6/20 => Loss 0.265, Train_accy 91.96:  30%|███       | 6/20 [00:24<00:57,  4.14s/it]
Task 7, Epoch 7/20 => Loss 0.189, Train_accy 94.67:  30%|███       | 6/20 [00:28<00:57,  4.14s/it]
Task 7, Epoch 7/20 => Loss 0.189, Train_accy 94.67:  35%|███▌      | 7/20 [00:28<00:53,  4.15s/it]
Task 7, Epoch 8/20 => Loss 0.243, Train_accy 92.28:  35%|███▌      | 7/20 [00:33<00:53,  4.15s/it]
Task 7, Epoch 8/20 => Loss 0.243, Train_accy 92.28:  40%|████      | 8/20 [00:33<00:49,  4.14s/it]
Task 7, Epoch 9/20 => Loss 0.209, Train_accy 95.11:  40%|████      | 8/20 [00:37<00:49,  4.14s/it]
Task 7, Epoch 9/20 => Loss 0.209, Train_accy 95.11:  45%|████▌     | 9/20 [00:37<00:45,  4.15s/it]
Task 7, Epoch 10/20 => Loss 0.199, Train_accy 93.37:  45%|████▌     | 9/20 [00:41<00:45,  4.15s/it]
Task 7, Epoch 10/20 => Loss 0.199, Train_accy 93.37:  50%|█████     | 10/20 [00:41<00:41,  4.14s/it]
Task 7, Epoch 11/20 => Loss 0.186, Train_accy 94.46:  50%|█████     | 10/20 [00:45<00:41,  4.14s/it]
Task 7, Epoch 11/20 => Loss 0.186, Train_accy 94.46:  55%|█████▌    | 11/20 [00:45<00:37,  4.12s/it]
Task 7, Epoch 12/20 => Loss 0.227, Train_accy 94.02:  55%|█████▌    | 11/20 [00:49<00:37,  4.12s/it]
Task 7, Epoch 12/20 => Loss 0.227, Train_accy 94.02:  60%|██████    | 12/20 [00:49<00:33,  4.13s/it]
Task 7, Epoch 13/20 => Loss 0.143, Train_accy 95.76:  60%|██████    | 12/20 [00:53<00:33,  4.13s/it]
Task 7, Epoch 13/20 => Loss 0.143, Train_accy 95.76:  65%|██████▌   | 13/20 [00:53<00:28,  4.12s/it]
Task 7, Epoch 14/20 => Loss 0.185, Train_accy 94.67:  65%|██████▌   | 13/20 [00:57<00:28,  4.12s/it]
Task 7, Epoch 14/20 => Loss 0.185, Train_accy 94.67:  70%|███████   | 14/20 [00:57<00:24,  4.12s/it]
Task 7, Epoch 15/20 => Loss 0.191, Train_accy 94.13:  70%|███████   | 14/20 [01:01<00:24,  4.12s/it]
Task 7, Epoch 15/20 => Loss 0.191, Train_accy 94.13:  75%|███████▌  | 15/20 [01:01<00:20,  4.11s/it]
Task 7, Epoch 16/20 => Loss 0.168, Train_accy 95.33:  75%|███████▌  | 15/20 [01:05<00:20,  4.11s/it]
Task 7, Epoch 16/20 => Loss 0.168, Train_accy 95.33:  80%|████████  | 16/20 [01:05<00:16,  4.11s/it]
Task 7, Epoch 17/20 => Loss 0.147, Train_accy 95.76:  80%|████████  | 16/20 [01:10<00:16,  4.11s/it]
Task 7, Epoch 17/20 => Loss 0.147, Train_accy 95.76:  85%|████████▌ | 17/20 [01:10<00:12,  4.11s/it]
Task 7, Epoch 18/20 => Loss 0.175, Train_accy 96.09:  85%|████████▌ | 17/20 [01:14<00:12,  4.11s/it]
Task 7, Epoch 18/20 => Loss 0.175, Train_accy 96.09:  90%|█████████ | 18/20 [01:14<00:08,  4.10s/it]
Task 7, Epoch 19/20 => Loss 0.143, Train_accy 95.43:  90%|█████████ | 18/20 [01:18<00:08,  4.10s/it]
Task 7, Epoch 19/20 => Loss 0.143, Train_accy 95.43:  95%|█████████▌| 19/20 [01:18<00:04,  4.13s/it]
Task 7, Epoch 20/20 => Loss 0.161, Train_accy 96.09:  95%|█████████▌| 19/20 [01:22<00:04,  4.13s/it]
Task 7, Epoch 20/20 => Loss 0.161, Train_accy 96.09: 100%|██████████| 20/20 [01:22<00:00,  4.12s/it]
Task 7, Epoch 20/20 => Loss 0.161, Train_accy 96.09: 100%|██████████| 20/20 [01:22<00:00,  4.12s/it]
2024-08-11 23:34:12,337 [inflora.py] => Task 7, Epoch 20/20 => Loss 0.161, Train_accy 96.09
Threshold:  0.985
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 32/768 type remove
Layer 4 : 40/768 type remove
Layer 5 : 57/768 type remove
Layer 6 : 53/768 type remove
Layer 7 : 52/768 type remove
Layer 8 : 54/768 type remove
Layer 9 : 56/768 type remove
Layer 10 : 55/768 type remove
Layer 11 : 32/768 type remove
Layer 12 : 29/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 23:34:22,997 [trainer.py] => Time:98.35616970062256
1891 1891
1891 1891
2024-08-11 23:34:27,219 [trainer.py] => Time:4.222375392913818
2024-08-11 23:34:27,220 [inflora.py] => Exemplar size: 0
2024-08-11 23:34:27,220 [trainer.py] => CNN: {'total': 79.16, '00-19': 87.85, '20-39': 83.09, '40-59': 83.49, '60-79': 78.54, '80-99': 68.4, '100-119': 75.0, '120-139': 76.44, '140-159': 82.0, 'old': 78.73, 'new': 82.0}
2024-08-11 23:34:27,220 [trainer.py] => CNN top1 curve: [97.98, 94.93, 92.26, 89.12, 85.19, 81.29, 79.4, 79.16]
2024-08-11 23:34:27,220 [trainer.py] => CNN top1 with task curve: [97.98, 97.8, 97.92, 97.93, 98.23, 98.16, 97.87, 98.1]
2024-08-11 23:34:27,220 [trainer.py] => CNN top1 task curve: [1.0, 0.9625550660792952, 0.9300595238095238, 0.8998911860718172, 0.8577441077441077, 0.818502824858757, 0.8013406459475929, 0.7974616604970914]
2024-08-11 23:34:27,698 [trainer.py] => All params: 109136611
2024-08-11 23:34:27,700 [trainer.py] => Trainable params: 89108
2024-08-11 23:34:27,700 [inflora.py] => Learning on 160-180
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.8.weight', 'image_encoder.blocks.4.attn.lora_B_v.8.weight', 'image_encoder.blocks.10.attn.lora_B_k.8.weight', 'image_encoder.blocks.2.attn.lora_B_k.8.weight', 'image_encoder.blocks.8.attn.lora_B_k.8.weight', 'image_encoder.blocks.9.attn.lora_B_k.8.weight', 'image_encoder.blocks.3.attn.lora_B_k.8.weight', 'image_encoder.blocks.0.attn.lora_B_v.8.weight', 'image_encoder.blocks.7.attn.lora_B_k.8.weight', 'image_encoder.blocks.6.attn.lora_B_k.8.weight', 'image_encoder.blocks.11.attn.lora_B_v.8.weight', 'image_encoder.blocks.11.attn.lora_B_k.8.weight', 'image_encoder.blocks.7.attn.lora_B_v.8.weight', 'image_encoder.blocks.2.attn.lora_B_v.8.weight', 'image_encoder.blocks.5.attn.lora_B_v.8.weight', 'image_encoder.blocks.4.attn.lora_B_k.8.weight', 'image_encoder.blocks.1.attn.lora_B_v.8.weight', 'image_encoder.blocks.0.attn.lora_B_k.8.weight', 'image_encoder.blocks.5.attn.lora_B_k.8.weight', 'image_encoder.blocks.1.attn.lora_B_k.8.weight', 'classifier_pool.8.bias', 'image_encoder.blocks.3.attn.lora_B_v.8.weight', 'image_encoder.blocks.8.attn.lora_B_v.8.weight', 'classifier_pool.8.weight', 'image_encoder.blocks.10.attn.lora_B_v.8.weight', 'image_encoder.blocks.9.attn.lora_B_v.8.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 8, Epoch 1/20 => Loss 1.685, Train_accy 55.22:   0%|          | 0/20 [00:04<?, ?it/s]
Task 8, Epoch 1/20 => Loss 1.685, Train_accy 55.22:   5%|▌         | 1/20 [00:04<01:19,  4.20s/it]
Task 8, Epoch 2/20 => Loss 0.382, Train_accy 89.67:   5%|▌         | 1/20 [00:08<01:19,  4.20s/it]
Task 8, Epoch 2/20 => Loss 0.382, Train_accy 89.67:  10%|█         | 2/20 [00:08<01:15,  4.22s/it]
Task 8, Epoch 3/20 => Loss 0.299, Train_accy 91.65:  10%|█         | 2/20 [00:12<01:15,  4.22s/it]
Task 8, Epoch 3/20 => Loss 0.299, Train_accy 91.65:  15%|█▌        | 3/20 [00:12<01:11,  4.23s/it]
Task 8, Epoch 4/20 => Loss 0.243, Train_accy 93.01:  15%|█▌        | 3/20 [00:17<01:11,  4.23s/it]
Task 8, Epoch 4/20 => Loss 0.243, Train_accy 93.01:  20%|██        | 4/20 [00:17<01:08,  4.27s/it]
Task 8, Epoch 5/20 => Loss 0.246, Train_accy 92.28:  20%|██        | 4/20 [00:21<01:08,  4.27s/it]
Task 8, Epoch 5/20 => Loss 0.246, Train_accy 92.28:  25%|██▌       | 5/20 [00:21<01:03,  4.27s/it]
Task 8, Epoch 6/20 => Loss 0.214, Train_accy 94.26:  25%|██▌       | 5/20 [00:25<01:03,  4.27s/it]
Task 8, Epoch 6/20 => Loss 0.214, Train_accy 94.26:  30%|███       | 6/20 [00:25<00:59,  4.26s/it]
Task 8, Epoch 7/20 => Loss 0.210, Train_accy 93.74:  30%|███       | 6/20 [00:29<00:59,  4.26s/it]
Task 8, Epoch 7/20 => Loss 0.210, Train_accy 93.74:  35%|███▌      | 7/20 [00:29<00:55,  4.28s/it]
Task 8, Epoch 8/20 => Loss 0.213, Train_accy 93.63:  35%|███▌      | 7/20 [00:34<00:55,  4.28s/it]
Task 8, Epoch 8/20 => Loss 0.213, Train_accy 93.63:  40%|████      | 8/20 [00:34<00:51,  4.28s/it]
Task 8, Epoch 9/20 => Loss 0.155, Train_accy 95.20:  40%|████      | 8/20 [00:38<00:51,  4.28s/it]
Task 8, Epoch 9/20 => Loss 0.155, Train_accy 95.20:  45%|████▌     | 9/20 [00:38<00:46,  4.27s/it]
Task 8, Epoch 10/20 => Loss 0.176, Train_accy 94.68:  45%|████▌     | 9/20 [00:42<00:46,  4.27s/it]
Task 8, Epoch 10/20 => Loss 0.176, Train_accy 94.68:  50%|█████     | 10/20 [00:42<00:42,  4.25s/it]
Task 8, Epoch 11/20 => Loss 0.196, Train_accy 93.95:  50%|█████     | 10/20 [00:46<00:42,  4.25s/it]
Task 8, Epoch 11/20 => Loss 0.196, Train_accy 93.95:  55%|█████▌    | 11/20 [00:46<00:38,  4.24s/it]
Task 8, Epoch 12/20 => Loss 0.199, Train_accy 93.84:  55%|█████▌    | 11/20 [00:51<00:38,  4.24s/it]
Task 8, Epoch 12/20 => Loss 0.199, Train_accy 93.84:  60%|██████    | 12/20 [00:51<00:33,  4.24s/it]
Task 8, Epoch 13/20 => Loss 0.144, Train_accy 95.82:  60%|██████    | 12/20 [00:55<00:33,  4.24s/it]
Task 8, Epoch 13/20 => Loss 0.144, Train_accy 95.82:  65%|██████▌   | 13/20 [00:55<00:29,  4.28s/it]
Task 8, Epoch 14/20 => Loss 0.173, Train_accy 95.20:  65%|██████▌   | 13/20 [00:59<00:29,  4.28s/it]
Task 8, Epoch 14/20 => Loss 0.173, Train_accy 95.20:  70%|███████   | 14/20 [00:59<00:25,  4.27s/it]
Task 8, Epoch 15/20 => Loss 0.173, Train_accy 93.95:  70%|███████   | 14/20 [01:03<00:25,  4.27s/it]
Task 8, Epoch 15/20 => Loss 0.173, Train_accy 93.95:  75%|███████▌  | 15/20 [01:03<00:21,  4.26s/it]
Task 8, Epoch 16/20 => Loss 0.176, Train_accy 94.57:  75%|███████▌  | 15/20 [01:08<00:21,  4.26s/it]
Task 8, Epoch 16/20 => Loss 0.176, Train_accy 94.57:  80%|████████  | 16/20 [01:08<00:17,  4.25s/it]
Task 8, Epoch 17/20 => Loss 0.174, Train_accy 94.89:  80%|████████  | 16/20 [01:12<00:17,  4.25s/it]
Task 8, Epoch 17/20 => Loss 0.174, Train_accy 94.89:  85%|████████▌ | 17/20 [01:12<00:12,  4.25s/it]
Task 8, Epoch 18/20 => Loss 0.197, Train_accy 94.05:  85%|████████▌ | 17/20 [01:16<00:12,  4.25s/it]
Task 8, Epoch 18/20 => Loss 0.197, Train_accy 94.05:  90%|█████████ | 18/20 [01:16<00:08,  4.24s/it]
Task 8, Epoch 19/20 => Loss 0.142, Train_accy 95.82:  90%|█████████ | 18/20 [01:20<00:08,  4.24s/it]
Task 8, Epoch 19/20 => Loss 0.142, Train_accy 95.82:  95%|█████████▌| 19/20 [01:20<00:04,  4.26s/it]
Task 8, Epoch 20/20 => Loss 0.133, Train_accy 96.03:  95%|█████████▌| 19/20 [01:25<00:04,  4.26s/it]
Task 8, Epoch 20/20 => Loss 0.133, Train_accy 96.03: 100%|██████████| 20/20 [01:25<00:00,  4.25s/it]
Task 8, Epoch 20/20 => Loss 0.133, Train_accy 96.03: 100%|██████████| 20/20 [01:25<00:00,  4.26s/it]
2024-08-11 23:35:59,250 [inflora.py] => Task 8, Epoch 20/20 => Loss 0.133, Train_accy 96.03
Threshold:  0.99
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 43/768 type remove
Layer 4 : 56/768 type remove
Layer 5 : 76/768 type remove
Layer 6 : 71/768 type remove
Layer 7 : 71/768 type remove
Layer 8 : 73/768 type remove
Layer 9 : 77/768 type remove
Layer 10 : 77/768 type remove
Layer 11 : 46/768 type remove
Layer 12 : 37/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 23:36:10,605 [trainer.py] => Time:102.90437388420105
2128 2128
2128 2128
2024-08-11 23:36:15,240 [trainer.py] => Time:4.635212182998657
2024-08-11 23:36:15,241 [inflora.py] => Exemplar size: 0
2024-08-11 23:36:15,241 [trainer.py] => CNN: {'total': 77.96, '00-19': 86.23, '20-39': 83.57, '40-59': 82.57, '60-79': 78.14, '80-99': 71.0, '100-119': 75.0, '120-139': 76.89, '140-159': 79.6, '160-179': 70.04, 'old': 78.95, 'new': 70.04}
2024-08-11 23:36:15,241 [trainer.py] => CNN top1 curve: [97.98, 94.93, 92.26, 89.12, 85.19, 81.29, 79.4, 79.16, 77.96]
2024-08-11 23:36:15,241 [trainer.py] => CNN top1 with task curve: [97.98, 97.8, 97.92, 97.93, 98.23, 98.16, 97.87, 98.1, 97.98]
2024-08-11 23:36:15,241 [trainer.py] => CNN top1 task curve: [1.0, 0.9625550660792952, 0.9300595238095238, 0.8998911860718172, 0.8577441077441077, 0.818502824858757, 0.8013406459475929, 0.7974616604970914, 0.7847744360902256]
2024-08-11 23:36:15,719 [trainer.py] => All params: 109136611
2024-08-11 23:36:15,721 [trainer.py] => Trainable params: 89108
2024-08-11 23:36:15,721 [inflora.py] => Learning on 180-200
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_k.9.weight', 'image_encoder.blocks.5.attn.lora_B_k.9.weight', 'image_encoder.blocks.10.attn.lora_B_k.9.weight', 'image_encoder.blocks.11.attn.lora_B_k.9.weight', 'image_encoder.blocks.8.attn.lora_B_k.9.weight', 'image_encoder.blocks.9.attn.lora_B_v.9.weight', 'image_encoder.blocks.4.attn.lora_B_v.9.weight', 'image_encoder.blocks.3.attn.lora_B_k.9.weight', 'image_encoder.blocks.9.attn.lora_B_k.9.weight', 'image_encoder.blocks.5.attn.lora_B_v.9.weight', 'image_encoder.blocks.4.attn.lora_B_k.9.weight', 'image_encoder.blocks.1.attn.lora_B_v.9.weight', 'image_encoder.blocks.1.attn.lora_B_k.9.weight', 'image_encoder.blocks.10.attn.lora_B_v.9.weight', 'image_encoder.blocks.6.attn.lora_B_v.9.weight', 'classifier_pool.9.weight', 'classifier_pool.9.bias', 'image_encoder.blocks.8.attn.lora_B_v.9.weight', 'image_encoder.blocks.6.attn.lora_B_k.9.weight', 'image_encoder.blocks.3.attn.lora_B_v.9.weight', 'image_encoder.blocks.2.attn.lora_B_v.9.weight', 'image_encoder.blocks.0.attn.lora_B_k.9.weight', 'image_encoder.blocks.7.attn.lora_B_v.9.weight', 'image_encoder.blocks.0.attn.lora_B_v.9.weight', 'image_encoder.blocks.2.attn.lora_B_k.9.weight', 'image_encoder.blocks.11.attn.lora_B_v.9.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 9, Epoch 1/20 => Loss 1.556, Train_accy 57.25:   0%|          | 0/20 [00:04<?, ?it/s]
Task 9, Epoch 1/20 => Loss 1.556, Train_accy 57.25:   5%|▌         | 1/20 [00:04<01:21,  4.27s/it]
Task 9, Epoch 2/20 => Loss 0.371, Train_accy 89.08:   5%|▌         | 1/20 [00:08<01:21,  4.27s/it]
Task 9, Epoch 2/20 => Loss 0.371, Train_accy 89.08:  10%|█         | 2/20 [00:08<01:16,  4.25s/it]
Task 9, Epoch 3/20 => Loss 0.268, Train_accy 91.91:  10%|█         | 2/20 [00:12<01:16,  4.25s/it]
Task 9, Epoch 3/20 => Loss 0.268, Train_accy 91.91:  15%|█▌        | 3/20 [00:12<01:12,  4.24s/it]
Task 9, Epoch 4/20 => Loss 0.230, Train_accy 93.07:  15%|█▌        | 3/20 [00:16<01:12,  4.24s/it]
Task 9, Epoch 4/20 => Loss 0.230, Train_accy 93.07:  20%|██        | 4/20 [00:16<01:07,  4.24s/it]
Task 9, Epoch 5/20 => Loss 0.236, Train_accy 93.07:  20%|██        | 4/20 [00:21<01:07,  4.24s/it]
Task 9, Epoch 5/20 => Loss 0.236, Train_accy 93.07:  25%|██▌       | 5/20 [00:21<01:03,  4.23s/it]
Task 9, Epoch 6/20 => Loss 0.226, Train_accy 92.44:  25%|██▌       | 5/20 [00:25<01:03,  4.23s/it]
Task 9, Epoch 6/20 => Loss 0.226, Train_accy 92.44:  30%|███       | 6/20 [00:25<00:59,  4.23s/it]
Task 9, Epoch 7/20 => Loss 0.200, Train_accy 94.01:  30%|███       | 6/20 [00:29<00:59,  4.23s/it]
Task 9, Epoch 7/20 => Loss 0.200, Train_accy 94.01:  35%|███▌      | 7/20 [00:29<00:55,  4.23s/it]
Task 9, Epoch 8/20 => Loss 0.194, Train_accy 93.80:  35%|███▌      | 7/20 [00:33<00:55,  4.23s/it]
Task 9, Epoch 8/20 => Loss 0.194, Train_accy 93.80:  40%|████      | 8/20 [00:33<00:50,  4.24s/it]
Task 9, Epoch 9/20 => Loss 0.189, Train_accy 94.22:  40%|████      | 8/20 [00:38<00:50,  4.24s/it]
Task 9, Epoch 9/20 => Loss 0.189, Train_accy 94.22:  45%|████▌     | 9/20 [00:38<00:46,  4.24s/it]
Task 9, Epoch 10/20 => Loss 0.165, Train_accy 94.43:  45%|████▌     | 9/20 [00:42<00:46,  4.24s/it]
Task 9, Epoch 10/20 => Loss 0.165, Train_accy 94.43:  50%|█████     | 10/20 [00:42<00:42,  4.24s/it]
Task 9, Epoch 11/20 => Loss 0.155, Train_accy 95.17:  50%|█████     | 10/20 [00:46<00:42,  4.24s/it]
Task 9, Epoch 11/20 => Loss 0.155, Train_accy 95.17:  55%|█████▌    | 11/20 [00:46<00:38,  4.25s/it]
Task 9, Epoch 12/20 => Loss 0.190, Train_accy 93.91:  55%|█████▌    | 11/20 [00:50<00:38,  4.25s/it]
Task 9, Epoch 12/20 => Loss 0.190, Train_accy 93.91:  60%|██████    | 12/20 [00:50<00:33,  4.25s/it]
Task 9, Epoch 13/20 => Loss 0.210, Train_accy 93.91:  60%|██████    | 12/20 [00:55<00:33,  4.25s/it]
Task 9, Epoch 13/20 => Loss 0.210, Train_accy 93.91:  65%|██████▌   | 13/20 [00:55<00:29,  4.26s/it]
Task 9, Epoch 14/20 => Loss 0.146, Train_accy 95.38:  65%|██████▌   | 13/20 [00:59<00:29,  4.26s/it]
Task 9, Epoch 14/20 => Loss 0.146, Train_accy 95.38:  70%|███████   | 14/20 [00:59<00:25,  4.25s/it]
Task 9, Epoch 15/20 => Loss 0.171, Train_accy 94.22:  70%|███████   | 14/20 [01:03<00:25,  4.25s/it]
Task 9, Epoch 15/20 => Loss 0.171, Train_accy 94.22:  75%|███████▌  | 15/20 [01:03<00:21,  4.26s/it]
Task 9, Epoch 16/20 => Loss 0.131, Train_accy 95.48:  75%|███████▌  | 15/20 [01:07<00:21,  4.26s/it]
Task 9, Epoch 16/20 => Loss 0.131, Train_accy 95.48:  80%|████████  | 16/20 [01:07<00:17,  4.26s/it]
Task 9, Epoch 17/20 => Loss 0.149, Train_accy 95.38:  80%|████████  | 16/20 [01:12<00:17,  4.26s/it]
Task 9, Epoch 17/20 => Loss 0.149, Train_accy 95.38:  85%|████████▌ | 17/20 [01:12<00:12,  4.26s/it]
Task 9, Epoch 18/20 => Loss 0.187, Train_accy 94.33:  85%|████████▌ | 17/20 [01:16<00:12,  4.26s/it]
Task 9, Epoch 18/20 => Loss 0.187, Train_accy 94.33:  90%|█████████ | 18/20 [01:16<00:08,  4.26s/it]
Task 9, Epoch 19/20 => Loss 0.176, Train_accy 95.17:  90%|█████████ | 18/20 [01:20<00:08,  4.26s/it]
Task 9, Epoch 19/20 => Loss 0.176, Train_accy 95.17:  95%|█████████▌| 19/20 [01:20<00:04,  4.26s/it]
Task 9, Epoch 20/20 => Loss 0.140, Train_accy 95.48:  95%|█████████▌| 19/20 [01:25<00:04,  4.26s/it]
Task 9, Epoch 20/20 => Loss 0.140, Train_accy 95.48: 100%|██████████| 20/20 [01:25<00:00,  4.28s/it]
Task 9, Epoch 20/20 => Loss 0.140, Train_accy 95.48: 100%|██████████| 20/20 [01:25<00:00,  4.25s/it]
2024-08-11 23:37:45,990 [inflora.py] => Task 9, Epoch 20/20 => Loss 0.140, Train_accy 95.48
Threshold:  0.995
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 24/768 type remove
Layer 3 : 72/768 type remove
Layer 4 : 99/768 type remove
Layer 5 : 129/768 type remove
Layer 6 : 120/768 type remove
Layer 7 : 123/768 type remove
Layer 8 : 134/768 type remove
Layer 9 : 147/768 type remove
Layer 10 : 150/768 type remove
Layer 11 : 91/768 type remove
Layer 12 : 57/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 23:37:56,897 [trainer.py] => Time:101.17629194259644
2358 2358
2358 2358
2024-08-11 23:38:01,964 [trainer.py] => Time:5.066310167312622
2024-08-11 23:38:01,964 [inflora.py] => Exemplar size: 0
2024-08-11 23:38:01,964 [trainer.py] => CNN: {'total': 76.17, '00-19': 86.64, '20-39': 81.16, '40-59': 81.65, '60-79': 76.52, '80-99': 69.52, '100-119': 74.12, '120-139': 73.78, '140-159': 78.8, '160-179': 69.2, '180-199': 71.3, 'old': 76.69, 'new': 71.3}
2024-08-11 23:38:01,964 [trainer.py] => CNN top1 curve: [97.98, 94.93, 92.26, 89.12, 85.19, 81.29, 79.4, 79.16, 77.96, 76.17]
2024-08-11 23:38:01,964 [trainer.py] => CNN top1 with task curve: [97.98, 97.8, 97.92, 97.93, 98.23, 98.16, 97.87, 98.1, 97.98, 97.96]
2024-08-11 23:38:01,964 [trainer.py] => CNN top1 task curve: [1.0, 0.9625550660792952, 0.9300595238095238, 0.8998911860718172, 0.8577441077441077, 0.818502824858757, 0.8013406459475929, 0.7974616604970914, 0.7847744360902256, 0.7675996607294318]
Traceback (most recent call last):
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/main.py", line 33, in <module>
    main()
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/main.py", line 11, in main
    train(args)
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/trainer.py", line 28, in train
    _set_random(args["seed"])
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/trainer.py", line 101, in _set_random
    torch.manual_seed(args['seed'])
TypeError: 'int' object is not subscriptable
logs/imagenet_r/40_40_sip/InfLoRA/adam/4/0.95_1.0-0.0005/1993
2024-08-11 23:38:05,741 [trainer.py] => config: ./configs/inr_inflora.json
2024-08-11 23:38:05,741 [trainer.py] => device: [device(type='cuda', index=0)]
2024-08-11 23:38:05,741 [trainer.py] => prefix: reproduce
2024-08-11 23:38:05,741 [trainer.py] => dataset: imagenet_r
2024-08-11 23:38:05,741 [trainer.py] => data_path: /mnt/mydisk/ruoheng.li/lrh/Dataset
2024-08-11 23:38:05,741 [trainer.py] => memory_size: 0
2024-08-11 23:38:05,741 [trainer.py] => memory_per_class: 0
2024-08-11 23:38:05,741 [trainer.py] => fixed_memory: True
2024-08-11 23:38:05,741 [trainer.py] => shuffle: True
2024-08-11 23:38:05,741 [trainer.py] => init_cls: 40
2024-08-11 23:38:05,741 [trainer.py] => increment: 40
2024-08-11 23:38:05,741 [trainer.py] => model_name: InfLoRA
2024-08-11 23:38:05,741 [trainer.py] => net_type: sip
2024-08-11 23:38:05,741 [trainer.py] => embd_dim: 768
2024-08-11 23:38:05,741 [trainer.py] => num_heads: 12
2024-08-11 23:38:05,741 [trainer.py] => total_sessions: 5
2024-08-11 23:38:05,741 [trainer.py] => seed: 1993
2024-08-11 23:38:05,741 [trainer.py] => EPSILON: 1e-08
2024-08-11 23:38:05,741 [trainer.py] => init_epoch: 20
2024-08-11 23:38:05,741 [trainer.py] => optim: adam
2024-08-11 23:38:05,741 [trainer.py] => init_lr: 0.0005
2024-08-11 23:38:05,741 [trainer.py] => init_lr_decay: 0.1
2024-08-11 23:38:05,741 [trainer.py] => init_weight_decay: 0.0
2024-08-11 23:38:05,741 [trainer.py] => epochs: 20
2024-08-11 23:38:05,741 [trainer.py] => lrate: 0.0005
2024-08-11 23:38:05,741 [trainer.py] => lrate_decay: 0.1
2024-08-11 23:38:05,741 [trainer.py] => batch_size: 48
2024-08-11 23:38:05,741 [trainer.py] => weight_decay: 0.0
2024-08-11 23:38:05,741 [trainer.py] => rank: 4
2024-08-11 23:38:05,741 [trainer.py] => lamb: 0.95
2024-08-11 23:38:05,741 [trainer.py] => lame: 1.0
2024-08-11 23:38:05,742 [trainer.py] => num_workers: 16
2024-08-11 23:38:05,814 [data_manager.py] => [168, 136, 51, 9, 183, 101, 171, 99, 42, 159, 191, 70, 16, 188, 27, 10, 175, 26, 68, 187, 98, 6, 85, 35, 112, 43, 100, 0, 103, 181, 88, 59, 4, 2, 116, 174, 94, 80, 106, 1, 147, 17, 141, 131, 72, 23, 173, 54, 197, 118, 87, 32, 79, 104, 91, 19, 135, 107, 178, 36, 11, 199, 142, 8, 122, 3, 28, 57, 153, 172, 190, 56, 49, 44, 97, 62, 151, 169, 194, 55, 192, 12, 189, 78, 66, 180, 15, 137, 109, 134, 92, 119, 126, 52, 170, 40, 148, 65, 144, 64, 138, 45, 77, 89, 154, 90, 71, 193, 74, 30, 113, 143, 96, 84, 67, 50, 186, 156, 69, 21, 18, 111, 108, 58, 125, 157, 150, 110, 182, 129, 166, 83, 81, 60, 13, 165, 14, 176, 63, 117, 5, 22, 145, 121, 38, 41, 82, 127, 114, 20, 31, 53, 37, 163, 196, 130, 152, 162, 86, 76, 24, 34, 184, 149, 33, 128, 198, 155, 146, 167, 139, 120, 140, 102, 47, 25, 158, 123, 46, 164, 61, 7, 115, 75, 133, 160, 105, 132, 179, 124, 48, 73, 93, 39, 95, 195, 29, 177, 185, 161]
2024-08-11 23:38:07,776 [trainer.py] => All params: 108399331
2024-08-11 23:38:07,778 [trainer.py] => Trainable params: 108399331
2024-08-11 23:38:07,778 [inflora.py] => Learning on 0-40
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.0.weight', 'image_encoder.blocks.7.attn.lora_B_v.0.weight', 'image_encoder.blocks.2.attn.lora_B_k.0.weight', 'image_encoder.blocks.3.attn.lora_B_v.0.weight', 'image_encoder.blocks.3.attn.lora_B_k.0.weight', 'image_encoder.blocks.8.attn.lora_B_v.0.weight', 'classifier_pool.0.weight', 'image_encoder.blocks.5.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_v.0.weight', 'image_encoder.blocks.4.attn.lora_B_v.0.weight', 'image_encoder.blocks.11.attn.lora_B_k.0.weight', 'image_encoder.blocks.10.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_v.0.weight', 'image_encoder.blocks.7.attn.lora_B_k.0.weight', 'classifier_pool.0.bias', 'image_encoder.blocks.2.attn.lora_B_v.0.weight', 'image_encoder.blocks.8.attn.lora_B_k.0.weight', 'image_encoder.blocks.6.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_k.0.weight', 'image_encoder.blocks.5.attn.lora_B_v.0.weight', 'image_encoder.blocks.0.attn.lora_B_v.0.weight', 'image_encoder.blocks.4.attn.lora_B_k.0.weight', 'image_encoder.blocks.11.attn.lora_B_v.0.weight', 'image_encoder.blocks.10.attn.lora_B_k.0.weight', 'image_encoder.blocks.9.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_k.0.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 0, Epoch 1/20 => Loss 1.848, Train_accy 54.04:   0%|          | 0/20 [00:18<?, ?it/s]
Task 0, Epoch 1/20 => Loss 1.848, Train_accy 54.04:   5%|▌         | 1/20 [00:18<05:56, 18.74s/it]
Task 0, Epoch 2/20 => Loss 0.672, Train_accy 82.52:   5%|▌         | 1/20 [00:37<05:56, 18.74s/it]
Task 0, Epoch 2/20 => Loss 0.672, Train_accy 82.52:  10%|█         | 2/20 [00:37<05:37, 18.75s/it]
Task 0, Epoch 3/20 => Loss 0.552, Train_accy 84.57:  10%|█         | 2/20 [00:56<05:37, 18.75s/it]
Task 0, Epoch 3/20 => Loss 0.552, Train_accy 84.57:  15%|█▌        | 3/20 [00:56<05:19, 18.78s/it]
Task 0, Epoch 4/20 => Loss 0.478, Train_accy 86.81:  15%|█▌        | 3/20 [01:15<05:19, 18.78s/it]
Task 0, Epoch 4/20 => Loss 0.478, Train_accy 86.81:  20%|██        | 4/20 [01:15<05:00, 18.77s/it]
Task 0, Epoch 5/20 => Loss 0.433, Train_accy 87.87:  20%|██        | 4/20 [01:33<05:00, 18.77s/it]
Task 0, Epoch 5/20 => Loss 0.433, Train_accy 87.87:  25%|██▌       | 5/20 [01:33<04:41, 18.75s/it]
Task 0, Epoch 6/20 => Loss 0.394, Train_accy 89.58:  25%|██▌       | 5/20 [01:52<04:41, 18.75s/it]
Task 0, Epoch 6/20 => Loss 0.394, Train_accy 89.58:  30%|███       | 6/20 [01:52<04:22, 18.75s/it]
Task 0, Epoch 7/20 => Loss 0.353, Train_accy 90.37:  30%|███       | 6/20 [02:11<04:22, 18.75s/it]
Task 0, Epoch 7/20 => Loss 0.353, Train_accy 90.37:  35%|███▌      | 7/20 [02:11<04:03, 18.73s/it]
Task 0, Epoch 8/20 => Loss 0.330, Train_accy 90.87:  35%|███▌      | 7/20 [02:29<04:03, 18.73s/it]
Task 0, Epoch 8/20 => Loss 0.330, Train_accy 90.87:  40%|████      | 8/20 [02:29<03:44, 18.67s/it]
Task 0, Epoch 9/20 => Loss 0.324, Train_accy 91.15:  40%|████      | 8/20 [02:48<03:44, 18.67s/it]
Task 0, Epoch 9/20 => Loss 0.324, Train_accy 91.15:  45%|████▌     | 9/20 [02:48<03:25, 18.65s/it]
Task 0, Epoch 10/20 => Loss 0.331, Train_accy 90.87:  45%|████▌     | 9/20 [03:06<03:25, 18.65s/it]
Task 0, Epoch 10/20 => Loss 0.331, Train_accy 90.87:  50%|█████     | 10/20 [03:06<03:05, 18.59s/it]
Task 0, Epoch 11/20 => Loss 0.285, Train_accy 91.80:  50%|█████     | 10/20 [03:25<03:05, 18.59s/it]
Task 0, Epoch 11/20 => Loss 0.285, Train_accy 91.80:  55%|█████▌    | 11/20 [03:25<02:47, 18.56s/it]
Task 0, Epoch 12/20 => Loss 0.290, Train_accy 92.14:  55%|█████▌    | 11/20 [03:43<02:47, 18.56s/it]
Task 0, Epoch 12/20 => Loss 0.290, Train_accy 92.14:  60%|██████    | 12/20 [03:43<02:28, 18.54s/it]
Task 0, Epoch 13/20 => Loss 0.276, Train_accy 92.49:  60%|██████    | 12/20 [04:02<02:28, 18.54s/it]
Task 0, Epoch 13/20 => Loss 0.276, Train_accy 92.49:  65%|██████▌   | 13/20 [04:02<02:09, 18.54s/it]
Task 0, Epoch 14/20 => Loss 0.253, Train_accy 93.16:  65%|██████▌   | 13/20 [04:20<02:09, 18.54s/it]
Task 0, Epoch 14/20 => Loss 0.253, Train_accy 93.16:  70%|███████   | 14/20 [04:20<01:51, 18.57s/it]
Task 0, Epoch 15/20 => Loss 0.247, Train_accy 92.99:  70%|███████   | 14/20 [04:39<01:51, 18.57s/it]
Task 0, Epoch 15/20 => Loss 0.247, Train_accy 92.99:  75%|███████▌  | 15/20 [04:39<01:33, 18.65s/it]
Task 0, Epoch 16/20 => Loss 0.240, Train_accy 93.31:  75%|███████▌  | 15/20 [04:58<01:33, 18.65s/it]
Task 0, Epoch 16/20 => Loss 0.240, Train_accy 93.31:  80%|████████  | 16/20 [04:58<01:14, 18.66s/it]
Task 0, Epoch 17/20 => Loss 0.210, Train_accy 94.06:  80%|████████  | 16/20 [05:17<01:14, 18.66s/it]
Task 0, Epoch 17/20 => Loss 0.210, Train_accy 94.06:  85%|████████▌ | 17/20 [05:17<00:56, 18.70s/it]
Task 0, Epoch 18/20 => Loss 0.233, Train_accy 93.79:  85%|████████▌ | 17/20 [05:36<00:56, 18.70s/it]
Task 0, Epoch 18/20 => Loss 0.233, Train_accy 93.79:  90%|█████████ | 18/20 [05:36<00:37, 18.70s/it]
Task 0, Epoch 19/20 => Loss 0.217, Train_accy 94.28:  90%|█████████ | 18/20 [05:54<00:37, 18.70s/it]
Task 0, Epoch 19/20 => Loss 0.217, Train_accy 94.28:  95%|█████████▌| 19/20 [05:54<00:18, 18.70s/it]
Task 0, Epoch 20/20 => Loss 0.232, Train_accy 93.53:  95%|█████████▌| 19/20 [06:13<00:18, 18.70s/it]
Task 0, Epoch 20/20 => Loss 0.232, Train_accy 93.53: 100%|██████████| 20/20 [06:13<00:00, 18.71s/it]
Task 0, Epoch 20/20 => Loss 0.232, Train_accy 93.53: 100%|██████████| 20/20 [06:13<00:00, 18.67s/it]
2024-08-11 23:44:38,090 [inflora.py] => Task 0, Epoch 20/20 => Loss 0.232, Train_accy 93.53
Threshold:  0.95
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 6/768 type remove
Layer 2 : 9/768 type remove
Layer 3 : 10/768 type remove
Layer 4 : 10/768 type remove
Layer 5 : 15/768 type remove
Layer 6 : 13/768 type remove
Layer 7 : 12/768 type remove
Layer 8 : 12/768 type remove
Layer 9 : 17/768 type remove
Layer 10 : 17/768 type remove
Layer 11 : 4/768 type remove
Layer 12 : 22/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 23:45:04,352 [trainer.py] => Time:416.5742630958557
1321 1321
1321 1321
2024-08-11 23:45:07,307 [trainer.py] => Time:2.9540934562683105
2024-08-11 23:45:07,307 [inflora.py] => Exemplar size: 0
2024-08-11 23:45:07,307 [trainer.py] => CNN: {'total': 91.07, '00-39': 91.07, 'old': 0, 'new': 91.07}
2024-08-11 23:45:07,307 [trainer.py] => CNN top1 curve: [91.07]
2024-08-11 23:45:07,307 [trainer.py] => CNN top1 with task curve: [91.07]
2024-08-11 23:45:07,307 [trainer.py] => CNN top1 task curve: [1.0]
2024-08-11 23:45:07,734 [trainer.py] => All params: 108399331
2024-08-11 23:45:07,735 [trainer.py] => Trainable params: 104488
2024-08-11 23:45:07,735 [inflora.py] => Learning on 40-80
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_k.1.weight', 'image_encoder.blocks.5.attn.lora_B_v.1.weight', 'image_encoder.blocks.8.attn.lora_B_v.1.weight', 'image_encoder.blocks.4.attn.lora_B_v.1.weight', 'image_encoder.blocks.3.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_k.1.weight', 'image_encoder.blocks.9.attn.lora_B_k.1.weight', 'image_encoder.blocks.1.attn.lora_B_k.1.weight', 'image_encoder.blocks.0.attn.lora_B_v.1.weight', 'image_encoder.blocks.8.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.1.weight', 'image_encoder.blocks.9.attn.lora_B_v.1.weight', 'image_encoder.blocks.11.attn.lora_B_k.1.weight', 'image_encoder.blocks.0.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_k.1.weight', 'classifier_pool.1.weight', 'image_encoder.blocks.5.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.1.weight', 'classifier_pool.1.bias', 'image_encoder.blocks.10.attn.lora_B_v.1.weight', 'image_encoder.blocks.11.attn.lora_B_v.1.weight', 'image_encoder.blocks.4.attn.lora_B_k.1.weight', 'image_encoder.blocks.2.attn.lora_B_v.1.weight', 'image_encoder.blocks.6.attn.lora_B_v.1.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 1, Epoch 1/20 => Loss 1.835, Train_accy 56.38:   0%|          | 0/20 [00:16<?, ?it/s]
Task 1, Epoch 1/20 => Loss 1.835, Train_accy 56.38:   5%|▌         | 1/20 [00:16<05:11, 16.41s/it]
Task 1, Epoch 2/20 => Loss 0.751, Train_accy 79.70:   5%|▌         | 1/20 [00:32<05:11, 16.41s/it]
Task 1, Epoch 2/20 => Loss 0.751, Train_accy 79.70:  10%|█         | 2/20 [00:32<04:55, 16.44s/it]
Task 1, Epoch 3/20 => Loss 0.640, Train_accy 82.85:  10%|█         | 2/20 [00:49<04:55, 16.44s/it]
Task 1, Epoch 3/20 => Loss 0.640, Train_accy 82.85:  15%|█▌        | 3/20 [00:49<04:39, 16.47s/it]
Task 1, Epoch 4/20 => Loss 0.522, Train_accy 85.44:  15%|█▌        | 3/20 [01:05<04:39, 16.47s/it]
Task 1, Epoch 4/20 => Loss 0.522, Train_accy 85.44:  20%|██        | 4/20 [01:05<04:23, 16.49s/it]
Task 1, Epoch 5/20 => Loss 0.471, Train_accy 86.91:  20%|██        | 4/20 [01:22<04:23, 16.49s/it]
Task 1, Epoch 5/20 => Loss 0.471, Train_accy 86.91:  25%|██▌       | 5/20 [01:22<04:07, 16.49s/it]
Task 1, Epoch 6/20 => Loss 0.431, Train_accy 88.11:  25%|██▌       | 5/20 [01:38<04:07, 16.49s/it]
Task 1, Epoch 6/20 => Loss 0.431, Train_accy 88.11:  30%|███       | 6/20 [01:38<03:51, 16.50s/it]
Task 1, Epoch 7/20 => Loss 0.407, Train_accy 88.46:  30%|███       | 6/20 [01:55<03:51, 16.50s/it]
Task 1, Epoch 7/20 => Loss 0.407, Train_accy 88.46:  35%|███▌      | 7/20 [01:55<03:34, 16.49s/it]
Task 1, Epoch 8/20 => Loss 0.364, Train_accy 90.08:  35%|███▌      | 7/20 [02:11<03:34, 16.49s/it]
Task 1, Epoch 8/20 => Loss 0.364, Train_accy 90.08:  40%|████      | 8/20 [02:11<03:17, 16.49s/it]
Task 1, Epoch 9/20 => Loss 0.370, Train_accy 89.91:  40%|████      | 8/20 [02:28<03:17, 16.49s/it]
Task 1, Epoch 9/20 => Loss 0.370, Train_accy 89.91:  45%|████▌     | 9/20 [02:28<03:01, 16.49s/it]
Task 1, Epoch 10/20 => Loss 0.347, Train_accy 90.40:  45%|████▌     | 9/20 [02:44<03:01, 16.49s/it]
Task 1, Epoch 10/20 => Loss 0.347, Train_accy 90.40:  50%|█████     | 10/20 [02:44<02:44, 16.46s/it]
Task 1, Epoch 11/20 => Loss 0.310, Train_accy 92.04:  50%|█████     | 10/20 [03:01<02:44, 16.46s/it]
Task 1, Epoch 11/20 => Loss 0.310, Train_accy 92.04:  55%|█████▌    | 11/20 [03:01<02:28, 16.48s/it]
Task 1, Epoch 12/20 => Loss 0.294, Train_accy 91.80:  55%|█████▌    | 11/20 [03:17<02:28, 16.48s/it]
Task 1, Epoch 12/20 => Loss 0.294, Train_accy 91.80:  60%|██████    | 12/20 [03:17<02:12, 16.50s/it]
Task 1, Epoch 13/20 => Loss 0.282, Train_accy 92.21:  60%|██████    | 12/20 [03:34<02:12, 16.50s/it]
Task 1, Epoch 13/20 => Loss 0.282, Train_accy 92.21:  65%|██████▌   | 13/20 [03:34<01:55, 16.49s/it]
Task 1, Epoch 14/20 => Loss 0.292, Train_accy 91.98:  65%|██████▌   | 13/20 [03:50<01:55, 16.49s/it]
Task 1, Epoch 14/20 => Loss 0.292, Train_accy 91.98:  70%|███████   | 14/20 [03:50<01:39, 16.50s/it]
Task 1, Epoch 15/20 => Loss 0.284, Train_accy 92.21:  70%|███████   | 14/20 [04:07<01:39, 16.50s/it]
Task 1, Epoch 15/20 => Loss 0.284, Train_accy 92.21:  75%|███████▌  | 15/20 [04:07<01:22, 16.50s/it]
Task 1, Epoch 16/20 => Loss 0.259, Train_accy 92.88:  75%|███████▌  | 15/20 [04:23<01:22, 16.50s/it]
Task 1, Epoch 16/20 => Loss 0.259, Train_accy 92.88:  80%|████████  | 16/20 [04:23<01:05, 16.48s/it]
Task 1, Epoch 17/20 => Loss 0.274, Train_accy 92.36:  80%|████████  | 16/20 [04:40<01:05, 16.48s/it]
Task 1, Epoch 17/20 => Loss 0.274, Train_accy 92.36:  85%|████████▌ | 17/20 [04:40<00:49, 16.49s/it]
Task 1, Epoch 18/20 => Loss 0.250, Train_accy 92.84:  85%|████████▌ | 17/20 [04:56<00:49, 16.49s/it]
Task 1, Epoch 18/20 => Loss 0.250, Train_accy 92.84:  90%|█████████ | 18/20 [04:56<00:32, 16.44s/it]
Task 1, Epoch 19/20 => Loss 0.259, Train_accy 92.86:  90%|█████████ | 18/20 [05:13<00:32, 16.44s/it]
Task 1, Epoch 19/20 => Loss 0.259, Train_accy 92.86:  95%|█████████▌| 19/20 [05:13<00:16, 16.45s/it]
Task 1, Epoch 20/20 => Loss 0.258, Train_accy 93.33:  95%|█████████▌| 19/20 [05:29<00:16, 16.45s/it]
Task 1, Epoch 20/20 => Loss 0.258, Train_accy 93.33: 100%|██████████| 20/20 [05:29<00:00, 16.47s/it]
Task 1, Epoch 20/20 => Loss 0.258, Train_accy 93.33: 100%|██████████| 20/20 [05:29<00:00, 16.48s/it]
2024-08-11 23:50:52,069 [inflora.py] => Task 1, Epoch 20/20 => Loss 0.258, Train_accy 93.33
Threshold:  0.96
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 10/768 type remove
Layer 3 : 13/768 type remove
Layer 4 : 14/768 type remove
Layer 5 : 20/768 type remove
Layer 6 : 19/768 type remove
Layer 7 : 20/768 type remove
Layer 8 : 20/768 type remove
Layer 9 : 31/768 type remove
Layer 10 : 36/768 type remove
Layer 11 : 12/768 type remove
Layer 12 : 53/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 23:51:17,916 [trainer.py] => Time:370.18111753463745
2498 2498
2498 2498
2024-08-11 23:51:23,089 [trainer.py] => Time:5.17264986038208
2024-08-11 23:51:23,090 [inflora.py] => Exemplar size: 0
2024-08-11 23:51:23,091 [trainer.py] => CNN: {'total': 86.19, '00-39': 88.57, '40-79': 83.52, 'old': 88.57, 'new': 83.52}
2024-08-11 23:51:23,091 [trainer.py] => CNN top1 curve: [91.07, 86.19]
2024-08-11 23:51:23,091 [trainer.py] => CNN top1 with task curve: [91.07, 89.99]
2024-08-11 23:51:23,091 [trainer.py] => CNN top1 task curve: [1.0, 0.9247397918334668]
2024-08-11 23:51:23,552 [trainer.py] => All params: 108399331
2024-08-11 23:51:23,554 [trainer.py] => Trainable params: 104488
2024-08-11 23:51:23,554 [inflora.py] => Learning on 80-120
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_k.2.weight', 'image_encoder.blocks.5.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.2.weight', 'image_encoder.blocks.10.attn.lora_B_k.2.weight', 'image_encoder.blocks.11.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_k.2.weight', 'image_encoder.blocks.9.attn.lora_B_k.2.weight', 'classifier_pool.2.bias', 'image_encoder.blocks.1.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_k.2.weight', 'image_encoder.blocks.4.attn.lora_B_k.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.2.weight', 'image_encoder.blocks.11.attn.lora_B_k.2.weight', 'image_encoder.blocks.1.attn.lora_B_k.2.weight', 'image_encoder.blocks.2.attn.lora_B_k.2.weight', 'image_encoder.blocks.9.attn.lora_B_v.2.weight', 'image_encoder.blocks.6.attn.lora_B_k.2.weight', 'image_encoder.blocks.2.attn.lora_B_v.2.weight', 'classifier_pool.2.weight', 'image_encoder.blocks.8.attn.lora_B_v.2.weight', 'image_encoder.blocks.10.attn.lora_B_v.2.weight', 'image_encoder.blocks.4.attn.lora_B_v.2.weight', 'image_encoder.blocks.7.attn.lora_B_k.2.weight', 'image_encoder.blocks.6.attn.lora_B_v.2.weight', 'image_encoder.blocks.7.attn.lora_B_v.2.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 2, Epoch 1/20 => Loss 1.738, Train_accy 57.23:   0%|          | 0/20 [00:17<?, ?it/s]
Task 2, Epoch 1/20 => Loss 1.738, Train_accy 57.23:   5%|▌         | 1/20 [00:17<05:25, 17.11s/it]
Task 2, Epoch 2/20 => Loss 0.728, Train_accy 80.50:   5%|▌         | 1/20 [00:34<05:25, 17.11s/it]
Task 2, Epoch 2/20 => Loss 0.728, Train_accy 80.50:  10%|█         | 2/20 [00:34<05:08, 17.12s/it]
Task 2, Epoch 3/20 => Loss 0.606, Train_accy 83.57:  10%|█         | 2/20 [00:51<05:08, 17.12s/it]
Task 2, Epoch 3/20 => Loss 0.606, Train_accy 83.57:  15%|█▌        | 3/20 [00:51<04:50, 17.09s/it]
Task 2, Epoch 4/20 => Loss 0.514, Train_accy 85.27:  15%|█▌        | 3/20 [01:08<04:50, 17.09s/it]
Task 2, Epoch 4/20 => Loss 0.514, Train_accy 85.27:  20%|██        | 4/20 [01:08<04:33, 17.07s/it]
Task 2, Epoch 5/20 => Loss 0.466, Train_accy 86.53:  20%|██        | 4/20 [01:25<04:33, 17.07s/it]
Task 2, Epoch 5/20 => Loss 0.466, Train_accy 86.53:  25%|██▌       | 5/20 [01:25<04:16, 17.07s/it]
Task 2, Epoch 6/20 => Loss 0.426, Train_accy 88.18:  25%|██▌       | 5/20 [01:42<04:16, 17.07s/it]
Task 2, Epoch 6/20 => Loss 0.426, Train_accy 88.18:  30%|███       | 6/20 [01:42<03:59, 17.08s/it]
Task 2, Epoch 7/20 => Loss 0.352, Train_accy 90.21:  30%|███       | 6/20 [01:59<03:59, 17.08s/it]
Task 2, Epoch 7/20 => Loss 0.352, Train_accy 90.21:  35%|███▌      | 7/20 [01:59<03:42, 17.10s/it]
Task 2, Epoch 8/20 => Loss 0.350, Train_accy 90.41:  35%|███▌      | 7/20 [02:16<03:42, 17.10s/it]
Task 2, Epoch 8/20 => Loss 0.350, Train_accy 90.41:  40%|████      | 8/20 [02:16<03:25, 17.10s/it]
Task 2, Epoch 9/20 => Loss 0.321, Train_accy 90.68:  40%|████      | 8/20 [02:33<03:25, 17.10s/it]
Task 2, Epoch 9/20 => Loss 0.321, Train_accy 90.68:  45%|████▌     | 9/20 [02:33<03:08, 17.14s/it]
Task 2, Epoch 10/20 => Loss 0.336, Train_accy 91.20:  45%|████▌     | 9/20 [02:51<03:08, 17.14s/it]
Task 2, Epoch 10/20 => Loss 0.336, Train_accy 91.20:  50%|█████     | 10/20 [02:51<02:51, 17.16s/it]
Task 2, Epoch 11/20 => Loss 0.332, Train_accy 91.03:  50%|█████     | 10/20 [03:08<02:51, 17.16s/it]
Task 2, Epoch 11/20 => Loss 0.332, Train_accy 91.03:  55%|█████▌    | 11/20 [03:08<02:34, 17.13s/it]
Task 2, Epoch 12/20 => Loss 0.315, Train_accy 91.22:  55%|█████▌    | 11/20 [03:25<02:34, 17.13s/it]
Task 2, Epoch 12/20 => Loss 0.315, Train_accy 91.22:  60%|██████    | 12/20 [03:25<02:17, 17.22s/it]
Task 2, Epoch 13/20 => Loss 0.323, Train_accy 91.01:  60%|██████    | 12/20 [03:42<02:17, 17.22s/it]
Task 2, Epoch 13/20 => Loss 0.323, Train_accy 91.01:  65%|██████▌   | 13/20 [03:42<02:00, 17.19s/it]
Task 2, Epoch 14/20 => Loss 0.291, Train_accy 92.38:  65%|██████▌   | 13/20 [03:59<02:00, 17.19s/it]
Task 2, Epoch 14/20 => Loss 0.291, Train_accy 92.38:  70%|███████   | 14/20 [03:59<01:42, 17.15s/it]
Task 2, Epoch 15/20 => Loss 0.276, Train_accy 92.25:  70%|███████   | 14/20 [04:16<01:42, 17.15s/it]
Task 2, Epoch 15/20 => Loss 0.276, Train_accy 92.25:  75%|███████▌  | 15/20 [04:16<01:25, 17.13s/it]
Task 2, Epoch 16/20 => Loss 0.270, Train_accy 92.73:  75%|███████▌  | 15/20 [04:34<01:25, 17.13s/it]
Task 2, Epoch 16/20 => Loss 0.270, Train_accy 92.73:  80%|████████  | 16/20 [04:34<01:08, 17.17s/it]
Task 2, Epoch 17/20 => Loss 0.249, Train_accy 92.95:  80%|████████  | 16/20 [04:51<01:08, 17.17s/it]
Task 2, Epoch 17/20 => Loss 0.249, Train_accy 92.95:  85%|████████▌ | 17/20 [04:51<00:51, 17.20s/it]
Task 2, Epoch 18/20 => Loss 0.217, Train_accy 93.82:  85%|████████▌ | 17/20 [05:08<00:51, 17.20s/it]
Task 2, Epoch 18/20 => Loss 0.217, Train_accy 93.82:  90%|█████████ | 18/20 [05:08<00:34, 17.15s/it]
Task 2, Epoch 19/20 => Loss 0.252, Train_accy 93.29:  90%|█████████ | 18/20 [05:25<00:34, 17.15s/it]
Task 2, Epoch 19/20 => Loss 0.252, Train_accy 93.29:  95%|█████████▌| 19/20 [05:25<00:17, 17.12s/it]
Task 2, Epoch 20/20 => Loss 0.237, Train_accy 93.33:  95%|█████████▌| 19/20 [05:42<00:17, 17.12s/it]
Task 2, Epoch 20/20 => Loss 0.237, Train_accy 93.33: 100%|██████████| 20/20 [05:42<00:00, 17.10s/it]
Task 2, Epoch 20/20 => Loss 0.237, Train_accy 93.33: 100%|██████████| 20/20 [05:42<00:00, 17.13s/it]
2024-08-11 23:57:21,782 [inflora.py] => Task 2, Epoch 20/20 => Loss 0.237, Train_accy 93.33
Threshold:  0.97
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 11/768 type remove
Layer 3 : 16/768 type remove
Layer 4 : 19/768 type remove
Layer 5 : 28/768 type remove
Layer 6 : 27/768 type remove
Layer 7 : 31/768 type remove
Layer 8 : 32/768 type remove
Layer 9 : 55/768 type remove
Layer 10 : 67/768 type remove
Layer 11 : 27/768 type remove
Layer 12 : 98/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-11 23:57:48,118 [trainer.py] => Time:384.56402039527893
3645 3645
3645 3645
2024-08-11 23:57:55,268 [trainer.py] => Time:7.149008274078369
2024-08-11 23:57:55,268 [inflora.py] => Exemplar size: 0
2024-08-11 23:57:55,268 [trainer.py] => CNN: {'total': 81.34, '00-39': 86.22, '40-79': 79.27, '80-119': 77.86, 'old': 82.95, 'new': 77.86}
2024-08-11 23:57:55,268 [trainer.py] => CNN top1 curve: [91.07, 86.19, 81.34]
2024-08-11 23:57:55,268 [trainer.py] => CNN top1 with task curve: [91.07, 89.99, 89.36]
2024-08-11 23:57:55,268 [trainer.py] => CNN top1 task curve: [1.0, 0.9247397918334668, 0.8655692729766804]
2024-08-11 23:57:55,700 [trainer.py] => All params: 108399331
2024-08-11 23:57:55,701 [trainer.py] => Trainable params: 104488
2024-08-11 23:57:55,702 [inflora.py] => Learning on 120-160
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_k.3.weight', 'image_encoder.blocks.7.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_k.3.weight', 'classifier_pool.3.weight', 'image_encoder.blocks.6.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.3.weight', 'image_encoder.blocks.1.attn.lora_B_v.3.weight', 'image_encoder.blocks.2.attn.lora_B_k.3.weight', 'image_encoder.blocks.3.attn.lora_B_k.3.weight', 'image_encoder.blocks.2.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_v.3.weight', 'classifier_pool.3.bias', 'image_encoder.blocks.1.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.3.weight', 'image_encoder.blocks.6.attn.lora_B_k.3.weight', 'image_encoder.blocks.5.attn.lora_B_v.3.weight', 'image_encoder.blocks.11.attn.lora_B_v.3.weight', 'image_encoder.blocks.5.attn.lora_B_k.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.3.weight', 'image_encoder.blocks.8.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_v.3.weight', 'image_encoder.blocks.3.attn.lora_B_v.3.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 3, Epoch 1/20 => Loss 1.651, Train_accy 60.02:   0%|          | 0/20 [00:17<?, ?it/s]
Task 3, Epoch 1/20 => Loss 1.651, Train_accy 60.02:   5%|▌         | 1/20 [00:17<05:34, 17.62s/it]
Task 3, Epoch 2/20 => Loss 0.705, Train_accy 81.36:   5%|▌         | 1/20 [00:35<05:34, 17.62s/it]
Task 3, Epoch 2/20 => Loss 0.705, Train_accy 81.36:  10%|█         | 2/20 [00:35<05:17, 17.63s/it]
Task 3, Epoch 3/20 => Loss 0.580, Train_accy 84.09:  10%|█         | 2/20 [00:52<05:17, 17.63s/it]
Task 3, Epoch 3/20 => Loss 0.580, Train_accy 84.09:  15%|█▌        | 3/20 [00:52<05:00, 17.65s/it]
Task 3, Epoch 4/20 => Loss 0.466, Train_accy 86.78:  15%|█▌        | 3/20 [01:10<05:00, 17.65s/it]
Task 3, Epoch 4/20 => Loss 0.466, Train_accy 86.78:  20%|██        | 4/20 [01:10<04:42, 17.65s/it]
Task 3, Epoch 5/20 => Loss 0.415, Train_accy 88.97:  20%|██        | 4/20 [01:28<04:42, 17.65s/it]
Task 3, Epoch 5/20 => Loss 0.415, Train_accy 88.97:  25%|██▌       | 5/20 [01:28<04:23, 17.58s/it]
Task 3, Epoch 6/20 => Loss 0.381, Train_accy 89.43:  25%|██▌       | 5/20 [01:45<04:23, 17.58s/it]
Task 3, Epoch 6/20 => Loss 0.381, Train_accy 89.43:  30%|███       | 6/20 [01:45<04:06, 17.59s/it]
Task 3, Epoch 7/20 => Loss 0.343, Train_accy 90.38:  30%|███       | 6/20 [02:03<04:06, 17.59s/it]
Task 3, Epoch 7/20 => Loss 0.343, Train_accy 90.38:  35%|███▌      | 7/20 [02:03<03:49, 17.62s/it]
Task 3, Epoch 8/20 => Loss 0.339, Train_accy 90.83:  35%|███▌      | 7/20 [02:21<03:49, 17.62s/it]
Task 3, Epoch 8/20 => Loss 0.339, Train_accy 90.83:  40%|████      | 8/20 [02:21<03:31, 17.64s/it]
Task 3, Epoch 9/20 => Loss 0.295, Train_accy 91.88:  40%|████      | 8/20 [02:38<03:31, 17.64s/it]
Task 3, Epoch 9/20 => Loss 0.295, Train_accy 91.88:  45%|████▌     | 9/20 [02:38<03:14, 17.65s/it]
Task 3, Epoch 10/20 => Loss 0.322, Train_accy 91.17:  45%|████▌     | 9/20 [02:56<03:14, 17.65s/it]
Task 3, Epoch 10/20 => Loss 0.322, Train_accy 91.17:  50%|█████     | 10/20 [02:56<02:56, 17.67s/it]
Task 3, Epoch 11/20 => Loss 0.264, Train_accy 92.98:  50%|█████     | 10/20 [03:14<02:56, 17.67s/it]
Task 3, Epoch 11/20 => Loss 0.264, Train_accy 92.98:  55%|█████▌    | 11/20 [03:14<02:39, 17.68s/it]
Task 3, Epoch 12/20 => Loss 0.286, Train_accy 92.21:  55%|█████▌    | 11/20 [03:31<02:39, 17.68s/it]
Task 3, Epoch 12/20 => Loss 0.286, Train_accy 92.21:  60%|██████    | 12/20 [03:31<02:21, 17.68s/it]
Task 3, Epoch 13/20 => Loss 0.264, Train_accy 92.89:  60%|██████    | 12/20 [03:49<02:21, 17.68s/it]
Task 3, Epoch 13/20 => Loss 0.264, Train_accy 92.89:  65%|██████▌   | 13/20 [03:49<02:03, 17.66s/it]
Task 3, Epoch 14/20 => Loss 0.272, Train_accy 92.98:  65%|██████▌   | 13/20 [04:07<02:03, 17.66s/it]
Task 3, Epoch 14/20 => Loss 0.272, Train_accy 92.98:  70%|███████   | 14/20 [04:07<01:45, 17.65s/it]
Task 3, Epoch 15/20 => Loss 0.260, Train_accy 93.02:  70%|███████   | 14/20 [04:24<01:45, 17.65s/it]
Task 3, Epoch 15/20 => Loss 0.260, Train_accy 93.02:  75%|███████▌  | 15/20 [04:24<01:28, 17.65s/it]
Task 3, Epoch 16/20 => Loss 0.235, Train_accy 93.46:  75%|███████▌  | 15/20 [04:42<01:28, 17.65s/it]
Task 3, Epoch 16/20 => Loss 0.235, Train_accy 93.46:  80%|████████  | 16/20 [04:42<01:10, 17.65s/it]
Task 3, Epoch 17/20 => Loss 0.227, Train_accy 93.77:  80%|████████  | 16/20 [04:59<01:10, 17.65s/it]
Task 3, Epoch 17/20 => Loss 0.227, Train_accy 93.77:  85%|████████▌ | 17/20 [04:59<00:52, 17.61s/it]
Task 3, Epoch 18/20 => Loss 0.230, Train_accy 93.72:  85%|████████▌ | 17/20 [05:17<00:52, 17.61s/it]
Task 3, Epoch 18/20 => Loss 0.230, Train_accy 93.72:  90%|█████████ | 18/20 [05:17<00:35, 17.69s/it]
Task 3, Epoch 19/20 => Loss 0.214, Train_accy 94.43:  90%|█████████ | 18/20 [05:35<00:35, 17.69s/it]
Task 3, Epoch 19/20 => Loss 0.214, Train_accy 94.43:  95%|█████████▌| 19/20 [05:35<00:17, 17.69s/it]
Task 3, Epoch 20/20 => Loss 0.223, Train_accy 93.85:  95%|█████████▌| 19/20 [05:53<00:17, 17.69s/it]
Task 3, Epoch 20/20 => Loss 0.223, Train_accy 93.85: 100%|██████████| 20/20 [05:53<00:00, 17.67s/it]
Task 3, Epoch 20/20 => Loss 0.223, Train_accy 93.85: 100%|██████████| 20/20 [05:53<00:00, 17.65s/it]
2024-08-12 00:04:04,513 [inflora.py] => Task 3, Epoch 20/20 => Loss 0.223, Train_accy 93.85
Threshold:  0.98
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 21/768 type remove
Layer 4 : 26/768 type remove
Layer 5 : 40/768 type remove
Layer 6 : 41/768 type remove
Layer 7 : 46/768 type remove
Layer 8 : 52/768 type remove
Layer 9 : 91/768 type remove
Layer 10 : 115/768 type remove
Layer 11 : 55/768 type remove
Layer 12 : 161/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:04:31,525 [trainer.py] => Time:395.8237020969391
4943 4943
4943 4943
2024-08-12 00:04:40,824 [trainer.py] => Time:9.298365831375122
2024-08-12 00:04:40,824 [inflora.py] => Exemplar size: 0
2024-08-12 00:04:40,824 [trainer.py] => CNN: {'total': 80.52, '00-39': 84.78, '40-79': 79.95, '80-119': 76.81, '120-159': 79.97, 'old': 80.71, 'new': 79.97}
2024-08-12 00:04:40,824 [trainer.py] => CNN top1 curve: [91.07, 86.19, 81.34, 80.52]
2024-08-12 00:04:40,824 [trainer.py] => CNN top1 with task curve: [91.07, 89.99, 89.36, 89.18]
2024-08-12 00:04:40,824 [trainer.py] => CNN top1 task curve: [1.0, 0.9247397918334668, 0.8655692729766804, 0.8492818126643739]
2024-08-12 00:04:41,284 [trainer.py] => All params: 108399331
2024-08-12 00:04:41,285 [trainer.py] => Trainable params: 104488
2024-08-12 00:04:41,285 [inflora.py] => Learning on 160-200
Parameters to be updated: {'classifier_pool.4.weight', 'image_encoder.blocks.4.attn.lora_B_k.4.weight', 'image_encoder.blocks.2.attn.lora_B_k.4.weight', 'image_encoder.blocks.7.attn.lora_B_v.4.weight', 'classifier_pool.4.bias', 'image_encoder.blocks.0.attn.lora_B_k.4.weight', 'image_encoder.blocks.9.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_v.4.weight', 'image_encoder.blocks.11.attn.lora_B_k.4.weight', 'image_encoder.blocks.10.attn.lora_B_k.4.weight', 'image_encoder.blocks.5.attn.lora_B_v.4.weight', 'image_encoder.blocks.1.attn.lora_B_k.4.weight', 'image_encoder.blocks.1.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_k.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.4.weight', 'image_encoder.blocks.6.attn.lora_B_k.4.weight', 'image_encoder.blocks.8.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.4.weight', 'image_encoder.blocks.0.attn.lora_B_v.4.weight', 'image_encoder.blocks.11.attn.lora_B_v.4.weight', 'image_encoder.blocks.6.attn.lora_B_v.4.weight', 'image_encoder.blocks.8.attn.lora_B_k.4.weight', 'image_encoder.blocks.7.attn.lora_B_k.4.weight', 'image_encoder.blocks.5.attn.lora_B_k.4.weight', 'image_encoder.blocks.2.attn.lora_B_v.4.weight', 'image_encoder.blocks.4.attn.lora_B_v.4.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 4, Epoch 1/20 => Loss 1.871, Train_accy 54.58:   0%|          | 0/20 [00:15<?, ?it/s]
Task 4, Epoch 1/20 => Loss 1.871, Train_accy 54.58:   5%|▌         | 1/20 [00:15<04:47, 15.14s/it]
Task 4, Epoch 2/20 => Loss 0.805, Train_accy 78.09:   5%|▌         | 1/20 [00:30<04:47, 15.14s/it]
Task 4, Epoch 2/20 => Loss 0.805, Train_accy 78.09:  10%|█         | 2/20 [00:30<04:33, 15.18s/it]
Task 4, Epoch 3/20 => Loss 0.618, Train_accy 83.85:  10%|█         | 2/20 [00:45<04:33, 15.18s/it]
Task 4, Epoch 3/20 => Loss 0.618, Train_accy 83.85:  15%|█▌        | 3/20 [00:45<04:18, 15.19s/it]
Task 4, Epoch 4/20 => Loss 0.542, Train_accy 85.92:  15%|█▌        | 3/20 [01:00<04:18, 15.19s/it]
Task 4, Epoch 4/20 => Loss 0.542, Train_accy 85.92:  20%|██        | 4/20 [01:00<04:03, 15.21s/it]
Task 4, Epoch 5/20 => Loss 0.490, Train_accy 86.37:  20%|██        | 4/20 [01:16<04:03, 15.21s/it]
Task 4, Epoch 5/20 => Loss 0.490, Train_accy 86.37:  25%|██▌       | 5/20 [01:16<03:48, 15.22s/it]
Task 4, Epoch 6/20 => Loss 0.432, Train_accy 88.60:  25%|██▌       | 5/20 [01:31<03:48, 15.22s/it]
Task 4, Epoch 6/20 => Loss 0.432, Train_accy 88.60:  30%|███       | 6/20 [01:31<03:33, 15.22s/it]
Task 4, Epoch 7/20 => Loss 0.430, Train_accy 88.62:  30%|███       | 6/20 [01:46<03:33, 15.22s/it]
Task 4, Epoch 7/20 => Loss 0.430, Train_accy 88.62:  35%|███▌      | 7/20 [01:46<03:17, 15.21s/it]
Task 4, Epoch 8/20 => Loss 0.363, Train_accy 89.73:  35%|███▌      | 7/20 [02:01<03:17, 15.21s/it]
Task 4, Epoch 8/20 => Loss 0.363, Train_accy 89.73:  40%|████      | 8/20 [02:01<03:02, 15.22s/it]
Task 4, Epoch 9/20 => Loss 0.370, Train_accy 90.49:  40%|████      | 8/20 [02:16<03:02, 15.22s/it]
Task 4, Epoch 9/20 => Loss 0.370, Train_accy 90.49:  45%|████▌     | 9/20 [02:16<02:47, 15.22s/it]
Task 4, Epoch 10/20 => Loss 0.340, Train_accy 90.83:  45%|████▌     | 9/20 [02:32<02:47, 15.22s/it]
Task 4, Epoch 10/20 => Loss 0.340, Train_accy 90.83:  50%|█████     | 10/20 [02:32<02:32, 15.21s/it]
Task 4, Epoch 11/20 => Loss 0.309, Train_accy 91.30:  50%|█████     | 10/20 [02:47<02:32, 15.21s/it]
Task 4, Epoch 11/20 => Loss 0.309, Train_accy 91.30:  55%|█████▌    | 11/20 [02:47<02:16, 15.17s/it]
Task 4, Epoch 12/20 => Loss 0.325, Train_accy 91.20:  55%|█████▌    | 11/20 [03:02<02:16, 15.17s/it]
Task 4, Epoch 12/20 => Loss 0.325, Train_accy 91.20:  60%|██████    | 12/20 [03:02<02:02, 15.25s/it]
Task 4, Epoch 13/20 => Loss 0.310, Train_accy 92.34:  60%|██████    | 12/20 [03:17<02:02, 15.25s/it]
Task 4, Epoch 13/20 => Loss 0.310, Train_accy 92.34:  65%|██████▌   | 13/20 [03:17<01:46, 15.26s/it]
Task 4, Epoch 14/20 => Loss 0.306, Train_accy 91.84:  65%|██████▌   | 13/20 [03:33<01:46, 15.26s/it]
Task 4, Epoch 14/20 => Loss 0.306, Train_accy 91.84:  70%|███████   | 14/20 [03:33<01:31, 15.25s/it]
Task 4, Epoch 15/20 => Loss 0.286, Train_accy 92.48:  70%|███████   | 14/20 [03:48<01:31, 15.25s/it]
Task 4, Epoch 15/20 => Loss 0.286, Train_accy 92.48:  75%|███████▌  | 15/20 [03:48<01:16, 15.23s/it]
Task 4, Epoch 16/20 => Loss 0.281, Train_accy 92.67:  75%|███████▌  | 15/20 [04:03<01:16, 15.23s/it]
Task 4, Epoch 16/20 => Loss 0.281, Train_accy 92.67:  80%|████████  | 16/20 [04:03<01:00, 15.20s/it]
Task 4, Epoch 17/20 => Loss 0.272, Train_accy 92.89:  80%|████████  | 16/20 [04:18<01:00, 15.20s/it]
Task 4, Epoch 17/20 => Loss 0.272, Train_accy 92.89:  85%|████████▌ | 17/20 [04:18<00:45, 15.22s/it]
Task 4, Epoch 18/20 => Loss 0.282, Train_accy 92.22:  85%|████████▌ | 17/20 [04:33<00:45, 15.22s/it]
Task 4, Epoch 18/20 => Loss 0.282, Train_accy 92.22:  90%|█████████ | 18/20 [04:33<00:30, 15.23s/it]
Task 4, Epoch 19/20 => Loss 0.251, Train_accy 93.31:  90%|█████████ | 18/20 [04:49<00:30, 15.23s/it]
Task 4, Epoch 19/20 => Loss 0.251, Train_accy 93.31:  95%|█████████▌| 19/20 [04:49<00:15, 15.20s/it]
Task 4, Epoch 20/20 => Loss 0.256, Train_accy 93.34:  95%|█████████▌| 19/20 [05:04<00:15, 15.20s/it]
Task 4, Epoch 20/20 => Loss 0.256, Train_accy 93.34: 100%|██████████| 20/20 [05:04<00:00, 15.21s/it]
Task 4, Epoch 20/20 => Loss 0.256, Train_accy 93.34: 100%|██████████| 20/20 [05:04<00:00, 15.22s/it]
2024-08-12 00:09:59,482 [inflora.py] => Task 4, Epoch 20/20 => Loss 0.256, Train_accy 93.34
Threshold:  0.99
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 35/768 type remove
Layer 4 : 45/768 type remove
Layer 5 : 68/768 type remove
Layer 6 : 72/768 type remove
Layer 7 : 86/768 type remove
Layer 8 : 101/768 type remove
Layer 9 : 171/768 type remove
Layer 10 : 218/768 type remove
Layer 11 : 140/768 type remove
Layer 12 : 284/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:10:24,260 [trainer.py] => Time:342.9749984741211
6000 6000
6000 6000
2024-08-12 00:10:35,276 [trainer.py] => Time:11.014773607254028
2024-08-12 00:10:35,276 [inflora.py] => Exemplar size: 0
2024-08-12 00:10:35,276 [trainer.py] => CNN: {'total': 77.87, '00-39': 83.12, '40-79': 77.99, '80-119': 75.24, '120-159': 78.89, '160-199': 72.75, 'old': 78.96, 'new': 72.75}
2024-08-12 00:10:35,276 [trainer.py] => CNN top1 curve: [91.07, 86.19, 81.34, 80.52, 77.87]
2024-08-12 00:10:35,276 [trainer.py] => CNN top1 with task curve: [91.07, 89.99, 89.36, 89.18, 88.92]
2024-08-12 00:10:35,276 [trainer.py] => CNN top1 task curve: [1.0, 0.9247397918334668, 0.8655692729766804, 0.8492818126643739, 0.8166666666666667]
Traceback (most recent call last):
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/main.py", line 33, in <module>
    main()
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/main.py", line 11, in main
    train(args)
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/trainer.py", line 28, in train
    _set_random(args["seed"])
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/trainer.py", line 101, in _set_random
    torch.manual_seed(args['seed'])
TypeError: 'int' object is not subscriptable
logs/imagenet_a/10_10_sip/InfLoRA/adam/4/0.95_1.0-0.0005/1993
2024-08-12 00:10:38,963 [trainer.py] => config: ./configs/ina_inflora.json
2024-08-12 00:10:38,963 [trainer.py] => device: [device(type='cuda', index=0)]
2024-08-12 00:10:38,963 [trainer.py] => prefix: reproduce
2024-08-12 00:10:38,963 [trainer.py] => dataset: imagenet_a
2024-08-12 00:10:38,963 [trainer.py] => data_path: /mnt/mydisk/ruoheng.li/lrh/Dataset
2024-08-12 00:10:38,963 [trainer.py] => memory_size: 0
2024-08-12 00:10:38,963 [trainer.py] => memory_per_class: 0
2024-08-12 00:10:38,963 [trainer.py] => fixed_memory: True
2024-08-12 00:10:38,963 [trainer.py] => shuffle: True
2024-08-12 00:10:38,963 [trainer.py] => init_cls: 10
2024-08-12 00:10:38,963 [trainer.py] => increment: 10
2024-08-12 00:10:38,963 [trainer.py] => model_name: InfLoRA
2024-08-12 00:10:38,963 [trainer.py] => net_type: sip
2024-08-12 00:10:38,963 [trainer.py] => embd_dim: 768
2024-08-12 00:10:38,963 [trainer.py] => num_heads: 12
2024-08-12 00:10:38,963 [trainer.py] => total_sessions: 20
2024-08-12 00:10:38,963 [trainer.py] => seed: 1993
2024-08-12 00:10:38,963 [trainer.py] => EPSILON: 1e-08
2024-08-12 00:10:38,963 [trainer.py] => init_epoch: 20
2024-08-12 00:10:38,963 [trainer.py] => optim: adam
2024-08-12 00:10:38,963 [trainer.py] => init_lr: 0.0005
2024-08-12 00:10:38,963 [trainer.py] => init_lr_decay: 0.1
2024-08-12 00:10:38,963 [trainer.py] => init_weight_decay: 0.0
2024-08-12 00:10:38,963 [trainer.py] => epochs: 20
2024-08-12 00:10:38,964 [trainer.py] => lrate: 0.0005
2024-08-12 00:10:38,964 [trainer.py] => lrate_decay: 0.1
2024-08-12 00:10:38,964 [trainer.py] => batch_size: 48
2024-08-12 00:10:38,964 [trainer.py] => weight_decay: 0.0
2024-08-12 00:10:38,964 [trainer.py] => rank: 4
2024-08-12 00:10:38,964 [trainer.py] => lamb: 0.95
2024-08-12 00:10:38,964 [trainer.py] => lame: 1.0
2024-08-12 00:10:38,964 [trainer.py] => num_workers: 16
2024-08-12 00:10:38,988 [data_manager.py] => [168, 136, 51, 9, 183, 101, 171, 99, 42, 159, 191, 70, 16, 188, 27, 10, 175, 26, 68, 187, 98, 6, 85, 35, 112, 43, 100, 0, 103, 181, 88, 59, 4, 2, 116, 174, 94, 80, 106, 1, 147, 17, 141, 131, 72, 23, 173, 54, 197, 118, 87, 32, 79, 104, 91, 19, 135, 107, 178, 36, 11, 199, 142, 8, 122, 3, 28, 57, 153, 172, 190, 56, 49, 44, 97, 62, 151, 169, 194, 55, 192, 12, 189, 78, 66, 180, 15, 137, 109, 134, 92, 119, 126, 52, 170, 40, 148, 65, 144, 64, 138, 45, 77, 89, 154, 90, 71, 193, 74, 30, 113, 143, 96, 84, 67, 50, 186, 156, 69, 21, 18, 111, 108, 58, 125, 157, 150, 110, 182, 129, 166, 83, 81, 60, 13, 165, 14, 176, 63, 117, 5, 22, 145, 121, 38, 41, 82, 127, 114, 20, 31, 53, 37, 163, 196, 130, 152, 162, 86, 76, 24, 34, 184, 149, 33, 128, 198, 155, 146, 167, 139, 120, 140, 102, 47, 25, 158, 123, 46, 164, 61, 7, 115, 75, 133, 160, 105, 132, 179, 124, 48, 73, 93, 39, 95, 195, 29, 177, 185, 161]
2024-08-12 00:10:41,029 [trainer.py] => All params: 110611171
2024-08-12 00:10:41,032 [trainer.py] => Trainable params: 110611171
2024-08-12 00:10:41,032 [inflora.py] => Learning on 0-10
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_v.0.weight', 'image_encoder.blocks.3.attn.lora_B_k.0.weight', 'image_encoder.blocks.4.attn.lora_B_k.0.weight', 'image_encoder.blocks.5.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_v.0.weight', 'image_encoder.blocks.6.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_k.0.weight', 'image_encoder.blocks.7.attn.lora_B_k.0.weight', 'classifier_pool.0.bias', 'image_encoder.blocks.5.attn.lora_B_v.0.weight', 'image_encoder.blocks.11.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_v.0.weight', 'classifier_pool.0.weight', 'image_encoder.blocks.8.attn.lora_B_k.0.weight', 'image_encoder.blocks.2.attn.lora_B_v.0.weight', 'image_encoder.blocks.10.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_k.0.weight', 'image_encoder.blocks.10.attn.lora_B_v.0.weight', 'image_encoder.blocks.4.attn.lora_B_v.0.weight', 'image_encoder.blocks.2.attn.lora_B_k.0.weight', 'image_encoder.blocks.3.attn.lora_B_v.0.weight', 'image_encoder.blocks.8.attn.lora_B_v.0.weight', 'image_encoder.blocks.6.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_v.0.weight', 'image_encoder.blocks.11.attn.lora_B_k.0.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 0, Epoch 1/20 => Loss 2.024, Train_accy 29.90:   0%|          | 0/20 [00:01<?, ?it/s]
Task 0, Epoch 1/20 => Loss 2.024, Train_accy 29.90:   5%|▌         | 1/20 [00:01<00:32,  1.69s/it]
Task 0, Epoch 2/20 => Loss 1.449, Train_accy 55.81:   5%|▌         | 1/20 [00:03<00:32,  1.69s/it]
Task 0, Epoch 2/20 => Loss 1.449, Train_accy 55.81:  10%|█         | 2/20 [00:03<00:29,  1.64s/it]
Task 0, Epoch 3/20 => Loss 1.110, Train_accy 67.44:  10%|█         | 2/20 [00:04<00:29,  1.64s/it]
Task 0, Epoch 3/20 => Loss 1.110, Train_accy 67.44:  15%|█▌        | 3/20 [00:04<00:27,  1.63s/it]
Task 0, Epoch 4/20 => Loss 0.798, Train_accy 75.08:  15%|█▌        | 3/20 [00:06<00:27,  1.63s/it]
Task 0, Epoch 4/20 => Loss 0.798, Train_accy 75.08:  20%|██        | 4/20 [00:06<00:25,  1.62s/it]
Task 0, Epoch 5/20 => Loss 0.535, Train_accy 84.72:  20%|██        | 4/20 [00:08<00:25,  1.62s/it]
Task 0, Epoch 5/20 => Loss 0.535, Train_accy 84.72:  25%|██▌       | 5/20 [00:08<00:24,  1.64s/it]
Task 0, Epoch 6/20 => Loss 0.417, Train_accy 86.05:  25%|██▌       | 5/20 [00:09<00:24,  1.64s/it]
Task 0, Epoch 6/20 => Loss 0.417, Train_accy 86.05:  30%|███       | 6/20 [00:09<00:22,  1.64s/it]
Task 0, Epoch 7/20 => Loss 0.317, Train_accy 92.69:  30%|███       | 6/20 [00:11<00:22,  1.64s/it]
Task 0, Epoch 7/20 => Loss 0.317, Train_accy 92.69:  35%|███▌      | 7/20 [00:11<00:21,  1.67s/it]
Task 0, Epoch 8/20 => Loss 0.251, Train_accy 94.68:  35%|███▌      | 7/20 [00:13<00:21,  1.67s/it]
Task 0, Epoch 8/20 => Loss 0.251, Train_accy 94.68:  40%|████      | 8/20 [00:13<00:19,  1.65s/it]
Task 0, Epoch 9/20 => Loss 0.259, Train_accy 91.69:  40%|████      | 8/20 [00:14<00:19,  1.65s/it]
Task 0, Epoch 9/20 => Loss 0.259, Train_accy 91.69:  45%|████▌     | 9/20 [00:14<00:18,  1.65s/it]
Task 0, Epoch 10/20 => Loss 0.188, Train_accy 94.68:  45%|████▌     | 9/20 [00:16<00:18,  1.65s/it]
Task 0, Epoch 10/20 => Loss 0.188, Train_accy 94.68:  50%|█████     | 10/20 [00:16<00:16,  1.64s/it]
Task 0, Epoch 11/20 => Loss 0.188, Train_accy 94.68:  50%|█████     | 10/20 [00:18<00:16,  1.64s/it]
Task 0, Epoch 11/20 => Loss 0.188, Train_accy 94.68:  55%|█████▌    | 11/20 [00:18<00:14,  1.65s/it]
Task 0, Epoch 12/20 => Loss 0.147, Train_accy 95.35:  55%|█████▌    | 11/20 [00:19<00:14,  1.65s/it]
Task 0, Epoch 12/20 => Loss 0.147, Train_accy 95.35:  60%|██████    | 12/20 [00:19<00:13,  1.65s/it]
Task 0, Epoch 13/20 => Loss 0.147, Train_accy 94.35:  60%|██████    | 12/20 [00:21<00:13,  1.65s/it]
Task 0, Epoch 13/20 => Loss 0.147, Train_accy 94.35:  65%|██████▌   | 13/20 [00:21<00:11,  1.64s/it]
Task 0, Epoch 14/20 => Loss 0.123, Train_accy 96.68:  65%|██████▌   | 13/20 [00:23<00:11,  1.64s/it]
Task 0, Epoch 14/20 => Loss 0.123, Train_accy 96.68:  70%|███████   | 14/20 [00:23<00:09,  1.65s/it]
Task 0, Epoch 15/20 => Loss 0.092, Train_accy 97.67:  70%|███████   | 14/20 [00:24<00:09,  1.65s/it]
Task 0, Epoch 15/20 => Loss 0.092, Train_accy 97.67:  75%|███████▌  | 15/20 [00:24<00:08,  1.64s/it]
Task 0, Epoch 16/20 => Loss 0.110, Train_accy 97.34:  75%|███████▌  | 15/20 [00:26<00:08,  1.64s/it]
Task 0, Epoch 16/20 => Loss 0.110, Train_accy 97.34:  80%|████████  | 16/20 [00:26<00:06,  1.64s/it]
Task 0, Epoch 17/20 => Loss 0.124, Train_accy 96.68:  80%|████████  | 16/20 [00:27<00:06,  1.64s/it]
Task 0, Epoch 17/20 => Loss 0.124, Train_accy 96.68:  85%|████████▌ | 17/20 [00:27<00:04,  1.63s/it]
Task 0, Epoch 18/20 => Loss 0.077, Train_accy 98.67:  85%|████████▌ | 17/20 [00:29<00:04,  1.63s/it]
Task 0, Epoch 18/20 => Loss 0.077, Train_accy 98.67:  90%|█████████ | 18/20 [00:29<00:03,  1.63s/it]
Task 0, Epoch 19/20 => Loss 0.106, Train_accy 97.34:  90%|█████████ | 18/20 [00:31<00:03,  1.63s/it]
Task 0, Epoch 19/20 => Loss 0.106, Train_accy 97.34:  95%|█████████▌| 19/20 [00:31<00:01,  1.63s/it]
Task 0, Epoch 20/20 => Loss 0.103, Train_accy 97.01:  95%|█████████▌| 19/20 [00:32<00:01,  1.63s/it]
Task 0, Epoch 20/20 => Loss 0.103, Train_accy 97.01: 100%|██████████| 20/20 [00:32<00:00,  1.62s/it]
Task 0, Epoch 20/20 => Loss 0.103, Train_accy 97.01: 100%|██████████| 20/20 [00:32<00:00,  1.64s/it]
2024-08-12 00:11:17,729 [inflora.py] => Task 0, Epoch 20/20 => Loss 0.103, Train_accy 97.01
Threshold:  0.95
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 6/768 type remove
Layer 2 : 9/768 type remove
Layer 3 : 11/768 type remove
Layer 4 : 12/768 type remove
Layer 5 : 19/768 type remove
Layer 6 : 17/768 type remove
Layer 7 : 15/768 type remove
Layer 8 : 19/768 type remove
Layer 9 : 35/768 type remove
Layer 10 : 47/768 type remove
Layer 11 : 10/768 type remove
Layer 12 : 23/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:11:23,536 [trainer.py] => Time:42.504337549209595
77 77
77 77
2024-08-12 00:11:24,476 [trainer.py] => Time:0.9389419555664062
2024-08-12 00:11:24,476 [inflora.py] => Exemplar size: 0
2024-08-12 00:11:24,476 [trainer.py] => CNN: {'total': 75.32, '00-09': 75.32, 'old': 0, 'new': 75.32}
2024-08-12 00:11:24,476 [trainer.py] => CNN top1 curve: [75.32]
2024-08-12 00:11:24,476 [trainer.py] => CNN top1 with task curve: [75.32]
2024-08-12 00:11:24,476 [trainer.py] => CNN top1 task curve: [1.0]
2024-08-12 00:11:24,986 [trainer.py] => All params: 110611171
2024-08-12 00:11:24,989 [trainer.py] => Trainable params: 81418
2024-08-12 00:11:24,989 [inflora.py] => Learning on 10-20
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.11.weight', 'image_encoder.blocks.3.attn.lora_B_v.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.13.weight', 'image_encoder.blocks.4.attn.lora_B_k.14.weight', 'image_encoder.blocks.9.attn.lora_B_v.10.weight', 'image_encoder.blocks.10.attn.lora_B_k.14.weight', 'image_encoder.blocks.6.attn.lora_B_v.14.weight', 'image_encoder.blocks.5.attn.lora_B_v.17.weight', 'image_encoder.blocks.10.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_k.11.weight', 'image_encoder.blocks.2.attn.lora_B_v.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.14.weight', 'image_encoder.blocks.10.attn.lora_B_v.10.weight', 'image_encoder.blocks.3.attn.lora_B_k.16.weight', 'image_encoder.blocks.5.attn.lora_B_v.11.weight', 'image_encoder.blocks.6.attn.lora_B_k.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.16.weight', 'image_encoder.blocks.5.attn.lora_B_v.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.15.weight', 'image_encoder.blocks.7.attn.lora_B_v.19.weight', 'image_encoder.blocks.9.attn.lora_B_v.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.11.weight', 'image_encoder.blocks.5.attn.lora_B_k.19.weight', 'image_encoder.blocks.6.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_k.1.weight', 'image_encoder.blocks.3.attn.lora_B_v.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.1.weight', 'image_encoder.blocks.9.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_v.15.weight', 'image_encoder.blocks.1.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_v.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.11.weight', 'image_encoder.blocks.1.attn.lora_B_v.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.19.weight', 'classifier_pool.16.bias', 'image_encoder.blocks.5.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_k.12.weight', 'image_encoder.blocks.3.attn.lora_B_k.18.weight', 'image_encoder.blocks.6.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_k.12.weight', 'classifier_pool.17.bias', 'image_encoder.blocks.8.attn.lora_B_v.18.weight', 'image_encoder.blocks.2.attn.lora_B_k.13.weight', 'image_encoder.blocks.0.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.14.weight', 'image_encoder.blocks.1.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_k.14.weight', 'image_encoder.blocks.2.attn.lora_B_k.12.weight', 'image_encoder.blocks.4.attn.lora_B_k.13.weight', 'image_encoder.blocks.1.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_v.19.weight', 'image_encoder.blocks.6.attn.lora_B_k.12.weight', 'image_encoder.blocks.7.attn.lora_B_k.11.weight', 'image_encoder.blocks.7.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.15.weight', 'image_encoder.blocks.7.attn.lora_B_k.19.weight', 'image_encoder.blocks.7.attn.lora_B_v.18.weight', 'image_encoder.blocks.8.attn.lora_B_v.1.weight', 'classifier_pool.12.weight', 'image_encoder.blocks.8.attn.lora_B_v.19.weight', 'image_encoder.blocks.5.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_k.13.weight', 'image_encoder.blocks.9.attn.lora_B_k.12.weight', 'image_encoder.blocks.3.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.14.weight', 'image_encoder.blocks.11.attn.lora_B_v.18.weight', 'image_encoder.blocks.1.attn.lora_B_k.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.13.weight', 'image_encoder.blocks.11.attn.lora_B_v.16.weight', 'image_encoder.blocks.2.attn.lora_B_v.17.weight', 'image_encoder.blocks.7.attn.lora_B_v.16.weight', 'classifier_pool.11.bias', 'image_encoder.blocks.1.attn.lora_B_k.19.weight', 'image_encoder.blocks.8.attn.lora_B_v.11.weight', 'image_encoder.blocks.1.attn.lora_B_v.1.weight', 'image_encoder.blocks.8.attn.lora_B_k.15.weight', 'image_encoder.blocks.1.attn.lora_B_v.19.weight', 'image_encoder.blocks.11.attn.lora_B_v.17.weight', 'image_encoder.blocks.1.attn.lora_B_v.15.weight', 'image_encoder.blocks.1.attn.lora_B_v.18.weight', 'image_encoder.blocks.11.attn.lora_B_v.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.11.weight', 'image_encoder.blocks.3.attn.lora_B_v.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.17.weight', 'image_encoder.blocks.2.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_v.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_k.10.weight', 'image_encoder.blocks.5.attn.lora_B_v.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.18.weight', 'image_encoder.blocks.7.attn.lora_B_v.12.weight', 'image_encoder.blocks.0.attn.lora_B_k.19.weight', 'image_encoder.blocks.7.attn.lora_B_v.1.weight', 'image_encoder.blocks.8.attn.lora_B_v.13.weight', 'image_encoder.blocks.10.attn.lora_B_k.16.weight', 'image_encoder.blocks.11.attn.lora_B_v.14.weight', 'classifier_pool.18.weight', 'image_encoder.blocks.6.attn.lora_B_v.18.weight', 'image_encoder.blocks.2.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_v.15.weight', 'image_encoder.blocks.11.attn.lora_B_v.11.weight', 'image_encoder.blocks.2.attn.lora_B_v.13.weight', 'image_encoder.blocks.1.attn.lora_B_v.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_v.19.weight', 'image_encoder.blocks.5.attn.lora_B_v.16.weight', 'image_encoder.blocks.6.attn.lora_B_v.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.16.weight', 'image_encoder.blocks.11.attn.lora_B_v.19.weight', 'image_encoder.blocks.10.attn.lora_B_k.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.19.weight', 'image_encoder.blocks.6.attn.lora_B_k.1.weight', 'image_encoder.blocks.0.attn.lora_B_k.14.weight', 'image_encoder.blocks.7.attn.lora_B_k.13.weight', 'image_encoder.blocks.7.attn.lora_B_k.14.weight', 'image_encoder.blocks.7.attn.lora_B_v.15.weight', 'classifier_pool.18.bias', 'image_encoder.blocks.10.attn.lora_B_k.13.weight', 'image_encoder.blocks.0.attn.lora_B_v.13.weight', 'image_encoder.blocks.1.attn.lora_B_k.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.11.weight', 'image_encoder.blocks.5.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.12.weight', 'image_encoder.blocks.6.attn.lora_B_k.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.13.weight', 'image_encoder.blocks.0.attn.lora_B_k.16.weight', 'image_encoder.blocks.8.attn.lora_B_v.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.15.weight', 'classifier_pool.16.weight', 'image_encoder.blocks.2.attn.lora_B_v.10.weight', 'image_encoder.blocks.1.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_k.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.19.weight', 'image_encoder.blocks.3.attn.lora_B_v.1.weight', 'image_encoder.blocks.4.attn.lora_B_k.1.weight', 'image_encoder.blocks.8.attn.lora_B_v.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_v.10.weight', 'image_encoder.blocks.10.attn.lora_B_v.15.weight', 'image_encoder.blocks.11.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_v.18.weight', 'image_encoder.blocks.5.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.1.weight', 'image_encoder.blocks.11.attn.lora_B_k.14.weight', 'image_encoder.blocks.4.attn.lora_B_v.10.weight', 'image_encoder.blocks.0.attn.lora_B_k.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.11.weight', 'image_encoder.blocks.6.attn.lora_B_v.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.14.weight', 'image_encoder.blocks.3.attn.lora_B_k.1.weight', 'image_encoder.blocks.9.attn.lora_B_v.12.weight', 'image_encoder.blocks.9.attn.lora_B_v.17.weight', 'image_encoder.blocks.7.attn.lora_B_k.12.weight', 'image_encoder.blocks.4.attn.lora_B_v.17.weight', 'image_encoder.blocks.5.attn.lora_B_k.14.weight', 'classifier_pool.10.weight', 'image_encoder.blocks.0.attn.lora_B_v.10.weight', 'image_encoder.blocks.3.attn.lora_B_k.12.weight', 'image_encoder.blocks.9.attn.lora_B_k.10.weight', 'image_encoder.blocks.11.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.19.weight', 'classifier_pool.14.bias', 'image_encoder.blocks.2.attn.lora_B_v.1.weight', 'classifier_pool.10.bias', 'image_encoder.blocks.10.attn.lora_B_v.17.weight', 'image_encoder.blocks.7.attn.lora_B_v.10.weight', 'image_encoder.blocks.10.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.10.weight', 'classifier_pool.1.bias', 'image_encoder.blocks.9.attn.lora_B_v.1.weight', 'image_encoder.blocks.4.attn.lora_B_v.13.weight', 'classifier_pool.19.bias', 'image_encoder.blocks.3.attn.lora_B_v.16.weight', 'image_encoder.blocks.0.attn.lora_B_k.17.weight', 'image_encoder.blocks.0.attn.lora_B_k.11.weight', 'image_encoder.blocks.1.attn.lora_B_k.10.weight', 'image_encoder.blocks.5.attn.lora_B_k.16.weight', 'image_encoder.blocks.1.attn.lora_B_k.14.weight', 'image_encoder.blocks.7.attn.lora_B_k.18.weight', 'image_encoder.blocks.9.attn.lora_B_k.17.weight', 'classifier_pool.15.weight', 'image_encoder.blocks.1.attn.lora_B_v.16.weight', 'image_encoder.blocks.3.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_v.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.16.weight', 'image_encoder.blocks.1.attn.lora_B_k.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.10.weight', 'image_encoder.blocks.11.attn.lora_B_k.18.weight', 'image_encoder.blocks.8.attn.lora_B_k.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.11.weight', 'image_encoder.blocks.3.attn.lora_B_k.19.weight', 'image_encoder.blocks.10.attn.lora_B_k.10.weight', 'image_encoder.blocks.4.attn.lora_B_k.16.weight', 'classifier_pool.13.weight', 'classifier_pool.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_v.11.weight', 'image_encoder.blocks.4.attn.lora_B_k.15.weight', 'classifier_pool.13.bias', 'image_encoder.blocks.5.attn.lora_B_v.1.weight', 'image_encoder.blocks.6.attn.lora_B_v.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.15.weight', 'image_encoder.blocks.11.attn.lora_B_k.12.weight', 'image_encoder.blocks.0.attn.lora_B_k.10.weight', 'image_encoder.blocks.7.attn.lora_B_k.1.weight', 'image_encoder.blocks.4.attn.lora_B_k.10.weight', 'image_encoder.blocks.11.attn.lora_B_k.19.weight', 'image_encoder.blocks.10.attn.lora_B_k.19.weight', 'image_encoder.blocks.6.attn.lora_B_k.16.weight', 'image_encoder.blocks.6.attn.lora_B_k.11.weight', 'image_encoder.blocks.8.attn.lora_B_k.18.weight', 'image_encoder.blocks.11.attn.lora_B_k.13.weight', 'image_encoder.blocks.1.attn.lora_B_k.15.weight', 'image_encoder.blocks.6.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.17.weight', 'image_encoder.blocks.7.attn.lora_B_k.17.weight', 'classifier_pool.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.18.weight', 'image_encoder.blocks.11.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.19.weight', 'image_encoder.blocks.0.attn.lora_B_k.1.weight', 'classifier_pool.12.bias', 'image_encoder.blocks.3.attn.lora_B_k.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.17.weight', 'image_encoder.blocks.3.attn.lora_B_v.11.weight', 'image_encoder.blocks.8.attn.lora_B_v.14.weight', 'image_encoder.blocks.10.attn.lora_B_k.12.weight', 'image_encoder.blocks.11.attn.lora_B_v.1.weight', 'image_encoder.blocks.4.attn.lora_B_v.1.weight', 'image_encoder.blocks.5.attn.lora_B_v.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.13.weight', 'image_encoder.blocks.10.attn.lora_B_k.17.weight', 'classifier_pool.15.bias', 'image_encoder.blocks.2.attn.lora_B_k.1.weight', 'image_encoder.blocks.4.attn.lora_B_v.16.weight', 'image_encoder.blocks.6.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_k.19.weight', 'classifier_pool.1.weight', 'image_encoder.blocks.9.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_v.11.weight', 'image_encoder.blocks.9.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_v.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.12.weight', 'image_encoder.blocks.2.attn.lora_B_k.17.weight', 'image_encoder.blocks.8.attn.lora_B_v.12.weight', 'image_encoder.blocks.9.attn.lora_B_v.11.weight', 'image_encoder.blocks.9.attn.lora_B_v.14.weight', 'classifier_pool.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.1.weight', 'image_encoder.blocks.11.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_k.18.weight', 'image_encoder.blocks.10.attn.lora_B_v.12.weight', 'classifier_pool.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.16.weight', 'image_encoder.blocks.0.attn.lora_B_k.13.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 1, Epoch 1/20 => Loss 2.589, Train_accy 20.05:   0%|          | 0/20 [00:02<?, ?it/s]
Task 1, Epoch 1/20 => Loss 2.589, Train_accy 20.05:   5%|▌         | 1/20 [00:02<00:40,  2.13s/it]
Task 1, Epoch 2/20 => Loss 1.236, Train_accy 57.92:   5%|▌         | 1/20 [00:04<00:40,  2.13s/it]
Task 1, Epoch 2/20 => Loss 1.236, Train_accy 57.92:  10%|█         | 2/20 [00:04<00:37,  2.11s/it]
Task 1, Epoch 3/20 => Loss 0.822, Train_accy 74.50:  10%|█         | 2/20 [00:06<00:37,  2.11s/it]
Task 1, Epoch 3/20 => Loss 0.822, Train_accy 74.50:  15%|█▌        | 3/20 [00:06<00:35,  2.11s/it]
Task 1, Epoch 4/20 => Loss 0.565, Train_accy 80.69:  15%|█▌        | 3/20 [00:08<00:35,  2.11s/it]
Task 1, Epoch 4/20 => Loss 0.565, Train_accy 80.69:  20%|██        | 4/20 [00:08<00:34,  2.13s/it]
Task 1, Epoch 5/20 => Loss 0.436, Train_accy 84.41:  20%|██        | 4/20 [00:10<00:34,  2.13s/it]
Task 1, Epoch 5/20 => Loss 0.436, Train_accy 84.41:  25%|██▌       | 5/20 [00:10<00:31,  2.12s/it]
Task 1, Epoch 6/20 => Loss 0.322, Train_accy 91.83:  25%|██▌       | 5/20 [00:12<00:31,  2.12s/it]
Task 1, Epoch 6/20 => Loss 0.322, Train_accy 91.83:  30%|███       | 6/20 [00:12<00:29,  2.13s/it]
Task 1, Epoch 7/20 => Loss 0.296, Train_accy 90.35:  30%|███       | 6/20 [00:14<00:29,  2.13s/it]
Task 1, Epoch 7/20 => Loss 0.296, Train_accy 90.35:  35%|███▌      | 7/20 [00:14<00:27,  2.13s/it]
Task 1, Epoch 8/20 => Loss 0.261, Train_accy 93.07:  35%|███▌      | 7/20 [00:17<00:27,  2.13s/it]
Task 1, Epoch 8/20 => Loss 0.261, Train_accy 93.07:  40%|████      | 8/20 [00:17<00:25,  2.14s/it]
Task 1, Epoch 9/20 => Loss 0.239, Train_accy 94.06:  40%|████      | 8/20 [00:19<00:25,  2.14s/it]
Task 1, Epoch 9/20 => Loss 0.239, Train_accy 94.06:  45%|████▌     | 9/20 [00:19<00:23,  2.15s/it]
Task 1, Epoch 10/20 => Loss 0.210, Train_accy 94.80:  45%|████▌     | 9/20 [00:21<00:23,  2.15s/it]
Task 1, Epoch 10/20 => Loss 0.210, Train_accy 94.80:  50%|█████     | 10/20 [00:21<00:21,  2.14s/it]
Task 1, Epoch 11/20 => Loss 0.223, Train_accy 93.07:  50%|█████     | 10/20 [00:23<00:21,  2.14s/it]
Task 1, Epoch 11/20 => Loss 0.223, Train_accy 93.07:  55%|█████▌    | 11/20 [00:23<00:19,  2.15s/it]
Task 1, Epoch 12/20 => Loss 0.260, Train_accy 92.82:  55%|█████▌    | 11/20 [00:25<00:19,  2.15s/it]
Task 1, Epoch 12/20 => Loss 0.260, Train_accy 92.82:  60%|██████    | 12/20 [00:25<00:17,  2.15s/it]
Task 1, Epoch 13/20 => Loss 0.152, Train_accy 95.30:  60%|██████    | 12/20 [00:27<00:17,  2.15s/it]
Task 1, Epoch 13/20 => Loss 0.152, Train_accy 95.30:  65%|██████▌   | 13/20 [00:27<00:15,  2.15s/it]
Task 1, Epoch 14/20 => Loss 0.170, Train_accy 94.55:  65%|██████▌   | 13/20 [00:29<00:15,  2.15s/it]
Task 1, Epoch 14/20 => Loss 0.170, Train_accy 94.55:  70%|███████   | 14/20 [00:29<00:12,  2.15s/it]
Task 1, Epoch 15/20 => Loss 0.153, Train_accy 95.54:  70%|███████   | 14/20 [00:32<00:12,  2.15s/it]
Task 1, Epoch 15/20 => Loss 0.153, Train_accy 95.54:  75%|███████▌  | 15/20 [00:32<00:10,  2.14s/it]
Task 1, Epoch 16/20 => Loss 0.164, Train_accy 94.55:  75%|███████▌  | 15/20 [00:34<00:10,  2.14s/it]
Task 1, Epoch 16/20 => Loss 0.164, Train_accy 94.55:  80%|████████  | 16/20 [00:34<00:08,  2.14s/it]
Task 1, Epoch 17/20 => Loss 0.155, Train_accy 94.80:  80%|████████  | 16/20 [00:36<00:08,  2.14s/it]
Task 1, Epoch 17/20 => Loss 0.155, Train_accy 94.80:  85%|████████▌ | 17/20 [00:36<00:06,  2.14s/it]
Task 1, Epoch 18/20 => Loss 0.120, Train_accy 96.78:  85%|████████▌ | 17/20 [00:38<00:06,  2.14s/it]
Task 1, Epoch 18/20 => Loss 0.120, Train_accy 96.78:  90%|█████████ | 18/20 [00:38<00:04,  2.15s/it]
Task 1, Epoch 19/20 => Loss 0.173, Train_accy 95.79:  90%|█████████ | 18/20 [00:40<00:04,  2.15s/it]
Task 1, Epoch 19/20 => Loss 0.173, Train_accy 95.79:  95%|█████████▌| 19/20 [00:40<00:02,  2.14s/it]
Task 1, Epoch 20/20 => Loss 0.137, Train_accy 96.29:  95%|█████████▌| 19/20 [00:42<00:02,  2.14s/it]
Task 1, Epoch 20/20 => Loss 0.137, Train_accy 96.29: 100%|██████████| 20/20 [00:42<00:00,  2.15s/it]
Task 1, Epoch 20/20 => Loss 0.137, Train_accy 96.29: 100%|██████████| 20/20 [00:42<00:00,  2.14s/it]
2024-08-12 00:12:11,584 [inflora.py] => Task 1, Epoch 20/20 => Loss 0.137, Train_accy 96.29
Threshold:  0.9524999999999999
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 10/768 type remove
Layer 3 : 13/768 type remove
Layer 4 : 14/768 type remove
Layer 5 : 22/768 type remove
Layer 6 : 20/768 type remove
Layer 7 : 18/768 type remove
Layer 8 : 26/768 type remove
Layer 9 : 49/768 type remove
Layer 10 : 62/768 type remove
Layer 11 : 15/768 type remove
Layer 12 : 30/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:12:20,201 [trainer.py] => Time:55.211822271347046
175 175
175 175
2024-08-12 00:12:21,401 [trainer.py] => Time:1.2000069618225098
2024-08-12 00:12:21,402 [inflora.py] => Exemplar size: 0
2024-08-12 00:12:21,402 [trainer.py] => CNN: {'total': 64.57, '00-09': 66.23, '10-19': 63.27, 'old': 66.23, 'new': 63.27}
2024-08-12 00:12:21,402 [trainer.py] => CNN top1 curve: [75.32, 64.57]
2024-08-12 00:12:21,402 [trainer.py] => CNN top1 with task curve: [75.32, 82.29]
2024-08-12 00:12:21,402 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571]
2024-08-12 00:12:21,920 [trainer.py] => All params: 110611171
2024-08-12 00:12:21,923 [trainer.py] => Trainable params: 895598
2024-08-12 00:12:21,923 [inflora.py] => Learning on 20-30
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_v.2.weight', 'image_encoder.blocks.1.attn.lora_B_k.2.weight', 'image_encoder.blocks.1.attn.lora_B_v.2.weight', 'image_encoder.blocks.6.attn.lora_B_k.2.weight', 'image_encoder.blocks.0.attn.lora_B_k.2.weight', 'image_encoder.blocks.7.attn.lora_B_k.2.weight', 'classifier_pool.2.weight', 'image_encoder.blocks.9.attn.lora_B_k.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.2.weight', 'image_encoder.blocks.10.attn.lora_B_k.2.weight', 'image_encoder.blocks.2.attn.lora_B_k.2.weight', 'image_encoder.blocks.9.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_v.2.weight', 'image_encoder.blocks.4.attn.lora_B_v.2.weight', 'image_encoder.blocks.4.attn.lora_B_k.2.weight', 'image_encoder.blocks.10.attn.lora_B_v.2.weight', 'image_encoder.blocks.7.attn.lora_B_v.2.weight', 'image_encoder.blocks.11.attn.lora_B_k.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_k.2.weight', 'image_encoder.blocks.6.attn.lora_B_v.2.weight', 'classifier_pool.2.bias', 'image_encoder.blocks.8.attn.lora_B_v.2.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 2, Epoch 1/20 => Loss 2.887, Train_accy 16.53:   0%|          | 0/20 [00:02<?, ?it/s]
Task 2, Epoch 1/20 => Loss 2.887, Train_accy 16.53:   5%|▌         | 1/20 [00:02<00:38,  2.04s/it]
Task 2, Epoch 2/20 => Loss 1.761, Train_accy 45.18:   5%|▌         | 1/20 [00:04<00:38,  2.04s/it]
Task 2, Epoch 2/20 => Loss 1.761, Train_accy 45.18:  10%|█         | 2/20 [00:04<00:37,  2.07s/it]
Task 2, Epoch 3/20 => Loss 1.165, Train_accy 63.09:  10%|█         | 2/20 [00:06<00:37,  2.07s/it]
Task 2, Epoch 3/20 => Loss 1.165, Train_accy 63.09:  15%|█▌        | 3/20 [00:06<00:34,  2.06s/it]
Task 2, Epoch 4/20 => Loss 0.845, Train_accy 73.00:  15%|█▌        | 3/20 [00:08<00:34,  2.06s/it]
Task 2, Epoch 4/20 => Loss 0.845, Train_accy 73.00:  20%|██        | 4/20 [00:08<00:33,  2.06s/it]
Task 2, Epoch 5/20 => Loss 0.739, Train_accy 77.96:  20%|██        | 4/20 [00:10<00:33,  2.06s/it]
Task 2, Epoch 5/20 => Loss 0.739, Train_accy 77.96:  25%|██▌       | 5/20 [00:10<00:30,  2.06s/it]
Task 2, Epoch 6/20 => Loss 0.624, Train_accy 79.89:  25%|██▌       | 5/20 [00:12<00:30,  2.06s/it]
Task 2, Epoch 6/20 => Loss 0.624, Train_accy 79.89:  30%|███       | 6/20 [00:12<00:29,  2.08s/it]
Task 2, Epoch 7/20 => Loss 0.512, Train_accy 84.85:  30%|███       | 6/20 [00:14<00:29,  2.08s/it]
Task 2, Epoch 7/20 => Loss 0.512, Train_accy 84.85:  35%|███▌      | 7/20 [00:14<00:27,  2.08s/it]
Task 2, Epoch 8/20 => Loss 0.475, Train_accy 85.40:  35%|███▌      | 7/20 [00:16<00:27,  2.08s/it]
Task 2, Epoch 8/20 => Loss 0.475, Train_accy 85.40:  40%|████      | 8/20 [00:16<00:24,  2.07s/it]
Task 2, Epoch 9/20 => Loss 0.368, Train_accy 90.63:  40%|████      | 8/20 [00:18<00:24,  2.07s/it]
Task 2, Epoch 9/20 => Loss 0.368, Train_accy 90.63:  45%|████▌     | 9/20 [00:18<00:22,  2.08s/it]
Task 2, Epoch 10/20 => Loss 0.338, Train_accy 90.36:  45%|████▌     | 9/20 [00:20<00:22,  2.08s/it]
Task 2, Epoch 10/20 => Loss 0.338, Train_accy 90.36:  50%|█████     | 10/20 [00:20<00:20,  2.07s/it]
Task 2, Epoch 11/20 => Loss 0.305, Train_accy 93.11:  50%|█████     | 10/20 [00:22<00:20,  2.07s/it]
Task 2, Epoch 11/20 => Loss 0.305, Train_accy 93.11:  55%|█████▌    | 11/20 [00:22<00:18,  2.06s/it]
Task 2, Epoch 12/20 => Loss 0.256, Train_accy 92.01:  55%|█████▌    | 11/20 [00:24<00:18,  2.06s/it]
Task 2, Epoch 12/20 => Loss 0.256, Train_accy 92.01:  60%|██████    | 12/20 [00:24<00:16,  2.06s/it]
Task 2, Epoch 13/20 => Loss 0.241, Train_accy 95.04:  60%|██████    | 12/20 [00:26<00:16,  2.06s/it]
Task 2, Epoch 13/20 => Loss 0.241, Train_accy 95.04:  65%|██████▌   | 13/20 [00:26<00:14,  2.08s/it]
Task 2, Epoch 14/20 => Loss 0.276, Train_accy 92.01:  65%|██████▌   | 13/20 [00:29<00:14,  2.08s/it]
Task 2, Epoch 14/20 => Loss 0.276, Train_accy 92.01:  70%|███████   | 14/20 [00:29<00:12,  2.09s/it]
Task 2, Epoch 15/20 => Loss 0.274, Train_accy 91.74:  70%|███████   | 14/20 [00:31<00:12,  2.09s/it]
Task 2, Epoch 15/20 => Loss 0.274, Train_accy 91.74:  75%|███████▌  | 15/20 [00:31<00:10,  2.10s/it]
Task 2, Epoch 16/20 => Loss 0.248, Train_accy 94.49:  75%|███████▌  | 15/20 [00:33<00:10,  2.10s/it]
Task 2, Epoch 16/20 => Loss 0.248, Train_accy 94.49:  80%|████████  | 16/20 [00:33<00:08,  2.09s/it]
Task 2, Epoch 17/20 => Loss 0.269, Train_accy 90.91:  80%|████████  | 16/20 [00:35<00:08,  2.09s/it]
Task 2, Epoch 17/20 => Loss 0.269, Train_accy 90.91:  85%|████████▌ | 17/20 [00:35<00:06,  2.10s/it]
Task 2, Epoch 18/20 => Loss 0.191, Train_accy 94.77:  85%|████████▌ | 17/20 [00:37<00:06,  2.10s/it]
Task 2, Epoch 18/20 => Loss 0.191, Train_accy 94.77:  90%|█████████ | 18/20 [00:37<00:04,  2.10s/it]
Task 2, Epoch 19/20 => Loss 0.229, Train_accy 93.39:  90%|█████████ | 18/20 [00:39<00:04,  2.10s/it]
Task 2, Epoch 19/20 => Loss 0.229, Train_accy 93.39:  95%|█████████▌| 19/20 [00:39<00:02,  2.11s/it]
Task 2, Epoch 20/20 => Loss 0.246, Train_accy 92.84:  95%|█████████▌| 19/20 [00:41<00:02,  2.11s/it]
Task 2, Epoch 20/20 => Loss 0.246, Train_accy 92.84: 100%|██████████| 20/20 [00:41<00:00,  2.11s/it]
Task 2, Epoch 20/20 => Loss 0.246, Train_accy 92.84: 100%|██████████| 20/20 [00:41<00:00,  2.09s/it]
2024-08-12 00:13:07,674 [inflora.py] => Task 2, Epoch 20/20 => Loss 0.246, Train_accy 92.84
Threshold:  0.955
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 10/768 type remove
Layer 3 : 14/768 type remove
Layer 4 : 15/768 type remove
Layer 5 : 24/768 type remove
Layer 6 : 21/768 type remove
Layer 7 : 19/768 type remove
Layer 8 : 27/768 type remove
Layer 9 : 53/768 type remove
Layer 10 : 74/768 type remove
Layer 11 : 18/768 type remove
Layer 12 : 44/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:13:16,953 [trainer.py] => Time:55.02937722206116
274 274
274 274
2024-08-12 00:13:18,334 [trainer.py] => Time:1.3812296390533447
2024-08-12 00:13:18,335 [inflora.py] => Exemplar size: 0
2024-08-12 00:13:18,335 [trainer.py] => CNN: {'total': 64.23, '00-09': 62.34, '10-19': 58.16, '20-29': 71.72, 'old': 60.0, 'new': 71.72}
2024-08-12 00:13:18,335 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23]
2024-08-12 00:13:18,335 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67]
2024-08-12 00:13:18,335 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029]
2024-08-12 00:13:18,887 [trainer.py] => All params: 110611171
2024-08-12 00:13:18,891 [trainer.py] => Trainable params: 81418
2024-08-12 00:13:18,891 [inflora.py] => Learning on 30-40
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_v.3.weight', 'classifier_pool.3.bias', 'image_encoder.blocks.3.attn.lora_B_k.3.weight', 'image_encoder.blocks.6.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_k.3.weight', 'image_encoder.blocks.8.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.3.weight', 'image_encoder.blocks.1.attn.lora_B_v.3.weight', 'image_encoder.blocks.5.attn.lora_B_k.3.weight', 'image_encoder.blocks.10.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.3.weight', 'image_encoder.blocks.5.attn.lora_B_v.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.3.weight', 'image_encoder.blocks.7.attn.lora_B_v.3.weight', 'image_encoder.blocks.3.attn.lora_B_v.3.weight', 'image_encoder.blocks.0.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_k.3.weight', 'image_encoder.blocks.1.attn.lora_B_k.3.weight', 'image_encoder.blocks.7.attn.lora_B_k.3.weight', 'classifier_pool.3.weight', 'image_encoder.blocks.2.attn.lora_B_v.3.weight', 'image_encoder.blocks.2.attn.lora_B_k.3.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 3, Epoch 1/20 => Loss 2.459, Train_accy 22.33:   0%|          | 0/20 [00:02<?, ?it/s]
Task 3, Epoch 1/20 => Loss 2.459, Train_accy 22.33:   5%|▌         | 1/20 [00:02<00:42,  2.23s/it]
Task 3, Epoch 2/20 => Loss 1.253, Train_accy 58.06:   5%|▌         | 1/20 [00:04<00:42,  2.23s/it]
Task 3, Epoch 2/20 => Loss 1.253, Train_accy 58.06:  10%|█         | 2/20 [00:04<00:40,  2.24s/it]
Task 3, Epoch 3/20 => Loss 0.791, Train_accy 72.21:  10%|█         | 2/20 [00:06<00:40,  2.24s/it]
Task 3, Epoch 3/20 => Loss 0.791, Train_accy 72.21:  15%|█▌        | 3/20 [00:06<00:37,  2.22s/it]
Task 3, Epoch 4/20 => Loss 0.633, Train_accy 78.41:  15%|█▌        | 3/20 [00:08<00:37,  2.22s/it]
Task 3, Epoch 4/20 => Loss 0.633, Train_accy 78.41:  20%|██        | 4/20 [00:08<00:35,  2.23s/it]
Task 3, Epoch 5/20 => Loss 0.537, Train_accy 83.37:  20%|██        | 4/20 [00:11<00:35,  2.23s/it]
Task 3, Epoch 5/20 => Loss 0.537, Train_accy 83.37:  25%|██▌       | 5/20 [00:11<00:33,  2.24s/it]
Task 3, Epoch 6/20 => Loss 0.441, Train_accy 87.10:  25%|██▌       | 5/20 [00:13<00:33,  2.24s/it]
Task 3, Epoch 6/20 => Loss 0.441, Train_accy 87.10:  30%|███       | 6/20 [00:13<00:31,  2.24s/it]
Task 3, Epoch 7/20 => Loss 0.417, Train_accy 86.85:  30%|███       | 6/20 [00:15<00:31,  2.24s/it]
Task 3, Epoch 7/20 => Loss 0.417, Train_accy 86.85:  35%|███▌      | 7/20 [00:15<00:28,  2.23s/it]
Task 3, Epoch 8/20 => Loss 0.340, Train_accy 89.58:  35%|███▌      | 7/20 [00:17<00:28,  2.23s/it]
Task 3, Epoch 8/20 => Loss 0.340, Train_accy 89.58:  40%|████      | 8/20 [00:17<00:26,  2.23s/it]
Task 3, Epoch 9/20 => Loss 0.274, Train_accy 93.30:  40%|████      | 8/20 [00:20<00:26,  2.23s/it]
Task 3, Epoch 9/20 => Loss 0.274, Train_accy 93.30:  45%|████▌     | 9/20 [00:20<00:24,  2.22s/it]
Task 3, Epoch 10/20 => Loss 0.304, Train_accy 92.06:  45%|████▌     | 9/20 [00:22<00:24,  2.22s/it]
Task 3, Epoch 10/20 => Loss 0.304, Train_accy 92.06:  50%|█████     | 10/20 [00:22<00:22,  2.22s/it]
Task 3, Epoch 11/20 => Loss 0.241, Train_accy 92.56:  50%|█████     | 10/20 [00:24<00:22,  2.22s/it]
Task 3, Epoch 11/20 => Loss 0.241, Train_accy 92.56:  55%|█████▌    | 11/20 [00:24<00:19,  2.22s/it]
Task 3, Epoch 12/20 => Loss 0.239, Train_accy 93.55:  55%|█████▌    | 11/20 [00:26<00:19,  2.22s/it]
Task 3, Epoch 12/20 => Loss 0.239, Train_accy 93.55:  60%|██████    | 12/20 [00:26<00:17,  2.21s/it]
Task 3, Epoch 13/20 => Loss 0.288, Train_accy 90.57:  60%|██████    | 12/20 [00:28<00:17,  2.21s/it]
Task 3, Epoch 13/20 => Loss 0.288, Train_accy 90.57:  65%|██████▌   | 13/20 [00:28<00:15,  2.22s/it]
Task 3, Epoch 14/20 => Loss 0.217, Train_accy 93.30:  65%|██████▌   | 13/20 [00:31<00:15,  2.22s/it]
Task 3, Epoch 14/20 => Loss 0.217, Train_accy 93.30:  70%|███████   | 14/20 [00:31<00:13,  2.22s/it]
Task 3, Epoch 15/20 => Loss 0.240, Train_accy 91.81:  70%|███████   | 14/20 [00:33<00:13,  2.22s/it]
Task 3, Epoch 15/20 => Loss 0.240, Train_accy 91.81:  75%|███████▌  | 15/20 [00:33<00:11,  2.21s/it]
Task 3, Epoch 16/20 => Loss 0.174, Train_accy 94.04:  75%|███████▌  | 15/20 [00:35<00:11,  2.21s/it]
Task 3, Epoch 16/20 => Loss 0.174, Train_accy 94.04:  80%|████████  | 16/20 [00:35<00:08,  2.23s/it]
Task 3, Epoch 17/20 => Loss 0.169, Train_accy 94.54:  80%|████████  | 16/20 [00:37<00:08,  2.23s/it]
Task 3, Epoch 17/20 => Loss 0.169, Train_accy 94.54:  85%|████████▌ | 17/20 [00:37<00:06,  2.22s/it]
Task 3, Epoch 18/20 => Loss 0.192, Train_accy 93.80:  85%|████████▌ | 17/20 [00:40<00:06,  2.22s/it]
Task 3, Epoch 18/20 => Loss 0.192, Train_accy 93.80:  90%|█████████ | 18/20 [00:40<00:04,  2.22s/it]
Task 3, Epoch 19/20 => Loss 0.229, Train_accy 92.56:  90%|█████████ | 18/20 [00:42<00:04,  2.22s/it]
Task 3, Epoch 19/20 => Loss 0.229, Train_accy 92.56:  95%|█████████▌| 19/20 [00:42<00:02,  2.22s/it]
Task 3, Epoch 20/20 => Loss 0.173, Train_accy 94.79:  95%|█████████▌| 19/20 [00:44<00:02,  2.22s/it]
Task 3, Epoch 20/20 => Loss 0.173, Train_accy 94.79: 100%|██████████| 20/20 [00:44<00:00,  2.23s/it]
Task 3, Epoch 20/20 => Loss 0.173, Train_accy 94.79: 100%|██████████| 20/20 [00:44<00:00,  2.22s/it]
2024-08-12 00:14:07,554 [inflora.py] => Task 3, Epoch 20/20 => Loss 0.173, Train_accy 94.79
Threshold:  0.9575
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 11/768 type remove
Layer 3 : 15/768 type remove
Layer 4 : 17/768 type remove
Layer 5 : 26/768 type remove
Layer 6 : 24/768 type remove
Layer 7 : 22/768 type remove
Layer 8 : 31/768 type remove
Layer 9 : 60/768 type remove
Layer 10 : 81/768 type remove
Layer 11 : 20/768 type remove
Layer 12 : 45/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:14:16,562 [trainer.py] => Time:57.67109417915344
360 360
360 360
2024-08-12 00:14:18,084 [trainer.py] => Time:1.5221757888793945
2024-08-12 00:14:18,085 [inflora.py] => Exemplar size: 0
2024-08-12 00:14:18,085 [trainer.py] => CNN: {'total': 58.89, '00-09': 57.14, '10-19': 53.06, '20-29': 65.66, '30-39': 59.3, 'old': 58.76, 'new': 59.3}
2024-08-12 00:14:18,085 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23, 58.89]
2024-08-12 00:14:18,085 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67, 83.89]
2024-08-12 00:14:18,085 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029, 0.6444444444444445]
2024-08-12 00:14:18,580 [trainer.py] => All params: 110611171
2024-08-12 00:14:18,583 [trainer.py] => Trainable params: 81418
2024-08-12 00:14:18,584 [inflora.py] => Learning on 40-50
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.4.weight', 'image_encoder.blocks.7.attn.lora_B_v.4.weight', 'image_encoder.blocks.2.attn.lora_B_k.4.weight', 'image_encoder.blocks.10.attn.lora_B_k.4.weight', 'image_encoder.blocks.3.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_v.4.weight', 'image_encoder.blocks.5.attn.lora_B_k.4.weight', 'image_encoder.blocks.0.attn.lora_B_k.4.weight', 'image_encoder.blocks.0.attn.lora_B_v.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.4.weight', 'image_encoder.blocks.7.attn.lora_B_k.4.weight', 'image_encoder.blocks.1.attn.lora_B_k.4.weight', 'image_encoder.blocks.6.attn.lora_B_v.4.weight', 'image_encoder.blocks.4.attn.lora_B_k.4.weight', 'image_encoder.blocks.11.attn.lora_B_k.4.weight', 'classifier_pool.4.weight', 'image_encoder.blocks.4.attn.lora_B_v.4.weight', 'classifier_pool.4.bias', 'image_encoder.blocks.5.attn.lora_B_v.4.weight', 'image_encoder.blocks.2.attn.lora_B_v.4.weight', 'image_encoder.blocks.8.attn.lora_B_k.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.4.weight', 'image_encoder.blocks.11.attn.lora_B_v.4.weight', 'image_encoder.blocks.1.attn.lora_B_v.4.weight', 'image_encoder.blocks.8.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_k.4.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 4, Epoch 1/20 => Loss 2.713, Train_accy 20.30:   0%|          | 0/20 [00:01<?, ?it/s]
Task 4, Epoch 1/20 => Loss 2.713, Train_accy 20.30:   5%|▌         | 1/20 [00:01<00:33,  1.78s/it]
Task 4, Epoch 2/20 => Loss 1.473, Train_accy 53.51:   5%|▌         | 1/20 [00:03<00:33,  1.78s/it]
Task 4, Epoch 2/20 => Loss 1.473, Train_accy 53.51:  10%|█         | 2/20 [00:03<00:31,  1.77s/it]
Task 4, Epoch 3/20 => Loss 1.013, Train_accy 67.53:  10%|█         | 2/20 [00:05<00:31,  1.77s/it]
Task 4, Epoch 3/20 => Loss 1.013, Train_accy 67.53:  15%|█▌        | 3/20 [00:05<00:30,  1.78s/it]
Task 4, Epoch 4/20 => Loss 0.766, Train_accy 76.01:  15%|█▌        | 3/20 [00:07<00:30,  1.78s/it]
Task 4, Epoch 4/20 => Loss 0.766, Train_accy 76.01:  20%|██        | 4/20 [00:07<00:29,  1.81s/it]
Task 4, Epoch 5/20 => Loss 0.609, Train_accy 79.34:  20%|██        | 4/20 [00:09<00:29,  1.81s/it]
Task 4, Epoch 5/20 => Loss 0.609, Train_accy 79.34:  25%|██▌       | 5/20 [00:09<00:27,  1.82s/it]
Task 4, Epoch 6/20 => Loss 0.456, Train_accy 83.76:  25%|██▌       | 5/20 [00:10<00:27,  1.82s/it]
Task 4, Epoch 6/20 => Loss 0.456, Train_accy 83.76:  30%|███       | 6/20 [00:10<00:25,  1.81s/it]
Task 4, Epoch 7/20 => Loss 0.411, Train_accy 88.56:  30%|███       | 6/20 [00:12<00:25,  1.81s/it]
Task 4, Epoch 7/20 => Loss 0.411, Train_accy 88.56:  35%|███▌      | 7/20 [00:12<00:23,  1.83s/it]
Task 4, Epoch 8/20 => Loss 0.298, Train_accy 91.14:  35%|███▌      | 7/20 [00:14<00:23,  1.83s/it]
Task 4, Epoch 8/20 => Loss 0.298, Train_accy 91.14:  40%|████      | 8/20 [00:14<00:21,  1.83s/it]
Task 4, Epoch 9/20 => Loss 0.290, Train_accy 92.62:  40%|████      | 8/20 [00:16<00:21,  1.83s/it]
Task 4, Epoch 9/20 => Loss 0.290, Train_accy 92.62:  45%|████▌     | 9/20 [00:16<00:20,  1.82s/it]
Task 4, Epoch 10/20 => Loss 0.224, Train_accy 94.10:  45%|████▌     | 9/20 [00:18<00:20,  1.82s/it]
Task 4, Epoch 10/20 => Loss 0.224, Train_accy 94.10:  50%|█████     | 10/20 [00:18<00:18,  1.81s/it]
Task 4, Epoch 11/20 => Loss 0.206, Train_accy 94.46:  50%|█████     | 10/20 [00:19<00:18,  1.81s/it]
Task 4, Epoch 11/20 => Loss 0.206, Train_accy 94.46:  55%|█████▌    | 11/20 [00:19<00:16,  1.81s/it]
Task 4, Epoch 12/20 => Loss 0.172, Train_accy 96.31:  55%|█████▌    | 11/20 [00:21<00:16,  1.81s/it]
Task 4, Epoch 12/20 => Loss 0.172, Train_accy 96.31:  60%|██████    | 12/20 [00:21<00:14,  1.80s/it]
Task 4, Epoch 13/20 => Loss 0.205, Train_accy 93.73:  60%|██████    | 12/20 [00:23<00:14,  1.80s/it]
Task 4, Epoch 13/20 => Loss 0.205, Train_accy 93.73:  65%|██████▌   | 13/20 [00:23<00:12,  1.80s/it]
Task 4, Epoch 14/20 => Loss 0.265, Train_accy 91.14:  65%|██████▌   | 13/20 [00:25<00:12,  1.80s/it]
Task 4, Epoch 14/20 => Loss 0.265, Train_accy 91.14:  70%|███████   | 14/20 [00:25<00:10,  1.80s/it]
Task 4, Epoch 15/20 => Loss 0.233, Train_accy 94.46:  70%|███████   | 14/20 [00:27<00:10,  1.80s/it]
Task 4, Epoch 15/20 => Loss 0.233, Train_accy 94.46:  75%|███████▌  | 15/20 [00:27<00:09,  1.80s/it]
Task 4, Epoch 16/20 => Loss 0.171, Train_accy 95.57:  75%|███████▌  | 15/20 [00:28<00:09,  1.80s/it]
Task 4, Epoch 16/20 => Loss 0.171, Train_accy 95.57:  80%|████████  | 16/20 [00:28<00:07,  1.81s/it]
Task 4, Epoch 17/20 => Loss 0.200, Train_accy 95.94:  80%|████████  | 16/20 [00:30<00:07,  1.81s/it]
Task 4, Epoch 17/20 => Loss 0.200, Train_accy 95.94:  85%|████████▌ | 17/20 [00:30<00:05,  1.81s/it]
Task 4, Epoch 18/20 => Loss 0.243, Train_accy 92.62:  85%|████████▌ | 17/20 [00:32<00:05,  1.81s/it]
Task 4, Epoch 18/20 => Loss 0.243, Train_accy 92.62:  90%|█████████ | 18/20 [00:32<00:03,  1.81s/it]
Task 4, Epoch 19/20 => Loss 0.175, Train_accy 95.20:  90%|█████████ | 18/20 [00:34<00:03,  1.81s/it]
Task 4, Epoch 19/20 => Loss 0.175, Train_accy 95.20:  95%|█████████▌| 19/20 [00:34<00:01,  1.81s/it]
Task 4, Epoch 20/20 => Loss 0.163, Train_accy 94.83:  95%|█████████▌| 19/20 [00:36<00:01,  1.81s/it]
Task 4, Epoch 20/20 => Loss 0.163, Train_accy 94.83: 100%|██████████| 20/20 [00:36<00:00,  1.81s/it]
Task 4, Epoch 20/20 => Loss 0.163, Train_accy 94.83: 100%|██████████| 20/20 [00:36<00:00,  1.81s/it]
2024-08-12 00:14:58,746 [inflora.py] => Task 4, Epoch 20/20 => Loss 0.163, Train_accy 94.83
Threshold:  0.96
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 16/768 type remove
Layer 4 : 18/768 type remove
Layer 5 : 29/768 type remove
Layer 6 : 26/768 type remove
Layer 7 : 25/768 type remove
Layer 8 : 36/768 type remove
Layer 9 : 68/768 type remove
Layer 10 : 92/768 type remove
Layer 11 : 23/768 type remove
Layer 12 : 47/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:15:06,810 [trainer.py] => Time:48.22631239891052
424 424
424 424
2024-08-12 00:15:08,440 [trainer.py] => Time:1.6300203800201416
2024-08-12 00:15:08,440 [inflora.py] => Exemplar size: 0
2024-08-12 00:15:08,440 [trainer.py] => CNN: {'total': 58.25, '00-09': 57.14, '10-19': 51.02, '20-29': 64.65, '30-39': 58.14, '40-49': 60.94, 'old': 57.78, 'new': 60.94}
2024-08-12 00:15:08,441 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23, 58.89, 58.25]
2024-08-12 00:15:08,441 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67, 83.89, 83.49]
2024-08-12 00:15:08,441 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029, 0.6444444444444445, 0.6320754716981132]
2024-08-12 00:15:08,921 [trainer.py] => All params: 110611171
2024-08-12 00:15:08,924 [trainer.py] => Trainable params: 81418
2024-08-12 00:15:08,924 [inflora.py] => Learning on 50-60
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.5.weight', 'image_encoder.blocks.5.attn.lora_B_k.5.weight', 'image_encoder.blocks.0.attn.lora_B_v.5.weight', 'image_encoder.blocks.2.attn.lora_B_k.5.weight', 'image_encoder.blocks.6.attn.lora_B_k.5.weight', 'image_encoder.blocks.3.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_v.5.weight', 'classifier_pool.5.weight', 'image_encoder.blocks.1.attn.lora_B_v.5.weight', 'image_encoder.blocks.10.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_k.5.weight', 'image_encoder.blocks.7.attn.lora_B_k.5.weight', 'image_encoder.blocks.9.attn.lora_B_k.5.weight', 'image_encoder.blocks.7.attn.lora_B_v.5.weight', 'image_encoder.blocks.11.attn.lora_B_k.5.weight', 'image_encoder.blocks.1.attn.lora_B_k.5.weight', 'image_encoder.blocks.0.attn.lora_B_k.5.weight', 'image_encoder.blocks.9.attn.lora_B_v.5.weight', 'image_encoder.blocks.4.attn.lora_B_k.5.weight', 'image_encoder.blocks.10.attn.lora_B_k.5.weight', 'image_encoder.blocks.3.attn.lora_B_k.5.weight', 'image_encoder.blocks.4.attn.lora_B_v.5.weight', 'classifier_pool.5.bias', 'image_encoder.blocks.2.attn.lora_B_v.5.weight', 'image_encoder.blocks.6.attn.lora_B_v.5.weight', 'image_encoder.blocks.11.attn.lora_B_v.5.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 5, Epoch 1/20 => Loss 2.569, Train_accy 22.50:   0%|          | 0/20 [00:01<?, ?it/s]
Task 5, Epoch 1/20 => Loss 2.569, Train_accy 22.50:   5%|▌         | 1/20 [00:01<00:31,  1.68s/it]
Task 5, Epoch 2/20 => Loss 1.618, Train_accy 50.83:   5%|▌         | 1/20 [00:03<00:31,  1.68s/it]
Task 5, Epoch 2/20 => Loss 1.618, Train_accy 50.83:  10%|█         | 2/20 [00:03<00:30,  1.70s/it]
Task 5, Epoch 3/20 => Loss 1.077, Train_accy 68.75:  10%|█         | 2/20 [00:05<00:30,  1.70s/it]
Task 5, Epoch 3/20 => Loss 1.077, Train_accy 68.75:  15%|█▌        | 3/20 [00:05<00:28,  1.70s/it]
Task 5, Epoch 4/20 => Loss 0.830, Train_accy 74.58:  15%|█▌        | 3/20 [00:06<00:28,  1.70s/it]
Task 5, Epoch 4/20 => Loss 0.830, Train_accy 74.58:  20%|██        | 4/20 [00:06<00:27,  1.70s/it]
Task 5, Epoch 5/20 => Loss 0.581, Train_accy 81.67:  20%|██        | 4/20 [00:08<00:27,  1.70s/it]
Task 5, Epoch 5/20 => Loss 0.581, Train_accy 81.67:  25%|██▌       | 5/20 [00:08<00:25,  1.71s/it]
Task 5, Epoch 6/20 => Loss 0.471, Train_accy 86.67:  25%|██▌       | 5/20 [00:10<00:25,  1.71s/it]
Task 5, Epoch 6/20 => Loss 0.471, Train_accy 86.67:  30%|███       | 6/20 [00:10<00:23,  1.71s/it]
Task 5, Epoch 7/20 => Loss 0.413, Train_accy 90.00:  30%|███       | 6/20 [00:11<00:23,  1.71s/it]
Task 5, Epoch 7/20 => Loss 0.413, Train_accy 90.00:  35%|███▌      | 7/20 [00:11<00:22,  1.71s/it]
Task 5, Epoch 8/20 => Loss 0.328, Train_accy 91.67:  35%|███▌      | 7/20 [00:13<00:22,  1.71s/it]
Task 5, Epoch 8/20 => Loss 0.328, Train_accy 91.67:  40%|████      | 8/20 [00:13<00:20,  1.71s/it]
Task 5, Epoch 9/20 => Loss 0.255, Train_accy 95.00:  40%|████      | 8/20 [00:15<00:20,  1.71s/it]
Task 5, Epoch 9/20 => Loss 0.255, Train_accy 95.00:  45%|████▌     | 9/20 [00:15<00:18,  1.72s/it]
Task 5, Epoch 10/20 => Loss 0.253, Train_accy 93.33:  45%|████▌     | 9/20 [00:17<00:18,  1.72s/it]
Task 5, Epoch 10/20 => Loss 0.253, Train_accy 93.33:  50%|█████     | 10/20 [00:17<00:17,  1.72s/it]
Task 5, Epoch 11/20 => Loss 0.263, Train_accy 91.67:  50%|█████     | 10/20 [00:18<00:17,  1.72s/it]
Task 5, Epoch 11/20 => Loss 0.263, Train_accy 91.67:  55%|█████▌    | 11/20 [00:18<00:15,  1.72s/it]
Task 5, Epoch 12/20 => Loss 0.170, Train_accy 96.25:  55%|█████▌    | 11/20 [00:20<00:15,  1.72s/it]
Task 5, Epoch 12/20 => Loss 0.170, Train_accy 96.25:  60%|██████    | 12/20 [00:20<00:13,  1.71s/it]
Task 5, Epoch 13/20 => Loss 0.150, Train_accy 97.92:  60%|██████    | 12/20 [00:22<00:13,  1.71s/it]
Task 5, Epoch 13/20 => Loss 0.150, Train_accy 97.92:  65%|██████▌   | 13/20 [00:22<00:12,  1.72s/it]
Task 5, Epoch 14/20 => Loss 0.197, Train_accy 96.25:  65%|██████▌   | 13/20 [00:24<00:12,  1.72s/it]
Task 5, Epoch 14/20 => Loss 0.197, Train_accy 96.25:  70%|███████   | 14/20 [00:24<00:10,  1.72s/it]
Task 5, Epoch 15/20 => Loss 0.210, Train_accy 94.17:  70%|███████   | 14/20 [00:25<00:10,  1.72s/it]
Task 5, Epoch 15/20 => Loss 0.210, Train_accy 94.17:  75%|███████▌  | 15/20 [00:25<00:08,  1.73s/it]
Task 5, Epoch 16/20 => Loss 0.143, Train_accy 96.25:  75%|███████▌  | 15/20 [00:27<00:08,  1.73s/it]
Task 5, Epoch 16/20 => Loss 0.143, Train_accy 96.25:  80%|████████  | 16/20 [00:27<00:06,  1.73s/it]
Task 5, Epoch 17/20 => Loss 0.114, Train_accy 97.50:  80%|████████  | 16/20 [00:29<00:06,  1.73s/it]
Task 5, Epoch 17/20 => Loss 0.114, Train_accy 97.50:  85%|████████▌ | 17/20 [00:29<00:05,  1.74s/it]
Task 5, Epoch 18/20 => Loss 0.170, Train_accy 95.00:  85%|████████▌ | 17/20 [00:30<00:05,  1.74s/it]
Task 5, Epoch 18/20 => Loss 0.170, Train_accy 95.00:  90%|█████████ | 18/20 [00:30<00:03,  1.73s/it]
Task 5, Epoch 19/20 => Loss 0.170, Train_accy 95.42:  90%|█████████ | 18/20 [00:32<00:03,  1.73s/it]
Task 5, Epoch 19/20 => Loss 0.170, Train_accy 95.42:  95%|█████████▌| 19/20 [00:32<00:01,  1.72s/it]
Task 5, Epoch 20/20 => Loss 0.138, Train_accy 97.92:  95%|█████████▌| 19/20 [00:34<00:01,  1.72s/it]
Task 5, Epoch 20/20 => Loss 0.138, Train_accy 97.92: 100%|██████████| 20/20 [00:34<00:00,  1.72s/it]
Task 5, Epoch 20/20 => Loss 0.138, Train_accy 97.92: 100%|██████████| 20/20 [00:34<00:00,  1.72s/it]
2024-08-12 00:15:47,167 [inflora.py] => Task 5, Epoch 20/20 => Loss 0.138, Train_accy 97.92
Threshold:  0.9624999999999999
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
Skip Updating DualGPM for layer: 4
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 17/768 type remove
Layer 4 : 18/768 type remove
Layer 5 : 30/768 type remove
Layer 6 : 27/768 type remove
Layer 7 : 27/768 type remove
Layer 8 : 38/768 type remove
Layer 9 : 69/768 type remove
Layer 10 : 93/768 type remove
Layer 11 : 24/768 type remove
Layer 12 : 49/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:15:55,240 [trainer.py] => Time:46.31534719467163
476 476
476 476
2024-08-12 00:15:57,053 [trainer.py] => Time:1.8132143020629883
2024-08-12 00:15:57,053 [inflora.py] => Exemplar size: 0
2024-08-12 00:15:57,053 [trainer.py] => CNN: {'total': 57.35, '00-09': 50.65, '10-19': 53.06, '20-29': 61.62, '30-39': 60.47, '40-49': 62.5, '50-59': 55.77, 'old': 57.55, 'new': 55.77}
2024-08-12 00:15:57,053 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23, 58.89, 58.25, 57.35]
2024-08-12 00:15:57,054 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67, 83.89, 83.49, 83.19]
2024-08-12 00:15:57,054 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029, 0.6444444444444445, 0.6320754716981132, 0.6197478991596639]
2024-08-12 00:15:57,551 [trainer.py] => All params: 110611171
2024-08-12 00:15:57,554 [trainer.py] => Trainable params: 81418
2024-08-12 00:15:57,554 [inflora.py] => Learning on 60-70
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.6.weight', 'image_encoder.blocks.11.attn.lora_B_k.6.weight', 'image_encoder.blocks.9.attn.lora_B_v.6.weight', 'image_encoder.blocks.10.attn.lora_B_v.6.weight', 'image_encoder.blocks.1.attn.lora_B_v.6.weight', 'image_encoder.blocks.0.attn.lora_B_v.6.weight', 'image_encoder.blocks.2.attn.lora_B_k.6.weight', 'image_encoder.blocks.8.attn.lora_B_k.6.weight', 'image_encoder.blocks.3.attn.lora_B_k.6.weight', 'image_encoder.blocks.7.attn.lora_B_k.6.weight', 'image_encoder.blocks.4.attn.lora_B_v.6.weight', 'image_encoder.blocks.7.attn.lora_B_v.6.weight', 'image_encoder.blocks.1.attn.lora_B_k.6.weight', 'image_encoder.blocks.6.attn.lora_B_k.6.weight', 'image_encoder.blocks.6.attn.lora_B_v.6.weight', 'image_encoder.blocks.5.attn.lora_B_v.6.weight', 'image_encoder.blocks.5.attn.lora_B_k.6.weight', 'classifier_pool.6.weight', 'image_encoder.blocks.3.attn.lora_B_v.6.weight', 'image_encoder.blocks.4.attn.lora_B_k.6.weight', 'image_encoder.blocks.9.attn.lora_B_k.6.weight', 'image_encoder.blocks.10.attn.lora_B_k.6.weight', 'classifier_pool.6.bias', 'image_encoder.blocks.8.attn.lora_B_v.6.weight', 'image_encoder.blocks.2.attn.lora_B_v.6.weight', 'image_encoder.blocks.0.attn.lora_B_k.6.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 6, Epoch 1/20 => Loss 2.224, Train_accy 30.28:   0%|          | 0/20 [00:02<?, ?it/s]
Task 6, Epoch 1/20 => Loss 2.224, Train_accy 30.28:   5%|▌         | 1/20 [00:02<00:47,  2.49s/it]
Task 6, Epoch 2/20 => Loss 1.122, Train_accy 63.30:   5%|▌         | 1/20 [00:04<00:47,  2.49s/it]
Task 6, Epoch 2/20 => Loss 1.122, Train_accy 63.30:  10%|█         | 2/20 [00:04<00:44,  2.45s/it]
Task 6, Epoch 3/20 => Loss 0.659, Train_accy 78.90:  10%|█         | 2/20 [00:07<00:44,  2.45s/it]
Task 6, Epoch 3/20 => Loss 0.659, Train_accy 78.90:  15%|█▌        | 3/20 [00:07<00:41,  2.43s/it]
Task 6, Epoch 4/20 => Loss 0.537, Train_accy 82.57:  15%|█▌        | 3/20 [00:09<00:41,  2.43s/it]
Task 6, Epoch 4/20 => Loss 0.537, Train_accy 82.57:  20%|██        | 4/20 [00:09<00:39,  2.45s/it]
Task 6, Epoch 5/20 => Loss 0.554, Train_accy 85.78:  20%|██        | 4/20 [00:12<00:39,  2.45s/it]
Task 6, Epoch 5/20 => Loss 0.554, Train_accy 85.78:  25%|██▌       | 5/20 [00:12<00:36,  2.44s/it]
Task 6, Epoch 6/20 => Loss 0.357, Train_accy 88.53:  25%|██▌       | 5/20 [00:14<00:36,  2.44s/it]
Task 6, Epoch 6/20 => Loss 0.357, Train_accy 88.53:  30%|███       | 6/20 [00:14<00:34,  2.43s/it]
Task 6, Epoch 7/20 => Loss 0.426, Train_accy 89.91:  30%|███       | 6/20 [00:17<00:34,  2.43s/it]
Task 6, Epoch 7/20 => Loss 0.426, Train_accy 89.91:  35%|███▌      | 7/20 [00:17<00:31,  2.45s/it]
Task 6, Epoch 8/20 => Loss 0.359, Train_accy 88.53:  35%|███▌      | 7/20 [00:19<00:31,  2.45s/it]
Task 6, Epoch 8/20 => Loss 0.359, Train_accy 88.53:  40%|████      | 8/20 [00:19<00:29,  2.44s/it]
Task 6, Epoch 9/20 => Loss 0.293, Train_accy 91.74:  40%|████      | 8/20 [00:22<00:29,  2.44s/it]
Task 6, Epoch 9/20 => Loss 0.293, Train_accy 91.74:  45%|████▌     | 9/20 [00:22<00:27,  2.46s/it]
Task 6, Epoch 10/20 => Loss 0.300, Train_accy 91.28:  45%|████▌     | 9/20 [00:24<00:27,  2.46s/it]
Task 6, Epoch 10/20 => Loss 0.300, Train_accy 91.28:  50%|█████     | 10/20 [00:24<00:24,  2.50s/it]
Task 6, Epoch 11/20 => Loss 0.220, Train_accy 93.12:  50%|█████     | 10/20 [00:27<00:24,  2.50s/it]
Task 6, Epoch 11/20 => Loss 0.220, Train_accy 93.12:  55%|█████▌    | 11/20 [00:27<00:22,  2.48s/it]
Task 6, Epoch 12/20 => Loss 0.227, Train_accy 91.97:  55%|█████▌    | 11/20 [00:29<00:22,  2.48s/it]
Task 6, Epoch 12/20 => Loss 0.227, Train_accy 91.97:  60%|██████    | 12/20 [00:29<00:19,  2.49s/it]
Task 6, Epoch 13/20 => Loss 0.203, Train_accy 94.50:  60%|██████    | 12/20 [00:32<00:19,  2.49s/it]
Task 6, Epoch 13/20 => Loss 0.203, Train_accy 94.50:  65%|██████▌   | 13/20 [00:32<00:17,  2.49s/it]
Task 6, Epoch 14/20 => Loss 0.174, Train_accy 94.04:  65%|██████▌   | 13/20 [00:34<00:17,  2.49s/it]
Task 6, Epoch 14/20 => Loss 0.174, Train_accy 94.04:  70%|███████   | 14/20 [00:34<00:14,  2.49s/it]
Task 6, Epoch 15/20 => Loss 0.175, Train_accy 94.50:  70%|███████   | 14/20 [00:37<00:14,  2.49s/it]
Task 6, Epoch 15/20 => Loss 0.175, Train_accy 94.50:  75%|███████▌  | 15/20 [00:37<00:12,  2.48s/it]
Task 6, Epoch 16/20 => Loss 0.239, Train_accy 93.12:  75%|███████▌  | 15/20 [00:39<00:12,  2.48s/it]
Task 6, Epoch 16/20 => Loss 0.239, Train_accy 93.12:  80%|████████  | 16/20 [00:39<00:09,  2.47s/it]
Task 6, Epoch 17/20 => Loss 0.190, Train_accy 94.27:  80%|████████  | 16/20 [00:41<00:09,  2.47s/it]
Task 6, Epoch 17/20 => Loss 0.190, Train_accy 94.27:  85%|████████▌ | 17/20 [00:41<00:07,  2.47s/it]
Task 6, Epoch 18/20 => Loss 0.142, Train_accy 96.56:  85%|████████▌ | 17/20 [00:44<00:07,  2.47s/it]
Task 6, Epoch 18/20 => Loss 0.142, Train_accy 96.56:  90%|█████████ | 18/20 [00:44<00:04,  2.47s/it]
Task 6, Epoch 19/20 => Loss 0.210, Train_accy 94.27:  90%|█████████ | 18/20 [00:46<00:04,  2.47s/it]
Task 6, Epoch 19/20 => Loss 0.210, Train_accy 94.27:  95%|█████████▌| 19/20 [00:46<00:02,  2.46s/it]
Task 6, Epoch 20/20 => Loss 0.146, Train_accy 95.18:  95%|█████████▌| 19/20 [00:49<00:02,  2.46s/it]
Task 6, Epoch 20/20 => Loss 0.146, Train_accy 95.18: 100%|██████████| 20/20 [00:49<00:00,  2.48s/it]
Task 6, Epoch 20/20 => Loss 0.146, Train_accy 95.18: 100%|██████████| 20/20 [00:49<00:00,  2.47s/it]
2024-08-12 00:16:51,444 [inflora.py] => Task 6, Epoch 20/20 => Loss 0.146, Train_accy 95.18
Threshold:  0.965
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 18/768 type remove
Layer 4 : 19/768 type remove
Layer 5 : 33/768 type remove
Layer 6 : 31/768 type remove
Layer 7 : 31/768 type remove
Layer 8 : 44/768 type remove
Layer 9 : 75/768 type remove
Layer 10 : 97/768 type remove
Layer 11 : 25/768 type remove
Layer 12 : 50/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:17:00,451 [trainer.py] => Time:62.89680862426758
602 602
602 602
2024-08-12 00:17:02,514 [trainer.py] => Time:2.0622260570526123
2024-08-12 00:17:02,514 [inflora.py] => Exemplar size: 0
2024-08-12 00:17:02,514 [trainer.py] => CNN: {'total': 56.64, '00-09': 51.95, '10-19': 52.04, '20-29': 59.6, '30-39': 58.14, '40-49': 56.25, '50-59': 57.69, '60-69': 59.52, 'old': 55.88, 'new': 59.52}
2024-08-12 00:17:02,514 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23, 58.89, 58.25, 57.35, 56.64]
2024-08-12 00:17:02,514 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67, 83.89, 83.49, 83.19, 84.72]
2024-08-12 00:17:02,514 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029, 0.6444444444444445, 0.6320754716981132, 0.6197478991596639, 0.6029900332225914]
2024-08-12 00:17:03,003 [trainer.py] => All params: 110611171
2024-08-12 00:17:03,006 [trainer.py] => Trainable params: 81418
2024-08-12 00:17:03,006 [inflora.py] => Learning on 70-80
Parameters to be updated: {'classifier_pool.7.weight', 'image_encoder.blocks.4.attn.lora_B_v.7.weight', 'image_encoder.blocks.2.attn.lora_B_v.7.weight', 'image_encoder.blocks.0.attn.lora_B_v.7.weight', 'image_encoder.blocks.2.attn.lora_B_k.7.weight', 'image_encoder.blocks.4.attn.lora_B_k.7.weight', 'image_encoder.blocks.7.attn.lora_B_k.7.weight', 'classifier_pool.7.bias', 'image_encoder.blocks.5.attn.lora_B_v.7.weight', 'image_encoder.blocks.8.attn.lora_B_k.7.weight', 'image_encoder.blocks.3.attn.lora_B_k.7.weight', 'image_encoder.blocks.0.attn.lora_B_k.7.weight', 'image_encoder.blocks.6.attn.lora_B_v.7.weight', 'image_encoder.blocks.8.attn.lora_B_v.7.weight', 'image_encoder.blocks.6.attn.lora_B_k.7.weight', 'image_encoder.blocks.11.attn.lora_B_k.7.weight', 'image_encoder.blocks.9.attn.lora_B_v.7.weight', 'image_encoder.blocks.10.attn.lora_B_k.7.weight', 'image_encoder.blocks.1.attn.lora_B_v.7.weight', 'image_encoder.blocks.1.attn.lora_B_k.7.weight', 'image_encoder.blocks.9.attn.lora_B_k.7.weight', 'image_encoder.blocks.3.attn.lora_B_v.7.weight', 'image_encoder.blocks.11.attn.lora_B_v.7.weight', 'image_encoder.blocks.10.attn.lora_B_v.7.weight', 'image_encoder.blocks.7.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_k.7.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 7, Epoch 1/20 => Loss 2.222, Train_accy 29.31:   0%|          | 0/20 [00:02<?, ?it/s]
Task 7, Epoch 1/20 => Loss 2.222, Train_accy 29.31:   5%|▌         | 1/20 [00:02<00:40,  2.14s/it]
Task 7, Epoch 2/20 => Loss 1.042, Train_accy 64.08:   5%|▌         | 1/20 [00:04<00:40,  2.14s/it]
Task 7, Epoch 2/20 => Loss 1.042, Train_accy 64.08:  10%|█         | 2/20 [00:04<00:38,  2.12s/it]
Task 7, Epoch 3/20 => Loss 0.659, Train_accy 79.02:  10%|█         | 2/20 [00:06<00:38,  2.12s/it]
Task 7, Epoch 3/20 => Loss 0.659, Train_accy 79.02:  15%|█▌        | 3/20 [00:06<00:35,  2.12s/it]
Task 7, Epoch 4/20 => Loss 0.525, Train_accy 81.61:  15%|█▌        | 3/20 [00:08<00:35,  2.12s/it]
Task 7, Epoch 4/20 => Loss 0.525, Train_accy 81.61:  20%|██        | 4/20 [00:08<00:34,  2.13s/it]
Task 7, Epoch 5/20 => Loss 0.390, Train_accy 89.37:  20%|██        | 4/20 [00:10<00:34,  2.13s/it]
Task 7, Epoch 5/20 => Loss 0.390, Train_accy 89.37:  25%|██▌       | 5/20 [00:10<00:31,  2.12s/it]
Task 7, Epoch 6/20 => Loss 0.295, Train_accy 90.80:  25%|██▌       | 5/20 [00:12<00:31,  2.12s/it]
Task 7, Epoch 6/20 => Loss 0.295, Train_accy 90.80:  30%|███       | 6/20 [00:12<00:29,  2.12s/it]
Task 7, Epoch 7/20 => Loss 0.238, Train_accy 92.82:  30%|███       | 6/20 [00:14<00:29,  2.12s/it]
Task 7, Epoch 7/20 => Loss 0.238, Train_accy 92.82:  35%|███▌      | 7/20 [00:14<00:27,  2.15s/it]
Task 7, Epoch 8/20 => Loss 0.222, Train_accy 92.24:  35%|███▌      | 7/20 [00:17<00:27,  2.15s/it]
Task 7, Epoch 8/20 => Loss 0.222, Train_accy 92.24:  40%|████      | 8/20 [00:17<00:25,  2.14s/it]
Task 7, Epoch 9/20 => Loss 0.223, Train_accy 93.68:  40%|████      | 8/20 [00:19<00:25,  2.14s/it]
Task 7, Epoch 9/20 => Loss 0.223, Train_accy 93.68:  45%|████▌     | 9/20 [00:19<00:23,  2.14s/it]
Task 7, Epoch 10/20 => Loss 0.153, Train_accy 95.40:  45%|████▌     | 9/20 [00:21<00:23,  2.14s/it]
Task 7, Epoch 10/20 => Loss 0.153, Train_accy 95.40:  50%|█████     | 10/20 [00:21<00:21,  2.13s/it]
Task 7, Epoch 11/20 => Loss 0.139, Train_accy 96.55:  50%|█████     | 10/20 [00:23<00:21,  2.13s/it]
Task 7, Epoch 11/20 => Loss 0.139, Train_accy 96.55:  55%|█████▌    | 11/20 [00:23<00:19,  2.13s/it]
Task 7, Epoch 12/20 => Loss 0.226, Train_accy 92.24:  55%|█████▌    | 11/20 [00:25<00:19,  2.13s/it]
Task 7, Epoch 12/20 => Loss 0.226, Train_accy 92.24:  60%|██████    | 12/20 [00:25<00:17,  2.13s/it]
Task 7, Epoch 13/20 => Loss 0.147, Train_accy 95.98:  60%|██████    | 12/20 [00:27<00:17,  2.13s/it]
Task 7, Epoch 13/20 => Loss 0.147, Train_accy 95.98:  65%|██████▌   | 13/20 [00:27<00:14,  2.13s/it]
Task 7, Epoch 14/20 => Loss 0.169, Train_accy 94.54:  65%|██████▌   | 13/20 [00:29<00:14,  2.13s/it]
Task 7, Epoch 14/20 => Loss 0.169, Train_accy 94.54:  70%|███████   | 14/20 [00:29<00:12,  2.13s/it]
Task 7, Epoch 15/20 => Loss 0.202, Train_accy 92.82:  70%|███████   | 14/20 [00:31<00:12,  2.13s/it]
Task 7, Epoch 15/20 => Loss 0.202, Train_accy 92.82:  75%|███████▌  | 15/20 [00:31<00:10,  2.13s/it]
Task 7, Epoch 16/20 => Loss 0.142, Train_accy 96.55:  75%|███████▌  | 15/20 [00:34<00:10,  2.13s/it]
Task 7, Epoch 16/20 => Loss 0.142, Train_accy 96.55:  80%|████████  | 16/20 [00:34<00:08,  2.13s/it]
Task 7, Epoch 17/20 => Loss 0.152, Train_accy 95.11:  80%|████████  | 16/20 [00:36<00:08,  2.13s/it]
Task 7, Epoch 17/20 => Loss 0.152, Train_accy 95.11:  85%|████████▌ | 17/20 [00:36<00:06,  2.13s/it]
Task 7, Epoch 18/20 => Loss 0.140, Train_accy 95.98:  85%|████████▌ | 17/20 [00:38<00:06,  2.13s/it]
Task 7, Epoch 18/20 => Loss 0.140, Train_accy 95.98:  90%|█████████ | 18/20 [00:38<00:04,  2.13s/it]
Task 7, Epoch 19/20 => Loss 0.128, Train_accy 97.41:  90%|█████████ | 18/20 [00:40<00:04,  2.13s/it]
Task 7, Epoch 19/20 => Loss 0.128, Train_accy 97.41:  95%|█████████▌| 19/20 [00:40<00:02,  2.14s/it]
Task 7, Epoch 20/20 => Loss 0.115, Train_accy 96.55:  95%|█████████▌| 19/20 [00:42<00:02,  2.14s/it]
Task 7, Epoch 20/20 => Loss 0.115, Train_accy 96.55: 100%|██████████| 20/20 [00:42<00:00,  2.14s/it]
Task 7, Epoch 20/20 => Loss 0.115, Train_accy 96.55: 100%|██████████| 20/20 [00:42<00:00,  2.13s/it]
2024-08-12 00:17:49,643 [inflora.py] => Task 7, Epoch 20/20 => Loss 0.115, Train_accy 96.55
Threshold:  0.9675
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 19/768 type remove
Layer 4 : 20/768 type remove
Layer 5 : 35/768 type remove
Layer 6 : 33/768 type remove
Layer 7 : 34/768 type remove
Layer 8 : 48/768 type remove
Layer 9 : 83/768 type remove
Layer 10 : 107/768 type remove
Layer 11 : 31/768 type remove
Layer 12 : 53/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:17:58,240 [trainer.py] => Time:55.233643770217896
698 698
698 698
2024-08-12 00:18:00,455 [trainer.py] => Time:2.214646339416504
2024-08-12 00:18:00,455 [inflora.py] => Exemplar size: 0
2024-08-12 00:18:00,455 [trainer.py] => CNN: {'total': 55.87, '00-09': 46.75, '10-19': 50.0, '20-29': 60.61, '30-39': 55.81, '40-49': 54.69, '50-59': 55.77, '60-69': 59.52, '70-79': 60.42, 'old': 55.15, 'new': 60.42}
2024-08-12 00:18:00,455 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23, 58.89, 58.25, 57.35, 56.64, 55.87]
2024-08-12 00:18:00,455 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67, 83.89, 83.49, 83.19, 84.72, 84.67]
2024-08-12 00:18:00,455 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029, 0.6444444444444445, 0.6320754716981132, 0.6197478991596639, 0.6029900332225914, 0.5859598853868195]
2024-08-12 00:18:00,932 [trainer.py] => All params: 110611171
2024-08-12 00:18:00,934 [trainer.py] => Trainable params: 81418
2024-08-12 00:18:00,935 [inflora.py] => Learning on 80-90
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_v.8.weight', 'image_encoder.blocks.2.attn.lora_B_k.8.weight', 'classifier_pool.8.weight', 'image_encoder.blocks.7.attn.lora_B_k.8.weight', 'image_encoder.blocks.5.attn.lora_B_v.8.weight', 'image_encoder.blocks.6.attn.lora_B_k.8.weight', 'image_encoder.blocks.0.attn.lora_B_k.8.weight', 'image_encoder.blocks.11.attn.lora_B_v.8.weight', 'image_encoder.blocks.11.attn.lora_B_k.8.weight', 'image_encoder.blocks.0.attn.lora_B_v.8.weight', 'image_encoder.blocks.9.attn.lora_B_v.8.weight', 'image_encoder.blocks.10.attn.lora_B_k.8.weight', 'image_encoder.blocks.1.attn.lora_B_v.8.weight', 'image_encoder.blocks.2.attn.lora_B_v.8.weight', 'image_encoder.blocks.7.attn.lora_B_v.8.weight', 'image_encoder.blocks.10.attn.lora_B_v.8.weight', 'image_encoder.blocks.6.attn.lora_B_v.8.weight', 'image_encoder.blocks.1.attn.lora_B_k.8.weight', 'classifier_pool.8.bias', 'image_encoder.blocks.9.attn.lora_B_k.8.weight', 'image_encoder.blocks.3.attn.lora_B_v.8.weight', 'image_encoder.blocks.8.attn.lora_B_v.8.weight', 'image_encoder.blocks.3.attn.lora_B_k.8.weight', 'image_encoder.blocks.5.attn.lora_B_k.8.weight', 'image_encoder.blocks.4.attn.lora_B_k.8.weight', 'image_encoder.blocks.8.attn.lora_B_k.8.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 8, Epoch 1/20 => Loss 2.446, Train_accy 16.96:   0%|          | 0/20 [00:01<?, ?it/s]
Task 8, Epoch 1/20 => Loss 2.446, Train_accy 16.96:   5%|▌         | 1/20 [00:01<00:37,  1.96s/it]
Task 8, Epoch 2/20 => Loss 1.385, Train_accy 54.33:   5%|▌         | 1/20 [00:03<00:37,  1.96s/it]
Task 8, Epoch 2/20 => Loss 1.385, Train_accy 54.33:  10%|█         | 2/20 [00:03<00:35,  1.97s/it]
Task 8, Epoch 3/20 => Loss 0.765, Train_accy 72.32:  10%|█         | 2/20 [00:05<00:35,  1.97s/it]
Task 8, Epoch 3/20 => Loss 0.765, Train_accy 72.32:  15%|█▌        | 3/20 [00:05<00:33,  2.00s/it]
Task 8, Epoch 4/20 => Loss 0.788, Train_accy 80.97:  15%|█▌        | 3/20 [00:07<00:33,  2.00s/it]
Task 8, Epoch 4/20 => Loss 0.788, Train_accy 80.97:  20%|██        | 4/20 [00:07<00:31,  1.98s/it]
Task 8, Epoch 5/20 => Loss 0.371, Train_accy 87.54:  20%|██        | 4/20 [00:09<00:31,  1.98s/it]
Task 8, Epoch 5/20 => Loss 0.371, Train_accy 87.54:  25%|██▌       | 5/20 [00:09<00:29,  1.98s/it]
Task 8, Epoch 6/20 => Loss 0.347, Train_accy 91.70:  25%|██▌       | 5/20 [00:11<00:29,  1.98s/it]
Task 8, Epoch 6/20 => Loss 0.347, Train_accy 91.70:  30%|███       | 6/20 [00:11<00:27,  1.98s/it]
Task 8, Epoch 7/20 => Loss 0.295, Train_accy 89.97:  30%|███       | 6/20 [00:13<00:27,  1.98s/it]
Task 8, Epoch 7/20 => Loss 0.295, Train_accy 89.97:  35%|███▌      | 7/20 [00:13<00:25,  1.98s/it]
Task 8, Epoch 8/20 => Loss 0.258, Train_accy 92.04:  35%|███▌      | 7/20 [00:15<00:25,  1.98s/it]
Task 8, Epoch 8/20 => Loss 0.258, Train_accy 92.04:  40%|████      | 8/20 [00:15<00:23,  1.99s/it]
Task 8, Epoch 9/20 => Loss 0.221, Train_accy 93.08:  40%|████      | 8/20 [00:17<00:23,  1.99s/it]
Task 8, Epoch 9/20 => Loss 0.221, Train_accy 93.08:  45%|████▌     | 9/20 [00:17<00:21,  2.00s/it]
Task 8, Epoch 10/20 => Loss 0.278, Train_accy 93.43:  45%|████▌     | 9/20 [00:19<00:21,  2.00s/it]
Task 8, Epoch 10/20 => Loss 0.278, Train_accy 93.43:  50%|█████     | 10/20 [00:19<00:19,  1.99s/it]
Task 8, Epoch 11/20 => Loss 0.142, Train_accy 96.89:  50%|█████     | 10/20 [00:21<00:19,  1.99s/it]
Task 8, Epoch 11/20 => Loss 0.142, Train_accy 96.89:  55%|█████▌    | 11/20 [00:21<00:17,  1.98s/it]
Task 8, Epoch 12/20 => Loss 0.161, Train_accy 95.85:  55%|█████▌    | 11/20 [00:23<00:17,  1.98s/it]
Task 8, Epoch 12/20 => Loss 0.161, Train_accy 95.85:  60%|██████    | 12/20 [00:23<00:15,  2.00s/it]
Task 8, Epoch 13/20 => Loss 0.210, Train_accy 92.39:  60%|██████    | 12/20 [00:25<00:15,  2.00s/it]
Task 8, Epoch 13/20 => Loss 0.210, Train_accy 92.39:  65%|██████▌   | 13/20 [00:25<00:13,  1.98s/it]
Task 8, Epoch 14/20 => Loss 0.176, Train_accy 95.85:  65%|██████▌   | 13/20 [00:27<00:13,  1.98s/it]
Task 8, Epoch 14/20 => Loss 0.176, Train_accy 95.85:  70%|███████   | 14/20 [00:27<00:11,  1.98s/it]
Task 8, Epoch 15/20 => Loss 0.145, Train_accy 95.16:  70%|███████   | 14/20 [00:29<00:11,  1.98s/it]
Task 8, Epoch 15/20 => Loss 0.145, Train_accy 95.16:  75%|███████▌  | 15/20 [00:29<00:09,  1.98s/it]
Task 8, Epoch 16/20 => Loss 0.140, Train_accy 95.16:  75%|███████▌  | 15/20 [00:31<00:09,  1.98s/it]
Task 8, Epoch 16/20 => Loss 0.140, Train_accy 95.16:  80%|████████  | 16/20 [00:31<00:07,  1.98s/it]
Task 8, Epoch 17/20 => Loss 0.119, Train_accy 96.89:  80%|████████  | 16/20 [00:33<00:07,  1.98s/it]
Task 8, Epoch 17/20 => Loss 0.119, Train_accy 96.89:  85%|████████▌ | 17/20 [00:33<00:05,  1.98s/it]
Task 8, Epoch 18/20 => Loss 0.130, Train_accy 96.89:  85%|████████▌ | 17/20 [00:35<00:05,  1.98s/it]
Task 8, Epoch 18/20 => Loss 0.130, Train_accy 96.89:  90%|█████████ | 18/20 [00:35<00:03,  1.97s/it]
Task 8, Epoch 19/20 => Loss 0.162, Train_accy 93.08:  90%|█████████ | 18/20 [00:37<00:03,  1.97s/it]
Task 8, Epoch 19/20 => Loss 0.162, Train_accy 93.08:  95%|█████████▌| 19/20 [00:37<00:01,  1.99s/it]
Task 8, Epoch 20/20 => Loss 0.147, Train_accy 95.85:  95%|█████████▌| 19/20 [00:39<00:01,  1.99s/it]
Task 8, Epoch 20/20 => Loss 0.147, Train_accy 95.85: 100%|██████████| 20/20 [00:39<00:00,  1.98s/it]
Task 8, Epoch 20/20 => Loss 0.147, Train_accy 95.85: 100%|██████████| 20/20 [00:39<00:00,  1.98s/it]
2024-08-12 00:18:44,663 [inflora.py] => Task 8, Epoch 20/20 => Loss 0.147, Train_accy 95.85
Threshold:  0.97
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 21/768 type remove
Layer 4 : 23/768 type remove
Layer 5 : 40/768 type remove
Layer 6 : 38/768 type remove
Layer 7 : 38/768 type remove
Layer 8 : 58/768 type remove
Layer 9 : 96/768 type remove
Layer 10 : 122/768 type remove
Layer 11 : 38/768 type remove
Layer 12 : 59/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:18:52,941 [trainer.py] => Time:52.006861448287964
779 779
779 779
2024-08-12 00:18:55,299 [trainer.py] => Time:2.3574321269989014
2024-08-12 00:18:55,299 [inflora.py] => Exemplar size: 0
2024-08-12 00:18:55,299 [trainer.py] => CNN: {'total': 54.3, '00-09': 50.65, '10-19': 50.0, '20-29': 56.57, '30-39': 55.81, '40-49': 54.69, '50-59': 53.85, '60-69': 58.73, '70-79': 57.29, '80-89': 48.15, 'old': 55.01, 'new': 48.15}
2024-08-12 00:18:55,299 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23, 58.89, 58.25, 57.35, 56.64, 55.87, 54.3]
2024-08-12 00:18:55,299 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67, 83.89, 83.49, 83.19, 84.72, 84.67, 84.47]
2024-08-12 00:18:55,300 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029, 0.6444444444444445, 0.6320754716981132, 0.6197478991596639, 0.6029900332225914, 0.5859598853868195, 0.5661103979460848]
2024-08-12 00:18:55,918 [trainer.py] => All params: 110611171
2024-08-12 00:18:55,921 [trainer.py] => Trainable params: 81418
2024-08-12 00:18:55,921 [inflora.py] => Learning on 90-100
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.9.weight', 'image_encoder.blocks.4.attn.lora_B_k.9.weight', 'classifier_pool.9.weight', 'image_encoder.blocks.9.attn.lora_B_v.9.weight', 'image_encoder.blocks.1.attn.lora_B_k.9.weight', 'image_encoder.blocks.8.attn.lora_B_k.9.weight', 'image_encoder.blocks.11.attn.lora_B_k.9.weight', 'image_encoder.blocks.8.attn.lora_B_v.9.weight', 'image_encoder.blocks.4.attn.lora_B_v.9.weight', 'image_encoder.blocks.11.attn.lora_B_v.9.weight', 'image_encoder.blocks.6.attn.lora_B_k.9.weight', 'image_encoder.blocks.0.attn.lora_B_k.9.weight', 'image_encoder.blocks.3.attn.lora_B_k.9.weight', 'image_encoder.blocks.6.attn.lora_B_v.9.weight', 'image_encoder.blocks.2.attn.lora_B_v.9.weight', 'image_encoder.blocks.1.attn.lora_B_v.9.weight', 'image_encoder.blocks.7.attn.lora_B_k.9.weight', 'image_encoder.blocks.9.attn.lora_B_k.9.weight', 'image_encoder.blocks.10.attn.lora_B_k.9.weight', 'image_encoder.blocks.5.attn.lora_B_k.9.weight', 'image_encoder.blocks.3.attn.lora_B_v.9.weight', 'image_encoder.blocks.0.attn.lora_B_v.9.weight', 'classifier_pool.9.bias', 'image_encoder.blocks.7.attn.lora_B_v.9.weight', 'image_encoder.blocks.2.attn.lora_B_k.9.weight', 'image_encoder.blocks.10.attn.lora_B_v.9.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 9, Epoch 1/20 => Loss 2.999, Train_accy 14.89:   0%|          | 0/20 [00:01<?, ?it/s]
Task 9, Epoch 1/20 => Loss 2.999, Train_accy 14.89:   5%|▌         | 1/20 [00:01<00:33,  1.77s/it]
Task 9, Epoch 2/20 => Loss 1.735, Train_accy 43.83:   5%|▌         | 1/20 [00:03<00:33,  1.77s/it]
Task 9, Epoch 2/20 => Loss 1.735, Train_accy 43.83:  10%|█         | 2/20 [00:03<00:32,  1.78s/it]
Task 9, Epoch 3/20 => Loss 1.161, Train_accy 62.98:  10%|█         | 2/20 [00:05<00:32,  1.78s/it]
Task 9, Epoch 3/20 => Loss 1.161, Train_accy 62.98:  15%|█▌        | 3/20 [00:05<00:30,  1.78s/it]
Task 9, Epoch 4/20 => Loss 0.901, Train_accy 69.36:  15%|█▌        | 3/20 [00:07<00:30,  1.78s/it]
Task 9, Epoch 4/20 => Loss 0.901, Train_accy 69.36:  20%|██        | 4/20 [00:07<00:28,  1.78s/it]
Task 9, Epoch 5/20 => Loss 0.659, Train_accy 76.60:  20%|██        | 4/20 [00:08<00:28,  1.78s/it]
Task 9, Epoch 5/20 => Loss 0.659, Train_accy 76.60:  25%|██▌       | 5/20 [00:08<00:26,  1.78s/it]
Task 9, Epoch 6/20 => Loss 0.522, Train_accy 82.55:  25%|██▌       | 5/20 [00:10<00:26,  1.78s/it]
Task 9, Epoch 6/20 => Loss 0.522, Train_accy 82.55:  30%|███       | 6/20 [00:10<00:24,  1.78s/it]
Task 9, Epoch 7/20 => Loss 0.501, Train_accy 83.83:  30%|███       | 6/20 [00:12<00:24,  1.78s/it]
Task 9, Epoch 7/20 => Loss 0.501, Train_accy 83.83:  35%|███▌      | 7/20 [00:12<00:23,  1.79s/it]
Task 9, Epoch 8/20 => Loss 0.399, Train_accy 86.81:  35%|███▌      | 7/20 [00:14<00:23,  1.79s/it]
Task 9, Epoch 8/20 => Loss 0.399, Train_accy 86.81:  40%|████      | 8/20 [00:14<00:21,  1.79s/it]
Task 9, Epoch 9/20 => Loss 0.317, Train_accy 90.64:  40%|████      | 8/20 [00:16<00:21,  1.79s/it]
Task 9, Epoch 9/20 => Loss 0.317, Train_accy 90.64:  45%|████▌     | 9/20 [00:16<00:19,  1.79s/it]
Task 9, Epoch 10/20 => Loss 0.284, Train_accy 91.91:  45%|████▌     | 9/20 [00:17<00:19,  1.79s/it]
Task 9, Epoch 10/20 => Loss 0.284, Train_accy 91.91:  50%|█████     | 10/20 [00:17<00:17,  1.79s/it]
Task 9, Epoch 11/20 => Loss 0.266, Train_accy 92.34:  50%|█████     | 10/20 [00:19<00:17,  1.79s/it]
Task 9, Epoch 11/20 => Loss 0.266, Train_accy 92.34:  55%|█████▌    | 11/20 [00:19<00:16,  1.80s/it]
Task 9, Epoch 12/20 => Loss 0.232, Train_accy 93.62:  55%|█████▌    | 11/20 [00:21<00:16,  1.80s/it]
Task 9, Epoch 12/20 => Loss 0.232, Train_accy 93.62:  60%|██████    | 12/20 [00:21<00:14,  1.80s/it]
Task 9, Epoch 13/20 => Loss 0.257, Train_accy 94.04:  60%|██████    | 12/20 [00:23<00:14,  1.80s/it]
Task 9, Epoch 13/20 => Loss 0.257, Train_accy 94.04:  65%|██████▌   | 13/20 [00:23<00:12,  1.79s/it]
Task 9, Epoch 14/20 => Loss 0.252, Train_accy 93.19:  65%|██████▌   | 13/20 [00:25<00:12,  1.79s/it]
Task 9, Epoch 14/20 => Loss 0.252, Train_accy 93.19:  70%|███████   | 14/20 [00:25<00:10,  1.78s/it]
Task 9, Epoch 15/20 => Loss 0.207, Train_accy 94.47:  70%|███████   | 14/20 [00:26<00:10,  1.78s/it]
Task 9, Epoch 15/20 => Loss 0.207, Train_accy 94.47:  75%|███████▌  | 15/20 [00:26<00:08,  1.78s/it]
Task 9, Epoch 16/20 => Loss 0.197, Train_accy 96.60:  75%|███████▌  | 15/20 [00:28<00:08,  1.78s/it]
Task 9, Epoch 16/20 => Loss 0.197, Train_accy 96.60:  80%|████████  | 16/20 [00:28<00:07,  1.78s/it]
Task 9, Epoch 17/20 => Loss 0.193, Train_accy 94.89:  80%|████████  | 16/20 [00:30<00:07,  1.78s/it]
Task 9, Epoch 17/20 => Loss 0.193, Train_accy 94.89:  85%|████████▌ | 17/20 [00:30<00:05,  1.79s/it]
Task 9, Epoch 18/20 => Loss 0.212, Train_accy 94.47:  85%|████████▌ | 17/20 [00:32<00:05,  1.79s/it]
Task 9, Epoch 18/20 => Loss 0.212, Train_accy 94.47:  90%|█████████ | 18/20 [00:32<00:03,  1.79s/it]
Task 9, Epoch 19/20 => Loss 0.190, Train_accy 94.47:  90%|█████████ | 18/20 [00:33<00:03,  1.79s/it]
Task 9, Epoch 19/20 => Loss 0.190, Train_accy 94.47:  95%|█████████▌| 19/20 [00:33<00:01,  1.79s/it]
Task 9, Epoch 20/20 => Loss 0.194, Train_accy 94.04:  95%|█████████▌| 19/20 [00:35<00:01,  1.79s/it]
Task 9, Epoch 20/20 => Loss 0.194, Train_accy 94.04: 100%|██████████| 20/20 [00:35<00:00,  1.78s/it]
Task 9, Epoch 20/20 => Loss 0.194, Train_accy 94.04: 100%|██████████| 20/20 [00:35<00:00,  1.79s/it]
2024-08-12 00:19:35,706 [inflora.py] => Task 9, Epoch 20/20 => Loss 0.194, Train_accy 94.04
Threshold:  0.9724999999999999
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 23/768 type remove
Layer 4 : 25/768 type remove
Layer 5 : 43/768 type remove
Layer 6 : 41/768 type remove
Layer 7 : 41/768 type remove
Layer 8 : 61/768 type remove
Layer 9 : 102/768 type remove
Layer 10 : 135/768 type remove
Layer 11 : 45/768 type remove
Layer 12 : 68/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:19:45,067 [trainer.py] => Time:49.14577269554138
841 841
841 841
2024-08-12 00:19:47,592 [trainer.py] => Time:2.5243844985961914
2024-08-12 00:19:47,592 [inflora.py] => Exemplar size: 0
2024-08-12 00:19:47,592 [trainer.py] => CNN: {'total': 51.25, '00-09': 44.16, '10-19': 47.96, '20-29': 54.55, '30-39': 55.81, '40-49': 54.69, '50-59': 51.92, '60-69': 54.76, '70-79': 52.08, '80-89': 46.91, '90-99': 46.77, 'old': 51.6, 'new': 46.77}
2024-08-12 00:19:47,592 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23, 58.89, 58.25, 57.35, 56.64, 55.87, 54.3, 51.25]
2024-08-12 00:19:47,592 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67, 83.89, 83.49, 83.19, 84.72, 84.67, 84.47, 84.19]
2024-08-12 00:19:47,592 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029, 0.6444444444444445, 0.6320754716981132, 0.6197478991596639, 0.6029900332225914, 0.5859598853868195, 0.5661103979460848, 0.539833531510107]
2024-08-12 00:19:48,087 [trainer.py] => All params: 110611171
2024-08-12 00:19:48,090 [trainer.py] => Trainable params: 81418
2024-08-12 00:19:48,090 [inflora.py] => Learning on 100-110
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.10.weight', 'image_encoder.blocks.0.attn.lora_B_k.10.weight', 'image_encoder.blocks.4.attn.lora_B_k.10.weight', 'image_encoder.blocks.4.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_k.10.weight', 'image_encoder.blocks.10.attn.lora_B_v.10.weight', 'classifier_pool.10.weight', 'image_encoder.blocks.6.attn.lora_B_v.10.weight', 'image_encoder.blocks.0.attn.lora_B_v.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.10.weight', 'image_encoder.blocks.11.attn.lora_B_k.10.weight', 'image_encoder.blocks.11.attn.lora_B_v.10.weight', 'image_encoder.blocks.6.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_k.10.weight', 'classifier_pool.10.bias', 'image_encoder.blocks.7.attn.lora_B_v.10.weight', 'image_encoder.blocks.7.attn.lora_B_k.10.weight', 'image_encoder.blocks.5.attn.lora_B_v.10.weight', 'image_encoder.blocks.1.attn.lora_B_k.10.weight', 'image_encoder.blocks.1.attn.lora_B_v.10.weight', 'image_encoder.blocks.3.attn.lora_B_k.10.weight', 'image_encoder.blocks.10.attn.lora_B_k.10.weight', 'image_encoder.blocks.5.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.10.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 10, Epoch 1/20 => Loss 2.439, Train_accy 22.45:   0%|          | 0/20 [00:01<?, ?it/s]
Task 10, Epoch 1/20 => Loss 2.439, Train_accy 22.45:   5%|▌         | 1/20 [00:01<00:34,  1.81s/it]
Task 10, Epoch 2/20 => Loss 1.576, Train_accy 50.20:   5%|▌         | 1/20 [00:03<00:34,  1.81s/it]
Task 10, Epoch 2/20 => Loss 1.576, Train_accy 50.20:  10%|█         | 2/20 [00:03<00:32,  1.83s/it]
Task 10, Epoch 3/20 => Loss 1.309, Train_accy 66.53:  10%|█         | 2/20 [00:05<00:32,  1.83s/it]
Task 10, Epoch 3/20 => Loss 1.309, Train_accy 66.53:  15%|█▌        | 3/20 [00:05<00:31,  1.85s/it]
Task 10, Epoch 4/20 => Loss 0.877, Train_accy 77.55:  15%|█▌        | 3/20 [00:07<00:31,  1.85s/it]
Task 10, Epoch 4/20 => Loss 0.877, Train_accy 77.55:  20%|██        | 4/20 [00:07<00:29,  1.84s/it]
Task 10, Epoch 5/20 => Loss 0.642, Train_accy 80.41:  20%|██        | 4/20 [00:09<00:29,  1.84s/it]
Task 10, Epoch 5/20 => Loss 0.642, Train_accy 80.41:  25%|██▌       | 5/20 [00:09<00:27,  1.85s/it]
Task 10, Epoch 6/20 => Loss 0.462, Train_accy 88.16:  25%|██▌       | 5/20 [00:11<00:27,  1.85s/it]
Task 10, Epoch 6/20 => Loss 0.462, Train_accy 88.16:  30%|███       | 6/20 [00:11<00:25,  1.85s/it]
Task 10, Epoch 7/20 => Loss 0.343, Train_accy 89.80:  30%|███       | 6/20 [00:12<00:25,  1.85s/it]
Task 10, Epoch 7/20 => Loss 0.343, Train_accy 89.80:  35%|███▌      | 7/20 [00:12<00:24,  1.86s/it]
Task 10, Epoch 8/20 => Loss 0.320, Train_accy 91.84:  35%|███▌      | 7/20 [00:14<00:24,  1.86s/it]
Task 10, Epoch 8/20 => Loss 0.320, Train_accy 91.84:  40%|████      | 8/20 [00:14<00:22,  1.86s/it]
Task 10, Epoch 9/20 => Loss 0.361, Train_accy 92.24:  40%|████      | 8/20 [00:16<00:22,  1.86s/it]
Task 10, Epoch 9/20 => Loss 0.361, Train_accy 92.24:  45%|████▌     | 9/20 [00:16<00:20,  1.88s/it]
Task 10, Epoch 10/20 => Loss 0.269, Train_accy 93.06:  45%|████▌     | 9/20 [00:18<00:20,  1.88s/it]
Task 10, Epoch 10/20 => Loss 0.269, Train_accy 93.06:  50%|█████     | 10/20 [00:18<00:18,  1.87s/it]
Task 10, Epoch 11/20 => Loss 0.234, Train_accy 94.29:  50%|█████     | 10/20 [00:20<00:18,  1.87s/it]
Task 10, Epoch 11/20 => Loss 0.234, Train_accy 94.29:  55%|█████▌    | 11/20 [00:20<00:16,  1.87s/it]
Task 10, Epoch 12/20 => Loss 0.269, Train_accy 91.02:  55%|█████▌    | 11/20 [00:22<00:16,  1.87s/it]
Task 10, Epoch 12/20 => Loss 0.269, Train_accy 91.02:  60%|██████    | 12/20 [00:22<00:14,  1.87s/it]
Task 10, Epoch 13/20 => Loss 0.233, Train_accy 95.10:  60%|██████    | 12/20 [00:24<00:14,  1.87s/it]
Task 10, Epoch 13/20 => Loss 0.233, Train_accy 95.10:  65%|██████▌   | 13/20 [00:24<00:13,  1.88s/it]
Task 10, Epoch 14/20 => Loss 0.191, Train_accy 93.88:  65%|██████▌   | 13/20 [00:26<00:13,  1.88s/it]
Task 10, Epoch 14/20 => Loss 0.191, Train_accy 93.88:  70%|███████   | 14/20 [00:26<00:11,  1.88s/it]
Task 10, Epoch 15/20 => Loss 0.213, Train_accy 93.06:  70%|███████   | 14/20 [00:27<00:11,  1.88s/it]
Task 10, Epoch 15/20 => Loss 0.213, Train_accy 93.06:  75%|███████▌  | 15/20 [00:27<00:09,  1.88s/it]
Task 10, Epoch 16/20 => Loss 0.220, Train_accy 93.88:  75%|███████▌  | 15/20 [00:29<00:09,  1.88s/it]
Task 10, Epoch 16/20 => Loss 0.220, Train_accy 93.88:  80%|████████  | 16/20 [00:29<00:07,  1.88s/it]
Task 10, Epoch 17/20 => Loss 0.204, Train_accy 93.88:  80%|████████  | 16/20 [00:31<00:07,  1.88s/it]
Task 10, Epoch 17/20 => Loss 0.204, Train_accy 93.88:  85%|████████▌ | 17/20 [00:31<00:05,  1.87s/it]
Task 10, Epoch 18/20 => Loss 0.146, Train_accy 97.55:  85%|████████▌ | 17/20 [00:33<00:05,  1.87s/it]
Task 10, Epoch 18/20 => Loss 0.146, Train_accy 97.55:  90%|█████████ | 18/20 [00:33<00:03,  1.87s/it]
Task 10, Epoch 19/20 => Loss 0.192, Train_accy 95.92:  90%|█████████ | 18/20 [00:35<00:03,  1.87s/it]
Task 10, Epoch 19/20 => Loss 0.192, Train_accy 95.92:  95%|█████████▌| 19/20 [00:35<00:01,  1.87s/it]
Task 10, Epoch 20/20 => Loss 0.133, Train_accy 95.51:  95%|█████████▌| 19/20 [00:37<00:01,  1.87s/it]
Task 10, Epoch 20/20 => Loss 0.133, Train_accy 95.51: 100%|██████████| 20/20 [00:37<00:00,  1.89s/it]
Task 10, Epoch 20/20 => Loss 0.133, Train_accy 95.51: 100%|██████████| 20/20 [00:37<00:00,  1.87s/it]
2024-08-12 00:20:29,624 [inflora.py] => Task 10, Epoch 20/20 => Loss 0.133, Train_accy 95.51
Threshold:  0.975
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 24/768 type remove
Layer 4 : 26/768 type remove
Layer 5 : 44/768 type remove
Layer 6 : 43/768 type remove
Layer 7 : 44/768 type remove
Layer 8 : 65/768 type remove
Layer 9 : 108/768 type remove
Layer 10 : 142/768 type remove
Layer 11 : 50/768 type remove
Layer 12 : 75/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:20:38,319 [trainer.py] => Time:50.22927665710449
901 901
901 901
2024-08-12 00:20:41,045 [trainer.py] => Time:2.7251663208007812
2024-08-12 00:20:41,045 [inflora.py] => Exemplar size: 0
2024-08-12 00:20:41,045 [trainer.py] => CNN: {'total': 50.28, '00-09': 44.16, '10-19': 47.96, '20-29': 56.57, '30-39': 56.98, '40-49': 53.12, '50-59': 46.15, '60-69': 53.17, '70-79': 51.04, '80-89': 46.91, '90-99': 46.77, '100-109': 43.33, 'old': 50.77, 'new': 43.33}
2024-08-12 00:20:41,045 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23, 58.89, 58.25, 57.35, 56.64, 55.87, 54.3, 51.25, 50.28]
2024-08-12 00:20:41,045 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67, 83.89, 83.49, 83.19, 84.72, 84.67, 84.47, 84.19, 84.13]
2024-08-12 00:20:41,045 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029, 0.6444444444444445, 0.6320754716981132, 0.6197478991596639, 0.6029900332225914, 0.5859598853868195, 0.5661103979460848, 0.539833531510107, 0.5294117647058824]
2024-08-12 00:20:41,578 [trainer.py] => All params: 110611171
2024-08-12 00:20:41,581 [trainer.py] => Trainable params: 81418
2024-08-12 00:20:41,581 [inflora.py] => Learning on 110-120
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.11.weight', 'image_encoder.blocks.11.attn.lora_B_k.11.weight', 'classifier_pool.11.bias', 'image_encoder.blocks.2.attn.lora_B_v.11.weight', 'image_encoder.blocks.3.attn.lora_B_k.11.weight', 'image_encoder.blocks.5.attn.lora_B_v.11.weight', 'image_encoder.blocks.8.attn.lora_B_v.11.weight', 'image_encoder.blocks.6.attn.lora_B_k.11.weight', 'image_encoder.blocks.1.attn.lora_B_k.11.weight', 'image_encoder.blocks.7.attn.lora_B_v.11.weight', 'classifier_pool.11.weight', 'image_encoder.blocks.3.attn.lora_B_v.11.weight', 'image_encoder.blocks.0.attn.lora_B_k.11.weight', 'image_encoder.blocks.10.attn.lora_B_v.11.weight', 'image_encoder.blocks.11.attn.lora_B_v.11.weight', 'image_encoder.blocks.1.attn.lora_B_v.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.11.weight', 'image_encoder.blocks.9.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_v.11.weight', 'image_encoder.blocks.8.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_k.11.weight', 'image_encoder.blocks.5.attn.lora_B_k.11.weight', 'image_encoder.blocks.6.attn.lora_B_v.11.weight', 'image_encoder.blocks.7.attn.lora_B_k.11.weight', 'image_encoder.blocks.9.attn.lora_B_v.11.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 11, Epoch 1/20 => Loss 2.403, Train_accy 21.94:   0%|          | 0/20 [00:02<?, ?it/s]
Task 11, Epoch 1/20 => Loss 2.403, Train_accy 21.94:   5%|▌         | 1/20 [00:02<00:39,  2.08s/it]
Task 11, Epoch 2/20 => Loss 1.252, Train_accy 60.07:   5%|▌         | 1/20 [00:04<00:39,  2.08s/it]
Task 11, Epoch 2/20 => Loss 1.252, Train_accy 60.07:  10%|█         | 2/20 [00:04<00:36,  2.03s/it]
Task 11, Epoch 3/20 => Loss 0.850, Train_accy 72.66:  10%|█         | 2/20 [00:06<00:36,  2.03s/it]
Task 11, Epoch 3/20 => Loss 0.850, Train_accy 72.66:  15%|█▌        | 3/20 [00:06<00:33,  2.00s/it]
Task 11, Epoch 4/20 => Loss 0.581, Train_accy 82.01:  15%|█▌        | 3/20 [00:07<00:33,  2.00s/it]
Task 11, Epoch 4/20 => Loss 0.581, Train_accy 82.01:  20%|██        | 4/20 [00:07<00:31,  1.98s/it]
Task 11, Epoch 5/20 => Loss 0.480, Train_accy 84.53:  20%|██        | 4/20 [00:09<00:31,  1.98s/it]
Task 11, Epoch 5/20 => Loss 0.480, Train_accy 84.53:  25%|██▌       | 5/20 [00:09<00:29,  1.98s/it]
Task 11, Epoch 6/20 => Loss 0.358, Train_accy 88.49:  25%|██▌       | 5/20 [00:11<00:29,  1.98s/it]
Task 11, Epoch 6/20 => Loss 0.358, Train_accy 88.49:  30%|███       | 6/20 [00:11<00:27,  1.99s/it]
Task 11, Epoch 7/20 => Loss 0.325, Train_accy 91.73:  30%|███       | 6/20 [00:13<00:27,  1.99s/it]
Task 11, Epoch 7/20 => Loss 0.325, Train_accy 91.73:  35%|███▌      | 7/20 [00:13<00:25,  1.98s/it]
Task 11, Epoch 8/20 => Loss 0.310, Train_accy 91.73:  35%|███▌      | 7/20 [00:15<00:25,  1.98s/it]
Task 11, Epoch 8/20 => Loss 0.310, Train_accy 91.73:  40%|████      | 8/20 [00:15<00:23,  2.00s/it]
Task 11, Epoch 9/20 => Loss 0.236, Train_accy 94.60:  40%|████      | 8/20 [00:17<00:23,  2.00s/it]
Task 11, Epoch 9/20 => Loss 0.236, Train_accy 94.60:  45%|████▌     | 9/20 [00:17<00:21,  1.99s/it]
Task 11, Epoch 10/20 => Loss 0.226, Train_accy 93.53:  45%|████▌     | 9/20 [00:20<00:21,  1.99s/it]
Task 11, Epoch 10/20 => Loss 0.226, Train_accy 93.53:  50%|█████     | 10/20 [00:20<00:20,  2.01s/it]
Task 11, Epoch 11/20 => Loss 0.177, Train_accy 96.40:  50%|█████     | 10/20 [00:21<00:20,  2.01s/it]
Task 11, Epoch 11/20 => Loss 0.177, Train_accy 96.40:  55%|█████▌    | 11/20 [00:21<00:18,  2.00s/it]
Task 11, Epoch 12/20 => Loss 0.182, Train_accy 94.60:  55%|█████▌    | 11/20 [00:23<00:18,  2.00s/it]
Task 11, Epoch 12/20 => Loss 0.182, Train_accy 94.60:  60%|██████    | 12/20 [00:23<00:15,  2.00s/it]
Task 11, Epoch 13/20 => Loss 0.192, Train_accy 94.24:  60%|██████    | 12/20 [00:25<00:15,  2.00s/it]
Task 11, Epoch 13/20 => Loss 0.192, Train_accy 94.24:  65%|██████▌   | 13/20 [00:25<00:13,  1.99s/it]
Task 11, Epoch 14/20 => Loss 0.168, Train_accy 96.04:  65%|██████▌   | 13/20 [00:27<00:13,  1.99s/it]
Task 11, Epoch 14/20 => Loss 0.168, Train_accy 96.04:  70%|███████   | 14/20 [00:27<00:11,  1.98s/it]
Task 11, Epoch 15/20 => Loss 0.167, Train_accy 95.68:  70%|███████   | 14/20 [00:29<00:11,  1.98s/it]
Task 11, Epoch 15/20 => Loss 0.167, Train_accy 95.68:  75%|███████▌  | 15/20 [00:29<00:09,  1.99s/it]
Task 11, Epoch 16/20 => Loss 0.152, Train_accy 95.32:  75%|███████▌  | 15/20 [00:31<00:09,  1.99s/it]
Task 11, Epoch 16/20 => Loss 0.152, Train_accy 95.32:  80%|████████  | 16/20 [00:31<00:07,  2.00s/it]
Task 11, Epoch 17/20 => Loss 0.164, Train_accy 95.68:  80%|████████  | 16/20 [00:33<00:07,  2.00s/it]
Task 11, Epoch 17/20 => Loss 0.164, Train_accy 95.68:  85%|████████▌ | 17/20 [00:33<00:05,  1.99s/it]
Task 11, Epoch 18/20 => Loss 0.141, Train_accy 96.76:  85%|████████▌ | 17/20 [00:35<00:05,  1.99s/it]
Task 11, Epoch 18/20 => Loss 0.141, Train_accy 96.76:  90%|█████████ | 18/20 [00:35<00:04,  2.00s/it]
Task 11, Epoch 19/20 => Loss 0.133, Train_accy 97.48:  90%|█████████ | 18/20 [00:37<00:04,  2.00s/it]
Task 11, Epoch 19/20 => Loss 0.133, Train_accy 97.48:  95%|█████████▌| 19/20 [00:37<00:02,  2.00s/it]
Task 11, Epoch 20/20 => Loss 0.152, Train_accy 95.68:  95%|█████████▌| 19/20 [00:39<00:02,  2.00s/it]
Task 11, Epoch 20/20 => Loss 0.152, Train_accy 95.68: 100%|██████████| 20/20 [00:39<00:00,  2.00s/it]
Task 11, Epoch 20/20 => Loss 0.152, Train_accy 95.68: 100%|██████████| 20/20 [00:39<00:00,  2.00s/it]
2024-08-12 00:21:25,982 [inflora.py] => Task 11, Epoch 20/20 => Loss 0.152, Train_accy 95.68
Threshold:  0.9775
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 27/768 type remove
Layer 4 : 30/768 type remove
Layer 5 : 50/768 type remove
Layer 6 : 49/768 type remove
Layer 7 : 49/768 type remove
Layer 8 : 72/768 type remove
Layer 9 : 115/768 type remove
Layer 10 : 152/768 type remove
Layer 11 : 56/768 type remove
Layer 12 : 79/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:21:34,604 [trainer.py] => Time:53.02292275428772
975 975
975 975
2024-08-12 00:21:37,376 [trainer.py] => Time:2.771684408187866
2024-08-12 00:21:37,377 [inflora.py] => Exemplar size: 0
2024-08-12 00:21:37,377 [trainer.py] => CNN: {'total': 49.95, '00-09': 44.16, '10-19': 46.94, '20-29': 58.59, '30-39': 55.81, '40-49': 54.69, '50-59': 46.15, '60-69': 53.17, '70-79': 52.08, '80-89': 46.91, '90-99': 43.55, '100-109': 41.67, '110-119': 47.3, 'old': 50.17, 'new': 47.3}
2024-08-12 00:21:37,377 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23, 58.89, 58.25, 57.35, 56.64, 55.87, 54.3, 51.25, 50.28, 49.95]
2024-08-12 00:21:37,377 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67, 83.89, 83.49, 83.19, 84.72, 84.67, 84.47, 84.19, 84.13, 84.92]
2024-08-12 00:21:37,377 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029, 0.6444444444444445, 0.6320754716981132, 0.6197478991596639, 0.6029900332225914, 0.5859598853868195, 0.5661103979460848, 0.539833531510107, 0.5294117647058824, 0.5230769230769231]
2024-08-12 00:21:37,892 [trainer.py] => All params: 110611171
2024-08-12 00:21:37,895 [trainer.py] => Trainable params: 81418
2024-08-12 00:21:37,895 [inflora.py] => Learning on 120-130
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_k.12.weight', 'image_encoder.blocks.9.attn.lora_B_k.12.weight', 'image_encoder.blocks.9.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_k.12.weight', 'image_encoder.blocks.3.attn.lora_B_k.12.weight', 'image_encoder.blocks.11.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_v.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.12.weight', 'classifier_pool.12.bias', 'image_encoder.blocks.7.attn.lora_B_v.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.12.weight', 'image_encoder.blocks.0.attn.lora_B_v.12.weight', 'image_encoder.blocks.10.attn.lora_B_k.12.weight', 'image_encoder.blocks.0.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_v.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.12.weight', 'image_encoder.blocks.1.attn.lora_B_k.12.weight', 'image_encoder.blocks.4.attn.lora_B_k.12.weight', 'image_encoder.blocks.5.attn.lora_B_k.12.weight', 'image_encoder.blocks.2.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_k.12.weight', 'image_encoder.blocks.1.attn.lora_B_v.12.weight', 'image_encoder.blocks.8.attn.lora_B_v.12.weight', 'image_encoder.blocks.2.attn.lora_B_v.12.weight', 'image_encoder.blocks.10.attn.lora_B_v.12.weight', 'classifier_pool.12.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 12, Epoch 1/20 => Loss 2.923, Train_accy 13.04:   0%|          | 0/20 [00:01<?, ?it/s]
Task 12, Epoch 1/20 => Loss 2.923, Train_accy 13.04:   5%|▌         | 1/20 [00:01<00:34,  1.81s/it]
Task 12, Epoch 2/20 => Loss 1.959, Train_accy 36.96:   5%|▌         | 1/20 [00:03<00:34,  1.81s/it]
Task 12, Epoch 2/20 => Loss 1.959, Train_accy 36.96:  10%|█         | 2/20 [00:03<00:33,  1.83s/it]
Task 12, Epoch 3/20 => Loss 1.390, Train_accy 57.83:  10%|█         | 2/20 [00:05<00:33,  1.83s/it]
Task 12, Epoch 3/20 => Loss 1.390, Train_accy 57.83:  15%|█▌        | 3/20 [00:05<00:31,  1.85s/it]
Task 12, Epoch 4/20 => Loss 1.164, Train_accy 64.78:  15%|█▌        | 3/20 [00:07<00:31,  1.85s/it]
Task 12, Epoch 4/20 => Loss 1.164, Train_accy 64.78:  20%|██        | 4/20 [00:07<00:29,  1.85s/it]
Task 12, Epoch 5/20 => Loss 0.806, Train_accy 77.83:  20%|██        | 4/20 [00:09<00:29,  1.85s/it]
Task 12, Epoch 5/20 => Loss 0.806, Train_accy 77.83:  25%|██▌       | 5/20 [00:09<00:27,  1.85s/it]
Task 12, Epoch 6/20 => Loss 0.711, Train_accy 79.57:  25%|██▌       | 5/20 [00:11<00:27,  1.85s/it]
Task 12, Epoch 6/20 => Loss 0.711, Train_accy 79.57:  30%|███       | 6/20 [00:11<00:25,  1.85s/it]
Task 12, Epoch 7/20 => Loss 0.629, Train_accy 82.17:  30%|███       | 6/20 [00:12<00:25,  1.85s/it]
Task 12, Epoch 7/20 => Loss 0.629, Train_accy 82.17:  35%|███▌      | 7/20 [00:12<00:24,  1.86s/it]
Task 12, Epoch 8/20 => Loss 0.533, Train_accy 85.65:  35%|███▌      | 7/20 [00:14<00:24,  1.86s/it]
Task 12, Epoch 8/20 => Loss 0.533, Train_accy 85.65:  40%|████      | 8/20 [00:14<00:22,  1.85s/it]
Task 12, Epoch 9/20 => Loss 0.529, Train_accy 87.39:  40%|████      | 8/20 [00:16<00:22,  1.85s/it]
Task 12, Epoch 9/20 => Loss 0.529, Train_accy 87.39:  45%|████▌     | 9/20 [00:16<00:20,  1.85s/it]
Task 12, Epoch 10/20 => Loss 0.435, Train_accy 87.83:  45%|████▌     | 9/20 [00:18<00:20,  1.85s/it]
Task 12, Epoch 10/20 => Loss 0.435, Train_accy 87.83:  50%|█████     | 10/20 [00:18<00:18,  1.84s/it]
Task 12, Epoch 11/20 => Loss 0.384, Train_accy 88.70:  50%|█████     | 10/20 [00:20<00:18,  1.84s/it]
Task 12, Epoch 11/20 => Loss 0.384, Train_accy 88.70:  55%|█████▌    | 11/20 [00:20<00:16,  1.84s/it]
Task 12, Epoch 12/20 => Loss 0.357, Train_accy 89.13:  55%|█████▌    | 11/20 [00:22<00:16,  1.84s/it]
Task 12, Epoch 12/20 => Loss 0.357, Train_accy 89.13:  60%|██████    | 12/20 [00:22<00:14,  1.84s/it]
Task 12, Epoch 13/20 => Loss 0.391, Train_accy 88.70:  60%|██████    | 12/20 [00:23<00:14,  1.84s/it]
Task 12, Epoch 13/20 => Loss 0.391, Train_accy 88.70:  65%|██████▌   | 13/20 [00:23<00:12,  1.84s/it]
Task 12, Epoch 14/20 => Loss 0.256, Train_accy 94.78:  65%|██████▌   | 13/20 [00:25<00:12,  1.84s/it]
Task 12, Epoch 14/20 => Loss 0.256, Train_accy 94.78:  70%|███████   | 14/20 [00:25<00:11,  1.84s/it]
Task 12, Epoch 15/20 => Loss 0.296, Train_accy 93.91:  70%|███████   | 14/20 [00:27<00:11,  1.84s/it]
Task 12, Epoch 15/20 => Loss 0.296, Train_accy 93.91:  75%|███████▌  | 15/20 [00:27<00:09,  1.85s/it]
Task 12, Epoch 16/20 => Loss 0.285, Train_accy 91.30:  75%|███████▌  | 15/20 [00:29<00:09,  1.85s/it]
Task 12, Epoch 16/20 => Loss 0.285, Train_accy 91.30:  80%|████████  | 16/20 [00:29<00:07,  1.86s/it]
Task 12, Epoch 17/20 => Loss 0.314, Train_accy 90.00:  80%|████████  | 16/20 [00:31<00:07,  1.86s/it]
Task 12, Epoch 17/20 => Loss 0.314, Train_accy 90.00:  85%|████████▌ | 17/20 [00:31<00:05,  1.86s/it]
Task 12, Epoch 18/20 => Loss 0.250, Train_accy 93.48:  85%|████████▌ | 17/20 [00:33<00:05,  1.86s/it]
Task 12, Epoch 18/20 => Loss 0.250, Train_accy 93.48:  90%|█████████ | 18/20 [00:33<00:03,  1.87s/it]
Task 12, Epoch 19/20 => Loss 0.232, Train_accy 93.91:  90%|█████████ | 18/20 [00:35<00:03,  1.87s/it]
Task 12, Epoch 19/20 => Loss 0.232, Train_accy 93.91:  95%|█████████▌| 19/20 [00:35<00:01,  1.85s/it]
Task 12, Epoch 20/20 => Loss 0.295, Train_accy 93.04:  95%|█████████▌| 19/20 [00:36<00:01,  1.85s/it]
Task 12, Epoch 20/20 => Loss 0.295, Train_accy 93.04: 100%|██████████| 20/20 [00:36<00:00,  1.85s/it]
Task 12, Epoch 20/20 => Loss 0.295, Train_accy 93.04: 100%|██████████| 20/20 [00:36<00:00,  1.85s/it]
2024-08-12 00:22:18,920 [inflora.py] => Task 12, Epoch 20/20 => Loss 0.295, Train_accy 93.04
Threshold:  0.98
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 29/768 type remove
Layer 4 : 33/768 type remove
Layer 5 : 55/768 type remove
Layer 6 : 55/768 type remove
Layer 7 : 55/768 type remove
Layer 8 : 83/768 type remove
Layer 9 : 139/768 type remove
Layer 10 : 185/768 type remove
Layer 11 : 76/768 type remove
Layer 12 : 103/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:22:27,784 [trainer.py] => Time:49.888752937316895
1035 1035
1035 1035
2024-08-12 00:22:30,712 [trainer.py] => Time:2.9276797771453857
2024-08-12 00:22:30,712 [inflora.py] => Exemplar size: 0
2024-08-12 00:22:30,712 [trainer.py] => CNN: {'total': 48.79, '00-09': 44.16, '10-19': 45.92, '20-29': 57.58, '30-39': 56.98, '40-49': 54.69, '50-59': 48.08, '60-69': 53.17, '70-79': 46.88, '80-89': 46.91, '90-99': 45.16, '100-109': 43.33, '110-119': 50.0, '120-129': 31.67, 'old': 49.85, 'new': 31.67}
2024-08-12 00:22:30,712 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23, 58.89, 58.25, 57.35, 56.64, 55.87, 54.3, 51.25, 50.28, 49.95, 48.79]
2024-08-12 00:22:30,712 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67, 83.89, 83.49, 83.19, 84.72, 84.67, 84.47, 84.19, 84.13, 84.92, 83.96]
2024-08-12 00:22:30,712 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029, 0.6444444444444445, 0.6320754716981132, 0.6197478991596639, 0.6029900332225914, 0.5859598853868195, 0.5661103979460848, 0.539833531510107, 0.5294117647058824, 0.5230769230769231, 0.5053140096618357]
2024-08-12 00:22:31,226 [trainer.py] => All params: 110611171
2024-08-12 00:22:31,229 [trainer.py] => Trainable params: 81418
2024-08-12 00:22:31,229 [inflora.py] => Learning on 130-140
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.13.weight', 'image_encoder.blocks.11.attn.lora_B_k.13.weight', 'image_encoder.blocks.9.attn.lora_B_v.13.weight', 'image_encoder.blocks.11.attn.lora_B_v.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.13.weight', 'image_encoder.blocks.8.attn.lora_B_v.13.weight', 'image_encoder.blocks.7.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_v.13.weight', 'image_encoder.blocks.9.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.13.weight', 'image_encoder.blocks.7.attn.lora_B_k.13.weight', 'image_encoder.blocks.2.attn.lora_B_k.13.weight', 'image_encoder.blocks.0.attn.lora_B_v.13.weight', 'image_encoder.blocks.10.attn.lora_B_k.13.weight', 'classifier_pool.13.weight', 'image_encoder.blocks.1.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_k.13.weight', 'image_encoder.blocks.4.attn.lora_B_k.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.13.weight', 'classifier_pool.13.bias', 'image_encoder.blocks.1.attn.lora_B_k.13.weight', 'image_encoder.blocks.0.attn.lora_B_k.13.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 13, Epoch 1/20 => Loss 2.514, Train_accy 17.80:   0%|          | 0/20 [00:02<?, ?it/s]
Task 13, Epoch 1/20 => Loss 2.514, Train_accy 17.80:   5%|▌         | 1/20 [00:02<00:40,  2.12s/it]
Task 13, Epoch 2/20 => Loss 1.428, Train_accy 54.69:   5%|▌         | 1/20 [00:04<00:40,  2.12s/it]
Task 13, Epoch 2/20 => Loss 1.428, Train_accy 54.69:  10%|█         | 2/20 [00:04<00:38,  2.13s/it]
Task 13, Epoch 3/20 => Loss 0.927, Train_accy 72.82:  10%|█         | 2/20 [00:06<00:38,  2.13s/it]
Task 13, Epoch 3/20 => Loss 0.927, Train_accy 72.82:  15%|█▌        | 3/20 [00:06<00:36,  2.13s/it]
Task 13, Epoch 4/20 => Loss 0.678, Train_accy 77.67:  15%|█▌        | 3/20 [00:08<00:36,  2.13s/it]
Task 13, Epoch 4/20 => Loss 0.678, Train_accy 77.67:  20%|██        | 4/20 [00:08<00:34,  2.14s/it]
Task 13, Epoch 5/20 => Loss 0.594, Train_accy 78.96:  20%|██        | 4/20 [00:10<00:34,  2.14s/it]
Task 13, Epoch 5/20 => Loss 0.594, Train_accy 78.96:  25%|██▌       | 5/20 [00:10<00:32,  2.14s/it]
Task 13, Epoch 6/20 => Loss 0.429, Train_accy 87.38:  25%|██▌       | 5/20 [00:12<00:32,  2.14s/it]
Task 13, Epoch 6/20 => Loss 0.429, Train_accy 87.38:  30%|███       | 6/20 [00:12<00:30,  2.15s/it]
Task 13, Epoch 7/20 => Loss 0.398, Train_accy 87.06:  30%|███       | 6/20 [00:15<00:30,  2.15s/it]
Task 13, Epoch 7/20 => Loss 0.398, Train_accy 87.06:  35%|███▌      | 7/20 [00:15<00:27,  2.15s/it]
Task 13, Epoch 8/20 => Loss 0.358, Train_accy 90.61:  35%|███▌      | 7/20 [00:17<00:27,  2.15s/it]
Task 13, Epoch 8/20 => Loss 0.358, Train_accy 90.61:  40%|████      | 8/20 [00:17<00:26,  2.18s/it]
Task 13, Epoch 9/20 => Loss 0.297, Train_accy 90.94:  40%|████      | 8/20 [00:19<00:26,  2.18s/it]
Task 13, Epoch 9/20 => Loss 0.297, Train_accy 90.94:  45%|████▌     | 9/20 [00:19<00:23,  2.15s/it]
Task 13, Epoch 10/20 => Loss 0.310, Train_accy 89.97:  45%|████▌     | 9/20 [00:21<00:23,  2.15s/it]
Task 13, Epoch 10/20 => Loss 0.310, Train_accy 89.97:  50%|█████     | 10/20 [00:21<00:21,  2.14s/it]
Task 13, Epoch 11/20 => Loss 0.228, Train_accy 94.82:  50%|█████     | 10/20 [00:23<00:21,  2.14s/it]
Task 13, Epoch 11/20 => Loss 0.228, Train_accy 94.82:  55%|█████▌    | 11/20 [00:23<00:19,  2.16s/it]
Task 13, Epoch 12/20 => Loss 0.267, Train_accy 94.17:  55%|█████▌    | 11/20 [00:25<00:19,  2.16s/it]
Task 13, Epoch 12/20 => Loss 0.267, Train_accy 94.17:  60%|██████    | 12/20 [00:25<00:17,  2.15s/it]
Task 13, Epoch 13/20 => Loss 0.204, Train_accy 95.79:  60%|██████    | 12/20 [00:27<00:17,  2.15s/it]
Task 13, Epoch 13/20 => Loss 0.204, Train_accy 95.79:  65%|██████▌   | 13/20 [00:27<00:15,  2.15s/it]
Task 13, Epoch 14/20 => Loss 0.205, Train_accy 95.47:  65%|██████▌   | 13/20 [00:30<00:15,  2.15s/it]
Task 13, Epoch 14/20 => Loss 0.205, Train_accy 95.47:  70%|███████   | 14/20 [00:30<00:12,  2.14s/it]
Task 13, Epoch 15/20 => Loss 0.262, Train_accy 93.53:  70%|███████   | 14/20 [00:32<00:12,  2.14s/it]
Task 13, Epoch 15/20 => Loss 0.262, Train_accy 93.53:  75%|███████▌  | 15/20 [00:32<00:10,  2.14s/it]
Task 13, Epoch 16/20 => Loss 0.268, Train_accy 93.85:  75%|███████▌  | 15/20 [00:34<00:10,  2.14s/it]
Task 13, Epoch 16/20 => Loss 0.268, Train_accy 93.85:  80%|████████  | 16/20 [00:34<00:08,  2.14s/it]
Task 13, Epoch 17/20 => Loss 0.211, Train_accy 93.20:  80%|████████  | 16/20 [00:36<00:08,  2.14s/it]
Task 13, Epoch 17/20 => Loss 0.211, Train_accy 93.20:  85%|████████▌ | 17/20 [00:36<00:06,  2.16s/it]
Task 13, Epoch 18/20 => Loss 0.155, Train_accy 96.44:  85%|████████▌ | 17/20 [00:38<00:06,  2.16s/it]
Task 13, Epoch 18/20 => Loss 0.155, Train_accy 96.44:  90%|█████████ | 18/20 [00:38<00:04,  2.16s/it]
Task 13, Epoch 19/20 => Loss 0.159, Train_accy 95.47:  90%|█████████ | 18/20 [00:40<00:04,  2.16s/it]
Task 13, Epoch 19/20 => Loss 0.159, Train_accy 95.47:  95%|█████████▌| 19/20 [00:40<00:02,  2.16s/it]
Task 13, Epoch 20/20 => Loss 0.190, Train_accy 93.20:  95%|█████████▌| 19/20 [00:42<00:02,  2.16s/it]
Task 13, Epoch 20/20 => Loss 0.190, Train_accy 93.20: 100%|██████████| 20/20 [00:42<00:00,  2.16s/it]
Task 13, Epoch 20/20 => Loss 0.190, Train_accy 93.20: 100%|██████████| 20/20 [00:42<00:00,  2.15s/it]
2024-08-12 00:23:18,965 [inflora.py] => Task 13, Epoch 20/20 => Loss 0.190, Train_accy 93.20
Threshold:  0.9824999999999999
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 34/768 type remove
Layer 4 : 39/768 type remove
Layer 5 : 62/768 type remove
Layer 6 : 61/768 type remove
Layer 7 : 62/768 type remove
Layer 8 : 94/768 type remove
Layer 9 : 153/768 type remove
Layer 10 : 202/768 type remove
Layer 11 : 82/768 type remove
Layer 12 : 109/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:23:28,131 [trainer.py] => Time:56.902121782302856
1121 1121
1121 1121
2024-08-12 00:23:31,167 [trainer.py] => Time:3.0350568294525146
2024-08-12 00:23:31,167 [inflora.py] => Exemplar size: 0
2024-08-12 00:23:31,167 [trainer.py] => CNN: {'total': 48.35, '00-09': 45.45, '10-19': 45.92, '20-29': 54.55, '30-39': 55.81, '40-49': 53.12, '50-59': 44.23, '60-69': 50.0, '70-79': 48.96, '80-89': 49.38, '90-99': 43.55, '100-109': 45.0, '110-119': 50.0, '120-129': 28.33, '130-139': 52.33, 'old': 48.02, 'new': 52.33}
2024-08-12 00:23:31,167 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23, 58.89, 58.25, 57.35, 56.64, 55.87, 54.3, 51.25, 50.28, 49.95, 48.79, 48.35]
2024-08-12 00:23:31,167 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67, 83.89, 83.49, 83.19, 84.72, 84.67, 84.47, 84.19, 84.13, 84.92, 83.96, 84.12]
2024-08-12 00:23:31,167 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029, 0.6444444444444445, 0.6320754716981132, 0.6197478991596639, 0.6029900332225914, 0.5859598853868195, 0.5661103979460848, 0.539833531510107, 0.5294117647058824, 0.5230769230769231, 0.5053140096618357, 0.49866190900981266]
2024-08-12 00:23:31,700 [trainer.py] => All params: 110611171
2024-08-12 00:23:31,703 [trainer.py] => Trainable params: 81418
2024-08-12 00:23:31,703 [inflora.py] => Learning on 140-150
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.14.weight', 'image_encoder.blocks.10.attn.lora_B_k.14.weight', 'image_encoder.blocks.6.attn.lora_B_v.14.weight', 'image_encoder.blocks.11.attn.lora_B_k.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.14.weight', 'image_encoder.blocks.9.attn.lora_B_k.14.weight', 'image_encoder.blocks.3.attn.lora_B_k.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.14.weight', 'classifier_pool.14.bias', 'image_encoder.blocks.2.attn.lora_B_v.14.weight', 'image_encoder.blocks.11.attn.lora_B_v.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.14.weight', 'image_encoder.blocks.5.attn.lora_B_v.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.14.weight', 'image_encoder.blocks.1.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_k.14.weight', 'image_encoder.blocks.7.attn.lora_B_k.14.weight', 'image_encoder.blocks.7.attn.lora_B_v.14.weight', 'image_encoder.blocks.2.attn.lora_B_k.14.weight', 'image_encoder.blocks.0.attn.lora_B_v.14.weight', 'image_encoder.blocks.9.attn.lora_B_v.14.weight', 'image_encoder.blocks.4.attn.lora_B_v.14.weight', 'classifier_pool.14.weight', 'image_encoder.blocks.10.attn.lora_B_v.14.weight', 'image_encoder.blocks.6.attn.lora_B_k.14.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 14, Epoch 1/20 => Loss 2.361, Train_accy 28.05:   0%|          | 0/20 [00:02<?, ?it/s]
Task 14, Epoch 1/20 => Loss 2.361, Train_accy 28.05:   5%|▌         | 1/20 [00:02<00:41,  2.19s/it]
Task 14, Epoch 2/20 => Loss 1.377, Train_accy 57.01:   5%|▌         | 1/20 [00:04<00:41,  2.19s/it]
Task 14, Epoch 2/20 => Loss 1.377, Train_accy 57.01:  10%|█         | 2/20 [00:04<00:39,  2.19s/it]
Task 14, Epoch 3/20 => Loss 0.997, Train_accy 67.07:  10%|█         | 2/20 [00:06<00:39,  2.19s/it]
Task 14, Epoch 3/20 => Loss 0.997, Train_accy 67.07:  15%|█▌        | 3/20 [00:06<00:37,  2.19s/it]
Task 14, Epoch 4/20 => Loss 0.804, Train_accy 72.87:  15%|█▌        | 3/20 [00:08<00:37,  2.19s/it]
Task 14, Epoch 4/20 => Loss 0.804, Train_accy 72.87:  20%|██        | 4/20 [00:08<00:34,  2.18s/it]
Task 14, Epoch 5/20 => Loss 0.568, Train_accy 81.71:  20%|██        | 4/20 [00:10<00:34,  2.18s/it]
Task 14, Epoch 5/20 => Loss 0.568, Train_accy 81.71:  25%|██▌       | 5/20 [00:10<00:32,  2.20s/it]
Task 14, Epoch 6/20 => Loss 0.496, Train_accy 84.45:  25%|██▌       | 5/20 [00:13<00:32,  2.20s/it]
Task 14, Epoch 6/20 => Loss 0.496, Train_accy 84.45:  30%|███       | 6/20 [00:13<00:30,  2.20s/it]
Task 14, Epoch 7/20 => Loss 0.409, Train_accy 88.72:  30%|███       | 6/20 [00:15<00:30,  2.20s/it]
Task 14, Epoch 7/20 => Loss 0.409, Train_accy 88.72:  35%|███▌      | 7/20 [00:15<00:28,  2.20s/it]
Task 14, Epoch 8/20 => Loss 0.423, Train_accy 86.89:  35%|███▌      | 7/20 [00:17<00:28,  2.20s/it]
Task 14, Epoch 8/20 => Loss 0.423, Train_accy 86.89:  40%|████      | 8/20 [00:17<00:26,  2.19s/it]
Task 14, Epoch 9/20 => Loss 0.334, Train_accy 90.24:  40%|████      | 8/20 [00:19<00:26,  2.19s/it]
Task 14, Epoch 9/20 => Loss 0.334, Train_accy 90.24:  45%|████▌     | 9/20 [00:19<00:24,  2.19s/it]
Task 14, Epoch 10/20 => Loss 0.259, Train_accy 93.60:  45%|████▌     | 9/20 [00:21<00:24,  2.19s/it]
Task 14, Epoch 10/20 => Loss 0.259, Train_accy 93.60:  50%|█████     | 10/20 [00:21<00:21,  2.18s/it]
Task 14, Epoch 11/20 => Loss 0.325, Train_accy 89.33:  50%|█████     | 10/20 [00:24<00:21,  2.18s/it]
Task 14, Epoch 11/20 => Loss 0.325, Train_accy 89.33:  55%|█████▌    | 11/20 [00:24<00:19,  2.18s/it]
Task 14, Epoch 12/20 => Loss 0.309, Train_accy 90.24:  55%|█████▌    | 11/20 [00:26<00:19,  2.18s/it]
Task 14, Epoch 12/20 => Loss 0.309, Train_accy 90.24:  60%|██████    | 12/20 [00:26<00:17,  2.18s/it]
Task 14, Epoch 13/20 => Loss 0.270, Train_accy 92.68:  60%|██████    | 12/20 [00:28<00:17,  2.18s/it]
Task 14, Epoch 13/20 => Loss 0.270, Train_accy 92.68:  65%|██████▌   | 13/20 [00:28<00:15,  2.18s/it]
Task 14, Epoch 14/20 => Loss 0.240, Train_accy 94.82:  65%|██████▌   | 13/20 [00:30<00:15,  2.18s/it]
Task 14, Epoch 14/20 => Loss 0.240, Train_accy 94.82:  70%|███████   | 14/20 [00:30<00:13,  2.18s/it]
Task 14, Epoch 15/20 => Loss 0.235, Train_accy 93.60:  70%|███████   | 14/20 [00:32<00:13,  2.18s/it]
Task 14, Epoch 15/20 => Loss 0.235, Train_accy 93.60:  75%|███████▌  | 15/20 [00:32<00:10,  2.17s/it]
Task 14, Epoch 16/20 => Loss 0.214, Train_accy 94.51:  75%|███████▌  | 15/20 [00:34<00:10,  2.17s/it]
Task 14, Epoch 16/20 => Loss 0.214, Train_accy 94.51:  80%|████████  | 16/20 [00:34<00:08,  2.19s/it]
Task 14, Epoch 17/20 => Loss 0.192, Train_accy 94.21:  80%|████████  | 16/20 [00:37<00:08,  2.19s/it]
Task 14, Epoch 17/20 => Loss 0.192, Train_accy 94.21:  85%|████████▌ | 17/20 [00:37<00:06,  2.19s/it]
Task 14, Epoch 18/20 => Loss 0.214, Train_accy 94.21:  85%|████████▌ | 17/20 [00:39<00:06,  2.19s/it]
Task 14, Epoch 18/20 => Loss 0.214, Train_accy 94.21:  90%|█████████ | 18/20 [00:39<00:04,  2.21s/it]
Task 14, Epoch 19/20 => Loss 0.208, Train_accy 92.68:  90%|█████████ | 18/20 [00:41<00:04,  2.21s/it]
Task 14, Epoch 19/20 => Loss 0.208, Train_accy 92.68:  95%|█████████▌| 19/20 [00:41<00:02,  2.20s/it]
Task 14, Epoch 20/20 => Loss 0.245, Train_accy 92.99:  95%|█████████▌| 19/20 [00:43<00:02,  2.20s/it]
Task 14, Epoch 20/20 => Loss 0.245, Train_accy 92.99: 100%|██████████| 20/20 [00:43<00:00,  2.19s/it]
Task 14, Epoch 20/20 => Loss 0.245, Train_accy 92.99: 100%|██████████| 20/20 [00:43<00:00,  2.19s/it]
2024-08-12 00:24:19,804 [inflora.py] => Task 14, Epoch 20/20 => Loss 0.245, Train_accy 92.99
Threshold:  0.985
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 18/768 type remove
Layer 3 : 36/768 type remove
Layer 4 : 43/768 type remove
Layer 5 : 68/768 type remove
Layer 6 : 68/768 type remove
Layer 7 : 72/768 type remove
Layer 8 : 105/768 type remove
Layer 9 : 163/768 type remove
Layer 10 : 213/768 type remove
Layer 11 : 90/768 type remove
Layer 12 : 123/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:24:28,700 [trainer.py] => Time:56.99667429924011
1197 1197
1197 1197
2024-08-12 00:24:31,936 [trainer.py] => Time:3.236382484436035
2024-08-12 00:24:31,936 [inflora.py] => Exemplar size: 0
2024-08-12 00:24:31,937 [trainer.py] => CNN: {'total': 47.87, '00-09': 46.75, '10-19': 44.9, '20-29': 53.54, '30-39': 52.33, '40-49': 51.56, '50-59': 40.38, '60-69': 49.21, '70-79': 46.88, '80-89': 49.38, '90-99': 43.55, '100-109': 46.67, '110-119': 51.35, '120-129': 30.0, '130-139': 53.49, '140-149': 48.68, 'old': 47.81, 'new': 48.68}
2024-08-12 00:24:31,937 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23, 58.89, 58.25, 57.35, 56.64, 55.87, 54.3, 51.25, 50.28, 49.95, 48.79, 48.35, 47.87]
2024-08-12 00:24:31,937 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67, 83.89, 83.49, 83.19, 84.72, 84.67, 84.47, 84.19, 84.13, 84.92, 83.96, 84.12, 83.38]
2024-08-12 00:24:31,937 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029, 0.6444444444444445, 0.6320754716981132, 0.6197478991596639, 0.6029900332225914, 0.5859598853868195, 0.5661103979460848, 0.539833531510107, 0.5294117647058824, 0.5230769230769231, 0.5053140096618357, 0.49866190900981266, 0.49122807017543857]
2024-08-12 00:24:32,445 [trainer.py] => All params: 110611171
2024-08-12 00:24:32,448 [trainer.py] => Trainable params: 81418
2024-08-12 00:24:32,448 [inflora.py] => Learning on 150-160
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_v.15.weight', 'image_encoder.blocks.10.attn.lora_B_k.15.weight', 'image_encoder.blocks.3.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_k.15.weight', 'image_encoder.blocks.1.attn.lora_B_k.15.weight', 'image_encoder.blocks.1.attn.lora_B_v.15.weight', 'image_encoder.blocks.2.attn.lora_B_v.15.weight', 'image_encoder.blocks.3.attn.lora_B_v.15.weight', 'image_encoder.blocks.6.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_v.15.weight', 'image_encoder.blocks.4.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_v.15.weight', 'classifier_pool.15.bias', 'classifier_pool.15.weight', 'image_encoder.blocks.2.attn.lora_B_k.15.weight', 'image_encoder.blocks.5.attn.lora_B_k.15.weight', 'image_encoder.blocks.7.attn.lora_B_v.15.weight', 'image_encoder.blocks.0.attn.lora_B_k.15.weight', 'image_encoder.blocks.11.attn.lora_B_v.15.weight', 'image_encoder.blocks.6.attn.lora_B_v.15.weight', 'image_encoder.blocks.7.attn.lora_B_k.15.weight', 'image_encoder.blocks.4.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.15.weight', 'image_encoder.blocks.11.attn.lora_B_k.15.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 15, Epoch 1/20 => Loss 2.772, Train_accy 10.84:   0%|          | 0/20 [00:01<?, ?it/s]
Task 15, Epoch 1/20 => Loss 2.772, Train_accy 10.84:   5%|▌         | 1/20 [00:01<00:33,  1.74s/it]
Task 15, Epoch 2/20 => Loss 1.683, Train_accy 45.81:   5%|▌         | 1/20 [00:03<00:33,  1.74s/it]
Task 15, Epoch 2/20 => Loss 1.683, Train_accy 45.81:  10%|█         | 2/20 [00:03<00:31,  1.75s/it]
Task 15, Epoch 3/20 => Loss 1.231, Train_accy 61.58:  10%|█         | 2/20 [00:05<00:31,  1.75s/it]
Task 15, Epoch 3/20 => Loss 1.231, Train_accy 61.58:  15%|█▌        | 3/20 [00:05<00:30,  1.77s/it]
Task 15, Epoch 4/20 => Loss 0.971, Train_accy 68.47:  15%|█▌        | 3/20 [00:07<00:30,  1.77s/it]
Task 15, Epoch 4/20 => Loss 0.971, Train_accy 68.47:  20%|██        | 4/20 [00:07<00:28,  1.76s/it]
Task 15, Epoch 5/20 => Loss 0.787, Train_accy 76.35:  20%|██        | 4/20 [00:08<00:28,  1.76s/it]
Task 15, Epoch 5/20 => Loss 0.787, Train_accy 76.35:  25%|██▌       | 5/20 [00:08<00:26,  1.77s/it]
Task 15, Epoch 6/20 => Loss 0.572, Train_accy 84.24:  25%|██▌       | 5/20 [00:10<00:26,  1.77s/it]
Task 15, Epoch 6/20 => Loss 0.572, Train_accy 84.24:  30%|███       | 6/20 [00:10<00:24,  1.78s/it]
Task 15, Epoch 7/20 => Loss 0.503, Train_accy 86.21:  30%|███       | 6/20 [00:12<00:24,  1.78s/it]
Task 15, Epoch 7/20 => Loss 0.503, Train_accy 86.21:  35%|███▌      | 7/20 [00:12<00:22,  1.77s/it]
Task 15, Epoch 8/20 => Loss 0.344, Train_accy 90.64:  35%|███▌      | 7/20 [00:14<00:22,  1.77s/it]
Task 15, Epoch 8/20 => Loss 0.344, Train_accy 90.64:  40%|████      | 8/20 [00:14<00:21,  1.77s/it]
Task 15, Epoch 9/20 => Loss 0.348, Train_accy 92.61:  40%|████      | 8/20 [00:15<00:21,  1.77s/it]
Task 15, Epoch 9/20 => Loss 0.348, Train_accy 92.61:  45%|████▌     | 9/20 [00:15<00:19,  1.77s/it]
Task 15, Epoch 10/20 => Loss 0.272, Train_accy 95.57:  45%|████▌     | 9/20 [00:17<00:19,  1.77s/it]
Task 15, Epoch 10/20 => Loss 0.272, Train_accy 95.57:  50%|█████     | 10/20 [00:17<00:17,  1.77s/it]
Task 15, Epoch 11/20 => Loss 0.283, Train_accy 93.10:  50%|█████     | 10/20 [00:19<00:17,  1.77s/it]
Task 15, Epoch 11/20 => Loss 0.283, Train_accy 93.10:  55%|█████▌    | 11/20 [00:19<00:15,  1.78s/it]
Task 15, Epoch 12/20 => Loss 0.196, Train_accy 96.06:  55%|█████▌    | 11/20 [00:21<00:15,  1.78s/it]
Task 15, Epoch 12/20 => Loss 0.196, Train_accy 96.06:  60%|██████    | 12/20 [00:21<00:14,  1.79s/it]
Task 15, Epoch 13/20 => Loss 0.222, Train_accy 94.58:  60%|██████    | 12/20 [00:23<00:14,  1.79s/it]
Task 15, Epoch 13/20 => Loss 0.222, Train_accy 94.58:  65%|██████▌   | 13/20 [00:23<00:12,  1.78s/it]
Task 15, Epoch 14/20 => Loss 0.203, Train_accy 94.58:  65%|██████▌   | 13/20 [00:24<00:12,  1.78s/it]
Task 15, Epoch 14/20 => Loss 0.203, Train_accy 94.58:  70%|███████   | 14/20 [00:24<00:10,  1.78s/it]
Task 15, Epoch 15/20 => Loss 0.229, Train_accy 94.09:  70%|███████   | 14/20 [00:26<00:10,  1.78s/it]
Task 15, Epoch 15/20 => Loss 0.229, Train_accy 94.09:  75%|███████▌  | 15/20 [00:26<00:08,  1.78s/it]
Task 15, Epoch 16/20 => Loss 0.250, Train_accy 92.61:  75%|███████▌  | 15/20 [00:28<00:08,  1.78s/it]
Task 15, Epoch 16/20 => Loss 0.250, Train_accy 92.61:  80%|████████  | 16/20 [00:28<00:07,  1.78s/it]
Task 15, Epoch 17/20 => Loss 0.183, Train_accy 94.58:  80%|████████  | 16/20 [00:30<00:07,  1.78s/it]
Task 15, Epoch 17/20 => Loss 0.183, Train_accy 94.58:  85%|████████▌ | 17/20 [00:30<00:05,  1.77s/it]
Task 15, Epoch 18/20 => Loss 0.177, Train_accy 96.06:  85%|████████▌ | 17/20 [00:31<00:05,  1.77s/it]
Task 15, Epoch 18/20 => Loss 0.177, Train_accy 96.06:  90%|█████████ | 18/20 [00:31<00:03,  1.78s/it]
Task 15, Epoch 19/20 => Loss 0.138, Train_accy 97.04:  90%|█████████ | 18/20 [00:33<00:03,  1.78s/it]
Task 15, Epoch 19/20 => Loss 0.138, Train_accy 97.04:  95%|█████████▌| 19/20 [00:33<00:01,  1.77s/it]
Task 15, Epoch 20/20 => Loss 0.172, Train_accy 95.07:  95%|█████████▌| 19/20 [00:35<00:01,  1.77s/it]
Task 15, Epoch 20/20 => Loss 0.172, Train_accy 95.07: 100%|██████████| 20/20 [00:35<00:00,  1.76s/it]
Task 15, Epoch 20/20 => Loss 0.172, Train_accy 95.07: 100%|██████████| 20/20 [00:35<00:00,  1.77s/it]
2024-08-12 00:25:11,878 [inflora.py] => Task 15, Epoch 20/20 => Loss 0.172, Train_accy 95.07
Threshold:  0.9875
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 22/768 type remove
Layer 3 : 44/768 type remove
Layer 4 : 54/768 type remove
Layer 5 : 85/768 type remove
Layer 6 : 84/768 type remove
Layer 7 : 89/768 type remove
Layer 8 : 132/768 type remove
Layer 9 : 211/768 type remove
Layer 10 : 274/768 type remove
Layer 11 : 143/768 type remove
Layer 12 : 178/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:25:20,062 [trainer.py] => Time:47.613232374191284
1248 1248
1248 1248
2024-08-12 00:25:23,434 [trainer.py] => Time:3.3720757961273193
2024-08-12 00:25:23,434 [inflora.py] => Exemplar size: 0
2024-08-12 00:25:23,434 [trainer.py] => CNN: {'total': 46.55, '00-09': 45.45, '10-19': 44.9, '20-29': 51.52, '30-39': 51.16, '40-49': 54.69, '50-59': 42.31, '60-69': 47.62, '70-79': 48.96, '80-89': 49.38, '90-99': 41.94, '100-109': 46.67, '110-119': 48.65, '120-129': 30.0, '130-139': 52.33, '140-149': 48.68, '150-159': 25.49, 'old': 47.45, 'new': 25.49}
2024-08-12 00:25:23,434 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23, 58.89, 58.25, 57.35, 56.64, 55.87, 54.3, 51.25, 50.28, 49.95, 48.79, 48.35, 47.87, 46.55]
2024-08-12 00:25:23,434 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67, 83.89, 83.49, 83.19, 84.72, 84.67, 84.47, 84.19, 84.13, 84.92, 83.96, 84.12, 83.38, 83.41]
2024-08-12 00:25:23,434 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029, 0.6444444444444445, 0.6320754716981132, 0.6197478991596639, 0.6029900332225914, 0.5859598853868195, 0.5661103979460848, 0.539833531510107, 0.5294117647058824, 0.5230769230769231, 0.5053140096618357, 0.49866190900981266, 0.49122807017543857, 0.47836538461538464]
2024-08-12 00:25:23,984 [trainer.py] => All params: 110611171
2024-08-12 00:25:23,987 [trainer.py] => Trainable params: 81418
2024-08-12 00:25:23,987 [inflora.py] => Learning on 160-170
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.16.weight', 'image_encoder.blocks.1.attn.lora_B_k.16.weight', 'image_encoder.blocks.6.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_v.16.weight', 'image_encoder.blocks.6.attn.lora_B_k.16.weight', 'image_encoder.blocks.7.attn.lora_B_v.16.weight', 'image_encoder.blocks.3.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_v.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.16.weight', 'image_encoder.blocks.10.attn.lora_B_k.16.weight', 'image_encoder.blocks.3.attn.lora_B_v.16.weight', 'image_encoder.blocks.2.attn.lora_B_k.16.weight', 'image_encoder.blocks.5.attn.lora_B_k.16.weight', 'image_encoder.blocks.8.attn.lora_B_v.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.16.weight', 'image_encoder.blocks.4.attn.lora_B_v.16.weight', 'image_encoder.blocks.5.attn.lora_B_v.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.16.weight', 'classifier_pool.16.bias', 'image_encoder.blocks.0.attn.lora_B_v.16.weight', 'image_encoder.blocks.7.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.16.weight', 'classifier_pool.16.weight', 'image_encoder.blocks.0.attn.lora_B_k.16.weight', 'image_encoder.blocks.10.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_k.16.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 16, Epoch 1/20 => Loss 2.920, Train_accy 12.28:   0%|          | 0/20 [00:02<?, ?it/s]
Task 16, Epoch 1/20 => Loss 2.920, Train_accy 12.28:   5%|▌         | 1/20 [00:02<00:42,  2.25s/it]
Task 16, Epoch 2/20 => Loss 1.836, Train_accy 37.43:   5%|▌         | 1/20 [00:04<00:42,  2.25s/it]
Task 16, Epoch 2/20 => Loss 1.836, Train_accy 37.43:  10%|█         | 2/20 [00:04<00:40,  2.25s/it]
Task 16, Epoch 3/20 => Loss 1.357, Train_accy 55.09:  10%|█         | 2/20 [00:06<00:40,  2.25s/it]
Task 16, Epoch 3/20 => Loss 1.357, Train_accy 55.09:  15%|█▌        | 3/20 [00:06<00:38,  2.27s/it]
Task 16, Epoch 4/20 => Loss 1.015, Train_accy 67.37:  15%|█▌        | 3/20 [00:09<00:38,  2.27s/it]
Task 16, Epoch 4/20 => Loss 1.015, Train_accy 67.37:  20%|██        | 4/20 [00:09<00:36,  2.29s/it]
Task 16, Epoch 5/20 => Loss 0.843, Train_accy 73.65:  20%|██        | 4/20 [00:11<00:36,  2.29s/it]
Task 16, Epoch 5/20 => Loss 0.843, Train_accy 73.65:  25%|██▌       | 5/20 [00:11<00:34,  2.29s/it]
Task 16, Epoch 6/20 => Loss 0.636, Train_accy 79.94:  25%|██▌       | 5/20 [00:13<00:34,  2.29s/it]
Task 16, Epoch 6/20 => Loss 0.636, Train_accy 79.94:  30%|███       | 6/20 [00:13<00:31,  2.28s/it]
Task 16, Epoch 7/20 => Loss 0.565, Train_accy 81.74:  30%|███       | 6/20 [00:15<00:31,  2.28s/it]
Task 16, Epoch 7/20 => Loss 0.565, Train_accy 81.74:  35%|███▌      | 7/20 [00:15<00:29,  2.27s/it]
Task 16, Epoch 8/20 => Loss 0.472, Train_accy 85.03:  35%|███▌      | 7/20 [00:18<00:29,  2.27s/it]
Task 16, Epoch 8/20 => Loss 0.472, Train_accy 85.03:  40%|████      | 8/20 [00:18<00:27,  2.28s/it]
Task 16, Epoch 9/20 => Loss 0.437, Train_accy 86.83:  40%|████      | 8/20 [00:20<00:27,  2.28s/it]
Task 16, Epoch 9/20 => Loss 0.437, Train_accy 86.83:  45%|████▌     | 9/20 [00:20<00:24,  2.26s/it]
Task 16, Epoch 10/20 => Loss 0.434, Train_accy 86.23:  45%|████▌     | 9/20 [00:22<00:24,  2.26s/it]
Task 16, Epoch 10/20 => Loss 0.434, Train_accy 86.23:  50%|█████     | 10/20 [00:22<00:22,  2.26s/it]
Task 16, Epoch 11/20 => Loss 0.362, Train_accy 89.22:  50%|█████     | 10/20 [00:24<00:22,  2.26s/it]
Task 16, Epoch 11/20 => Loss 0.362, Train_accy 89.22:  55%|█████▌    | 11/20 [00:24<00:20,  2.25s/it]
Task 16, Epoch 12/20 => Loss 0.341, Train_accy 89.52:  55%|█████▌    | 11/20 [00:27<00:20,  2.25s/it]
Task 16, Epoch 12/20 => Loss 0.341, Train_accy 89.52:  60%|██████    | 12/20 [00:27<00:18,  2.26s/it]
Task 16, Epoch 13/20 => Loss 0.336, Train_accy 89.82:  60%|██████    | 12/20 [00:29<00:18,  2.26s/it]
Task 16, Epoch 13/20 => Loss 0.336, Train_accy 89.82:  65%|██████▌   | 13/20 [00:29<00:15,  2.25s/it]
Task 16, Epoch 14/20 => Loss 0.289, Train_accy 91.62:  65%|██████▌   | 13/20 [00:31<00:15,  2.25s/it]
Task 16, Epoch 14/20 => Loss 0.289, Train_accy 91.62:  70%|███████   | 14/20 [00:31<00:13,  2.25s/it]
Task 16, Epoch 15/20 => Loss 0.300, Train_accy 90.42:  70%|███████   | 14/20 [00:33<00:13,  2.25s/it]
Task 16, Epoch 15/20 => Loss 0.300, Train_accy 90.42:  75%|███████▌  | 15/20 [00:33<00:11,  2.25s/it]
Task 16, Epoch 16/20 => Loss 0.298, Train_accy 91.62:  75%|███████▌  | 15/20 [00:36<00:11,  2.25s/it]
Task 16, Epoch 16/20 => Loss 0.298, Train_accy 91.62:  80%|████████  | 16/20 [00:36<00:09,  2.25s/it]
Task 16, Epoch 17/20 => Loss 0.263, Train_accy 92.22:  80%|████████  | 16/20 [00:38<00:09,  2.25s/it]
Task 16, Epoch 17/20 => Loss 0.263, Train_accy 92.22:  85%|████████▌ | 17/20 [00:38<00:06,  2.24s/it]
Task 16, Epoch 18/20 => Loss 0.257, Train_accy 94.31:  85%|████████▌ | 17/20 [00:40<00:06,  2.24s/it]
Task 16, Epoch 18/20 => Loss 0.257, Train_accy 94.31:  90%|█████████ | 18/20 [00:40<00:04,  2.25s/it]
Task 16, Epoch 19/20 => Loss 0.240, Train_accy 92.81:  90%|█████████ | 18/20 [00:42<00:04,  2.25s/it]
Task 16, Epoch 19/20 => Loss 0.240, Train_accy 92.81:  95%|█████████▌| 19/20 [00:42<00:02,  2.27s/it]
Task 16, Epoch 20/20 => Loss 0.256, Train_accy 91.92:  95%|█████████▌| 19/20 [00:45<00:02,  2.27s/it]
Task 16, Epoch 20/20 => Loss 0.256, Train_accy 91.92: 100%|██████████| 20/20 [00:45<00:00,  2.27s/it]
Task 16, Epoch 20/20 => Loss 0.256, Train_accy 91.92: 100%|██████████| 20/20 [00:45<00:00,  2.26s/it]
2024-08-12 00:26:13,491 [inflora.py] => Task 16, Epoch 20/20 => Loss 0.256, Train_accy 91.92
Threshold:  0.99
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 27/768 type remove
Layer 3 : 52/768 type remove
Layer 4 : 65/768 type remove
Layer 5 : 101/768 type remove
Layer 6 : 100/768 type remove
Layer 7 : 107/768 type remove
Layer 8 : 153/768 type remove
Layer 9 : 242/768 type remove
Layer 10 : 327/768 type remove
Layer 11 : 193/768 type remove
Layer 12 : 243/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:26:22,390 [trainer.py] => Time:58.403254985809326
1333 1333
1333 1333
2024-08-12 00:26:25,853 [trainer.py] => Time:3.4626452922821045
2024-08-12 00:26:25,853 [inflora.py] => Exemplar size: 0
2024-08-12 00:26:25,854 [trainer.py] => CNN: {'total': 44.79, '00-09': 44.16, '10-19': 43.88, '20-29': 49.49, '30-39': 50.0, '40-49': 50.0, '50-59': 44.23, '60-69': 46.83, '70-79': 46.88, '80-89': 49.38, '90-99': 41.94, '100-109': 48.33, '110-119': 44.59, '120-129': 31.67, '130-139': 51.16, '140-149': 50.0, '150-159': 27.45, '160-169': 30.59, 'old': 45.75, 'new': 30.59}
2024-08-12 00:26:25,854 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23, 58.89, 58.25, 57.35, 56.64, 55.87, 54.3, 51.25, 50.28, 49.95, 48.79, 48.35, 47.87, 46.55, 44.79]
2024-08-12 00:26:25,854 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67, 83.89, 83.49, 83.19, 84.72, 84.67, 84.47, 84.19, 84.13, 84.92, 83.96, 84.12, 83.38, 83.41, 83.5]
2024-08-12 00:26:25,854 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029, 0.6444444444444445, 0.6320754716981132, 0.6197478991596639, 0.6029900332225914, 0.5859598853868195, 0.5661103979460848, 0.539833531510107, 0.5294117647058824, 0.5230769230769231, 0.5053140096618357, 0.49866190900981266, 0.49122807017543857, 0.47836538461538464, 0.45911477869467365]
2024-08-12 00:26:26,343 [trainer.py] => All params: 110611171
2024-08-12 00:26:26,346 [trainer.py] => Trainable params: 81418
2024-08-12 00:26:26,346 [inflora.py] => Learning on 170-180
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.17.weight', 'image_encoder.blocks.5.attn.lora_B_k.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.17.weight', 'image_encoder.blocks.2.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.17.weight', 'image_encoder.blocks.6.attn.lora_B_k.17.weight', 'image_encoder.blocks.4.attn.lora_B_v.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.17.weight', 'image_encoder.blocks.0.attn.lora_B_v.17.weight', 'image_encoder.blocks.7.attn.lora_B_v.17.weight', 'image_encoder.blocks.7.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_v.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.17.weight', 'image_encoder.blocks.1.attn.lora_B_v.17.weight', 'image_encoder.blocks.0.attn.lora_B_k.17.weight', 'image_encoder.blocks.9.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_k.17.weight', 'classifier_pool.17.bias', 'image_encoder.blocks.8.attn.lora_B_k.17.weight', 'image_encoder.blocks.1.attn.lora_B_k.17.weight', 'image_encoder.blocks.8.attn.lora_B_v.17.weight', 'image_encoder.blocks.2.attn.lora_B_k.17.weight', 'classifier_pool.17.weight', 'image_encoder.blocks.6.attn.lora_B_v.17.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 17, Epoch 1/20 => Loss 2.514, Train_accy 19.83:   0%|          | 0/20 [00:01<?, ?it/s]
Task 17, Epoch 1/20 => Loss 2.514, Train_accy 19.83:   5%|▌         | 1/20 [00:01<00:35,  1.89s/it]
Task 17, Epoch 2/20 => Loss 1.605, Train_accy 48.10:   5%|▌         | 1/20 [00:03<00:35,  1.89s/it]
Task 17, Epoch 2/20 => Loss 1.605, Train_accy 48.10:  10%|█         | 2/20 [00:03<00:34,  1.90s/it]
Task 17, Epoch 3/20 => Loss 1.289, Train_accy 58.65:  10%|█         | 2/20 [00:05<00:34,  1.90s/it]
Task 17, Epoch 3/20 => Loss 1.289, Train_accy 58.65:  15%|█▌        | 3/20 [00:05<00:32,  1.92s/it]
Task 17, Epoch 4/20 => Loss 0.998, Train_accy 68.35:  15%|█▌        | 3/20 [00:07<00:32,  1.92s/it]
Task 17, Epoch 4/20 => Loss 0.998, Train_accy 68.35:  20%|██        | 4/20 [00:07<00:30,  1.91s/it]
Task 17, Epoch 5/20 => Loss 0.723, Train_accy 78.06:  20%|██        | 4/20 [00:09<00:30,  1.91s/it]
Task 17, Epoch 5/20 => Loss 0.723, Train_accy 78.06:  25%|██▌       | 5/20 [00:09<00:28,  1.92s/it]
Task 17, Epoch 6/20 => Loss 0.573, Train_accy 81.86:  25%|██▌       | 5/20 [00:11<00:28,  1.92s/it]
Task 17, Epoch 6/20 => Loss 0.573, Train_accy 81.86:  30%|███       | 6/20 [00:11<00:26,  1.92s/it]
Task 17, Epoch 7/20 => Loss 0.480, Train_accy 86.08:  30%|███       | 6/20 [00:13<00:26,  1.92s/it]
Task 17, Epoch 7/20 => Loss 0.480, Train_accy 86.08:  35%|███▌      | 7/20 [00:13<00:25,  1.93s/it]
Task 17, Epoch 8/20 => Loss 0.384, Train_accy 91.14:  35%|███▌      | 7/20 [00:15<00:25,  1.93s/it]
Task 17, Epoch 8/20 => Loss 0.384, Train_accy 91.14:  40%|████      | 8/20 [00:15<00:23,  1.93s/it]
Task 17, Epoch 9/20 => Loss 0.338, Train_accy 89.87:  40%|████      | 8/20 [00:17<00:23,  1.93s/it]
Task 17, Epoch 9/20 => Loss 0.338, Train_accy 89.87:  45%|████▌     | 9/20 [00:17<00:21,  1.93s/it]
Task 17, Epoch 10/20 => Loss 0.333, Train_accy 93.67:  45%|████▌     | 9/20 [00:19<00:21,  1.93s/it]
Task 17, Epoch 10/20 => Loss 0.333, Train_accy 93.67:  50%|█████     | 10/20 [00:19<00:19,  1.93s/it]
Task 17, Epoch 11/20 => Loss 0.295, Train_accy 91.14:  50%|█████     | 10/20 [00:21<00:19,  1.93s/it]
Task 17, Epoch 11/20 => Loss 0.295, Train_accy 91.14:  55%|█████▌    | 11/20 [00:21<00:17,  1.93s/it]
Task 17, Epoch 12/20 => Loss 0.301, Train_accy 91.56:  55%|█████▌    | 11/20 [00:23<00:17,  1.93s/it]
Task 17, Epoch 12/20 => Loss 0.301, Train_accy 91.56:  60%|██████    | 12/20 [00:23<00:15,  1.93s/it]
Task 17, Epoch 13/20 => Loss 0.223, Train_accy 94.09:  60%|██████    | 12/20 [00:25<00:15,  1.93s/it]
Task 17, Epoch 13/20 => Loss 0.223, Train_accy 94.09:  65%|██████▌   | 13/20 [00:25<00:13,  1.93s/it]
Task 17, Epoch 14/20 => Loss 0.212, Train_accy 94.51:  65%|██████▌   | 13/20 [00:26<00:13,  1.93s/it]
Task 17, Epoch 14/20 => Loss 0.212, Train_accy 94.51:  70%|███████   | 14/20 [00:26<00:11,  1.93s/it]
Task 17, Epoch 15/20 => Loss 0.236, Train_accy 92.41:  70%|███████   | 14/20 [00:28<00:11,  1.93s/it]
Task 17, Epoch 15/20 => Loss 0.236, Train_accy 92.41:  75%|███████▌  | 15/20 [00:28<00:09,  1.93s/it]
Task 17, Epoch 16/20 => Loss 0.210, Train_accy 95.36:  75%|███████▌  | 15/20 [00:30<00:09,  1.93s/it]
Task 17, Epoch 16/20 => Loss 0.210, Train_accy 95.36:  80%|████████  | 16/20 [00:30<00:07,  1.92s/it]
Task 17, Epoch 17/20 => Loss 0.218, Train_accy 94.09:  80%|████████  | 16/20 [00:32<00:07,  1.92s/it]
Task 17, Epoch 17/20 => Loss 0.218, Train_accy 94.09:  85%|████████▌ | 17/20 [00:32<00:05,  1.93s/it]
Task 17, Epoch 18/20 => Loss 0.166, Train_accy 96.62:  85%|████████▌ | 17/20 [00:34<00:05,  1.93s/it]
Task 17, Epoch 18/20 => Loss 0.166, Train_accy 96.62:  90%|█████████ | 18/20 [00:34<00:03,  1.93s/it]
Task 17, Epoch 19/20 => Loss 0.220, Train_accy 93.25:  90%|█████████ | 18/20 [00:36<00:03,  1.93s/it]
Task 17, Epoch 19/20 => Loss 0.220, Train_accy 93.25:  95%|█████████▌| 19/20 [00:36<00:01,  1.94s/it]
Task 17, Epoch 20/20 => Loss 0.225, Train_accy 93.67:  95%|█████████▌| 19/20 [00:38<00:01,  1.94s/it]
Task 17, Epoch 20/20 => Loss 0.225, Train_accy 93.67: 100%|██████████| 20/20 [00:38<00:00,  1.93s/it]
Task 17, Epoch 20/20 => Loss 0.225, Train_accy 93.67: 100%|██████████| 20/20 [00:38<00:00,  1.93s/it]
2024-08-12 00:27:08,777 [inflora.py] => Task 17, Epoch 20/20 => Loss 0.225, Train_accy 93.67
Threshold:  0.9924999999999999
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 33/768 type remove
Layer 3 : 64/768 type remove
Layer 4 : 83/768 type remove
Layer 5 : 126/768 type remove
Layer 6 : 126/768 type remove
Layer 7 : 138/768 type remove
Layer 8 : 196/768 type remove
Layer 9 : 295/768 type remove
Layer 10 : 383/768 type remove
Layer 11 : 245/768 type remove
Layer 12 : 295/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:27:17,603 [trainer.py] => Time:51.256844997406006
1395 1395
1395 1395
2024-08-12 00:27:21,193 [trainer.py] => Time:3.59029483795166
2024-08-12 00:27:21,193 [inflora.py] => Exemplar size: 0
2024-08-12 00:27:21,193 [trainer.py] => CNN: {'total': 43.08, '00-09': 41.56, '10-19': 42.86, '20-29': 47.47, '30-39': 51.16, '40-49': 50.0, '50-59': 46.15, '60-69': 48.41, '70-79': 44.79, '80-89': 43.21, '90-99': 41.94, '100-109': 48.33, '110-119': 47.3, '120-129': 26.67, '130-139': 52.33, '140-149': 48.68, '150-159': 27.45, '160-169': 27.06, '170-179': 25.81, 'old': 43.89, 'new': 25.81}
2024-08-12 00:27:21,194 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23, 58.89, 58.25, 57.35, 56.64, 55.87, 54.3, 51.25, 50.28, 49.95, 48.79, 48.35, 47.87, 46.55, 44.79, 43.08]
2024-08-12 00:27:21,194 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67, 83.89, 83.49, 83.19, 84.72, 84.67, 84.47, 84.19, 84.13, 84.92, 83.96, 84.12, 83.38, 83.41, 83.5, 83.23]
2024-08-12 00:27:21,194 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029, 0.6444444444444445, 0.6320754716981132, 0.6197478991596639, 0.6029900332225914, 0.5859598853868195, 0.5661103979460848, 0.539833531510107, 0.5294117647058824, 0.5230769230769231, 0.5053140096618357, 0.49866190900981266, 0.49122807017543857, 0.47836538461538464, 0.45911477869467365, 0.443010752688172]
2024-08-12 00:27:21,688 [trainer.py] => All params: 110611171
2024-08-12 00:27:21,691 [trainer.py] => Trainable params: 81418
2024-08-12 00:27:21,691 [inflora.py] => Learning on 180-190
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_v.18.weight', 'image_encoder.blocks.11.attn.lora_B_v.18.weight', 'image_encoder.blocks.0.attn.lora_B_k.18.weight', 'image_encoder.blocks.8.attn.lora_B_k.18.weight', 'image_encoder.blocks.1.attn.lora_B_v.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.18.weight', 'classifier_pool.18.weight', 'image_encoder.blocks.4.attn.lora_B_k.18.weight', 'image_encoder.blocks.3.attn.lora_B_v.18.weight', 'image_encoder.blocks.6.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_k.18.weight', 'image_encoder.blocks.2.attn.lora_B_v.18.weight', 'image_encoder.blocks.9.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.18.weight', 'image_encoder.blocks.6.attn.lora_B_k.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.18.weight', 'classifier_pool.18.bias', 'image_encoder.blocks.1.attn.lora_B_k.18.weight', 'image_encoder.blocks.11.attn.lora_B_k.18.weight', 'image_encoder.blocks.8.attn.lora_B_v.18.weight', 'image_encoder.blocks.9.attn.lora_B_k.18.weight', 'image_encoder.blocks.4.attn.lora_B_v.18.weight', 'image_encoder.blocks.2.attn.lora_B_k.18.weight', 'image_encoder.blocks.7.attn.lora_B_v.18.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 18, Epoch 1/20 => Loss 3.092, Train_accy 12.17:   0%|          | 0/20 [00:01<?, ?it/s]
Task 18, Epoch 1/20 => Loss 3.092, Train_accy 12.17:   5%|▌         | 1/20 [00:01<00:33,  1.77s/it]
Task 18, Epoch 2/20 => Loss 2.009, Train_accy 37.04:   5%|▌         | 1/20 [00:03<00:33,  1.77s/it]
Task 18, Epoch 2/20 => Loss 2.009, Train_accy 37.04:  10%|█         | 2/20 [00:03<00:32,  1.78s/it]
Task 18, Epoch 3/20 => Loss 1.408, Train_accy 56.61:  10%|█         | 2/20 [00:05<00:32,  1.78s/it]
Task 18, Epoch 3/20 => Loss 1.408, Train_accy 56.61:  15%|█▌        | 3/20 [00:05<00:30,  1.81s/it]
Task 18, Epoch 4/20 => Loss 1.055, Train_accy 62.43:  15%|█▌        | 3/20 [00:07<00:30,  1.81s/it]
Task 18, Epoch 4/20 => Loss 1.055, Train_accy 62.43:  20%|██        | 4/20 [00:07<00:29,  1.81s/it]
Task 18, Epoch 5/20 => Loss 0.820, Train_accy 69.84:  20%|██        | 4/20 [00:09<00:29,  1.81s/it]
Task 18, Epoch 5/20 => Loss 0.820, Train_accy 69.84:  25%|██▌       | 5/20 [00:09<00:27,  1.80s/it]
Task 18, Epoch 6/20 => Loss 0.674, Train_accy 79.89:  25%|██▌       | 5/20 [00:10<00:27,  1.80s/it]
Task 18, Epoch 6/20 => Loss 0.674, Train_accy 79.89:  30%|███       | 6/20 [00:10<00:25,  1.80s/it]
Task 18, Epoch 7/20 => Loss 0.529, Train_accy 82.54:  30%|███       | 6/20 [00:12<00:25,  1.80s/it]
Task 18, Epoch 7/20 => Loss 0.529, Train_accy 82.54:  35%|███▌      | 7/20 [00:12<00:23,  1.80s/it]
Task 18, Epoch 8/20 => Loss 0.389, Train_accy 88.36:  35%|███▌      | 7/20 [00:14<00:23,  1.80s/it]
Task 18, Epoch 8/20 => Loss 0.389, Train_accy 88.36:  40%|████      | 8/20 [00:14<00:21,  1.80s/it]
Task 18, Epoch 9/20 => Loss 0.410, Train_accy 87.30:  40%|████      | 8/20 [00:16<00:21,  1.80s/it]
Task 18, Epoch 9/20 => Loss 0.410, Train_accy 87.30:  45%|████▌     | 9/20 [00:16<00:19,  1.80s/it]
Task 18, Epoch 10/20 => Loss 0.306, Train_accy 92.06:  45%|████▌     | 9/20 [00:17<00:19,  1.80s/it]
Task 18, Epoch 10/20 => Loss 0.306, Train_accy 92.06:  50%|█████     | 10/20 [00:17<00:17,  1.79s/it]
Task 18, Epoch 11/20 => Loss 0.327, Train_accy 91.53:  50%|█████     | 10/20 [00:19<00:17,  1.79s/it]
Task 18, Epoch 11/20 => Loss 0.327, Train_accy 91.53:  55%|█████▌    | 11/20 [00:19<00:16,  1.79s/it]
Task 18, Epoch 12/20 => Loss 0.234, Train_accy 95.24:  55%|█████▌    | 11/20 [00:21<00:16,  1.79s/it]
Task 18, Epoch 12/20 => Loss 0.234, Train_accy 95.24:  60%|██████    | 12/20 [00:21<00:14,  1.80s/it]
Task 18, Epoch 13/20 => Loss 0.264, Train_accy 93.65:  60%|██████    | 12/20 [00:23<00:14,  1.80s/it]
Task 18, Epoch 13/20 => Loss 0.264, Train_accy 93.65:  65%|██████▌   | 13/20 [00:23<00:12,  1.81s/it]
Task 18, Epoch 14/20 => Loss 0.222, Train_accy 95.24:  65%|██████▌   | 13/20 [00:25<00:12,  1.81s/it]
Task 18, Epoch 14/20 => Loss 0.222, Train_accy 95.24:  70%|███████   | 14/20 [00:25<00:10,  1.81s/it]
Task 18, Epoch 15/20 => Loss 0.263, Train_accy 95.24:  70%|███████   | 14/20 [00:27<00:10,  1.81s/it]
Task 18, Epoch 15/20 => Loss 0.263, Train_accy 95.24:  75%|███████▌  | 15/20 [00:27<00:09,  1.80s/it]
Task 18, Epoch 16/20 => Loss 0.266, Train_accy 92.59:  75%|███████▌  | 15/20 [00:28<00:09,  1.80s/it]
Task 18, Epoch 16/20 => Loss 0.266, Train_accy 92.59:  80%|████████  | 16/20 [00:28<00:07,  1.80s/it]
Task 18, Epoch 17/20 => Loss 0.225, Train_accy 94.18:  80%|████████  | 16/20 [00:30<00:07,  1.80s/it]
Task 18, Epoch 17/20 => Loss 0.225, Train_accy 94.18:  85%|████████▌ | 17/20 [00:30<00:05,  1.80s/it]
Task 18, Epoch 18/20 => Loss 0.208, Train_accy 94.71:  85%|████████▌ | 17/20 [00:32<00:05,  1.80s/it]
Task 18, Epoch 18/20 => Loss 0.208, Train_accy 94.71:  90%|█████████ | 18/20 [00:32<00:03,  1.80s/it]
Task 18, Epoch 19/20 => Loss 0.186, Train_accy 95.24:  90%|█████████ | 18/20 [00:34<00:03,  1.80s/it]
Task 18, Epoch 19/20 => Loss 0.186, Train_accy 95.24:  95%|█████████▌| 19/20 [00:34<00:01,  1.79s/it]
Task 18, Epoch 20/20 => Loss 0.201, Train_accy 94.18:  95%|█████████▌| 19/20 [00:36<00:01,  1.79s/it]
Task 18, Epoch 20/20 => Loss 0.201, Train_accy 94.18: 100%|██████████| 20/20 [00:36<00:00,  1.80s/it]
Task 18, Epoch 20/20 => Loss 0.201, Train_accy 94.18: 100%|██████████| 20/20 [00:36<00:00,  1.80s/it]
2024-08-12 00:28:01,485 [inflora.py] => Task 18, Epoch 20/20 => Loss 0.201, Train_accy 94.18
Threshold:  0.995
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 11/768 type remove
Layer 2 : 43/768 type remove
Layer 3 : 88/768 type remove
Layer 4 : 115/768 type remove
Layer 5 : 167/768 type remove
Layer 6 : 168/768 type remove
Layer 7 : 186/768 type remove
Layer 8 : 269/768 type remove
Layer 9 : 383/768 type remove
Layer 10 : 308/768 type retain
Layer 11 : 327/768 type remove
Layer 12 : 371/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:28:10,783 [trainer.py] => Time:49.091668367385864
1434 1434
1434 1434
2024-08-12 00:28:14,385 [trainer.py] => Time:3.6015400886535645
2024-08-12 00:28:14,385 [inflora.py] => Exemplar size: 0
2024-08-12 00:28:14,385 [trainer.py] => CNN: {'total': 42.75, '00-09': 45.45, '10-19': 44.9, '20-29': 47.47, '30-39': 51.16, '40-49': 50.0, '50-59': 42.31, '60-69': 49.21, '70-79': 41.67, '80-89': 45.68, '90-99': 38.71, '100-109': 48.33, '110-119': 45.95, '120-129': 26.67, '130-139': 51.16, '140-149': 51.32, '150-159': 27.45, '160-169': 30.59, '170-179': 25.81, '180-189': 20.51, 'old': 43.37, 'new': 20.51}
2024-08-12 00:28:14,385 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23, 58.89, 58.25, 57.35, 56.64, 55.87, 54.3, 51.25, 50.28, 49.95, 48.79, 48.35, 47.87, 46.55, 44.79, 43.08, 42.75]
2024-08-12 00:28:14,385 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67, 83.89, 83.49, 83.19, 84.72, 84.67, 84.47, 84.19, 84.13, 84.92, 83.96, 84.12, 83.38, 83.41, 83.5, 83.23, 83.26]
2024-08-12 00:28:14,385 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029, 0.6444444444444445, 0.6320754716981132, 0.6197478991596639, 0.6029900332225914, 0.5859598853868195, 0.5661103979460848, 0.539833531510107, 0.5294117647058824, 0.5230769230769231, 0.5053140096618357, 0.49866190900981266, 0.49122807017543857, 0.47836538461538464, 0.45911477869467365, 0.443010752688172, 0.4393305439330544]
2024-08-12 00:28:14,904 [trainer.py] => All params: 110611171
2024-08-12 00:28:14,907 [trainer.py] => Trainable params: 81418
2024-08-12 00:28:14,907 [inflora.py] => Learning on 190-200
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_k.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.19.weight', 'image_encoder.blocks.10.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_k.19.weight', 'image_encoder.blocks.1.attn.lora_B_v.19.weight', 'image_encoder.blocks.5.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_v.19.weight', 'image_encoder.blocks.9.attn.lora_B_k.19.weight', 'image_encoder.blocks.5.attn.lora_B_k.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.19.weight', 'image_encoder.blocks.10.attn.lora_B_v.19.weight', 'classifier_pool.19.bias', 'image_encoder.blocks.0.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_v.19.weight', 'image_encoder.blocks.6.attn.lora_B_k.19.weight', 'image_encoder.blocks.8.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_k.19.weight', 'image_encoder.blocks.11.attn.lora_B_v.19.weight', 'image_encoder.blocks.9.attn.lora_B_v.19.weight', 'image_encoder.blocks.0.attn.lora_B_v.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.19.weight', 'classifier_pool.19.weight', 'image_encoder.blocks.4.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_v.19.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 19, Epoch 1/20 => Loss 2.651, Train_accy 20.12:   0%|          | 0/20 [00:02<?, ?it/s]
Task 19, Epoch 1/20 => Loss 2.651, Train_accy 20.12:   5%|▌         | 1/20 [00:02<00:43,  2.30s/it]
Task 19, Epoch 2/20 => Loss 1.383, Train_accy 52.07:   5%|▌         | 1/20 [00:04<00:43,  2.30s/it]
Task 19, Epoch 2/20 => Loss 1.383, Train_accy 52.07:  10%|█         | 2/20 [00:04<00:41,  2.30s/it]
Task 19, Epoch 3/20 => Loss 0.877, Train_accy 68.34:  10%|█         | 2/20 [00:06<00:41,  2.30s/it]
Task 19, Epoch 3/20 => Loss 0.877, Train_accy 68.34:  15%|█▌        | 3/20 [00:06<00:39,  2.31s/it]
Task 19, Epoch 4/20 => Loss 0.708, Train_accy 76.92:  15%|█▌        | 3/20 [00:09<00:39,  2.31s/it]
Task 19, Epoch 4/20 => Loss 0.708, Train_accy 76.92:  20%|██        | 4/20 [00:09<00:36,  2.30s/it]
Task 19, Epoch 5/20 => Loss 0.504, Train_accy 80.18:  20%|██        | 4/20 [00:11<00:36,  2.30s/it]
Task 19, Epoch 5/20 => Loss 0.504, Train_accy 80.18:  25%|██▌       | 5/20 [00:11<00:34,  2.31s/it]
Task 19, Epoch 6/20 => Loss 0.655, Train_accy 85.80:  25%|██▌       | 5/20 [00:13<00:34,  2.31s/it]
Task 19, Epoch 6/20 => Loss 0.655, Train_accy 85.80:  30%|███       | 6/20 [00:13<00:32,  2.31s/it]
Task 19, Epoch 7/20 => Loss 0.481, Train_accy 85.80:  30%|███       | 6/20 [00:16<00:32,  2.31s/it]
Task 19, Epoch 7/20 => Loss 0.481, Train_accy 85.80:  35%|███▌      | 7/20 [00:16<00:30,  2.31s/it]
Task 19, Epoch 8/20 => Loss 0.380, Train_accy 87.57:  35%|███▌      | 7/20 [00:18<00:30,  2.31s/it]
Task 19, Epoch 8/20 => Loss 0.380, Train_accy 87.57:  40%|████      | 8/20 [00:18<00:27,  2.31s/it]
Task 19, Epoch 9/20 => Loss 0.434, Train_accy 87.57:  40%|████      | 8/20 [00:20<00:27,  2.31s/it]
Task 19, Epoch 9/20 => Loss 0.434, Train_accy 87.57:  45%|████▌     | 9/20 [00:20<00:25,  2.30s/it]
Task 19, Epoch 10/20 => Loss 0.310, Train_accy 89.64:  45%|████▌     | 9/20 [00:23<00:25,  2.30s/it]
Task 19, Epoch 10/20 => Loss 0.310, Train_accy 89.64:  50%|█████     | 10/20 [00:23<00:23,  2.30s/it]
Task 19, Epoch 11/20 => Loss 0.401, Train_accy 89.64:  50%|█████     | 10/20 [00:25<00:23,  2.30s/it]
Task 19, Epoch 11/20 => Loss 0.401, Train_accy 89.64:  55%|█████▌    | 11/20 [00:25<00:20,  2.31s/it]
Task 19, Epoch 12/20 => Loss 0.237, Train_accy 91.42:  55%|█████▌    | 11/20 [00:27<00:20,  2.31s/it]
Task 19, Epoch 12/20 => Loss 0.237, Train_accy 91.42:  60%|██████    | 12/20 [00:27<00:18,  2.31s/it]
Task 19, Epoch 13/20 => Loss 0.291, Train_accy 92.31:  60%|██████    | 12/20 [00:30<00:18,  2.31s/it]
Task 19, Epoch 13/20 => Loss 0.291, Train_accy 92.31:  65%|██████▌   | 13/20 [00:30<00:16,  2.33s/it]
Task 19, Epoch 14/20 => Loss 0.265, Train_accy 91.42:  65%|██████▌   | 13/20 [00:32<00:16,  2.33s/it]
Task 19, Epoch 14/20 => Loss 0.265, Train_accy 91.42:  70%|███████   | 14/20 [00:32<00:14,  2.34s/it]
Task 19, Epoch 15/20 => Loss 0.260, Train_accy 92.31:  70%|███████   | 14/20 [00:34<00:14,  2.34s/it]
Task 19, Epoch 15/20 => Loss 0.260, Train_accy 92.31:  75%|███████▌  | 15/20 [00:34<00:11,  2.32s/it]
Task 19, Epoch 16/20 => Loss 0.306, Train_accy 91.42:  75%|███████▌  | 15/20 [00:37<00:11,  2.32s/it]
Task 19, Epoch 16/20 => Loss 0.306, Train_accy 91.42:  80%|████████  | 16/20 [00:37<00:09,  2.33s/it]
Task 19, Epoch 17/20 => Loss 0.209, Train_accy 92.90:  80%|████████  | 16/20 [00:39<00:09,  2.33s/it]
Task 19, Epoch 17/20 => Loss 0.209, Train_accy 92.90:  85%|████████▌ | 17/20 [00:39<00:06,  2.33s/it]
Task 19, Epoch 18/20 => Loss 0.240, Train_accy 91.42:  85%|████████▌ | 17/20 [00:41<00:06,  2.33s/it]
Task 19, Epoch 18/20 => Loss 0.240, Train_accy 91.42:  90%|█████████ | 18/20 [00:41<00:04,  2.33s/it]
Task 19, Epoch 19/20 => Loss 0.192, Train_accy 95.56:  90%|█████████ | 18/20 [00:44<00:04,  2.33s/it]
Task 19, Epoch 19/20 => Loss 0.192, Train_accy 95.56:  95%|█████████▌| 19/20 [00:44<00:02,  2.32s/it]
Task 19, Epoch 20/20 => Loss 0.177, Train_accy 94.67:  95%|█████████▌| 19/20 [00:46<00:02,  2.32s/it]
Task 19, Epoch 20/20 => Loss 0.177, Train_accy 94.67: 100%|██████████| 20/20 [00:46<00:00,  2.32s/it]
Task 19, Epoch 20/20 => Loss 0.177, Train_accy 94.67: 100%|██████████| 20/20 [00:46<00:00,  2.32s/it]
2024-08-12 00:29:05,313 [inflora.py] => Task 19, Epoch 20/20 => Loss 0.177, Train_accy 94.67
Threshold:  0.9975
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 54/768 type remove
Layer 3 : 117/768 type remove
Layer 4 : 164/768 type remove
Layer 5 : 231/768 type remove
Layer 6 : 243/768 type remove
Layer 7 : 274/768 type remove
Layer 8 : 370/768 type remove
Layer 9 : 276/768 type retain
Layer 10 : 197/768 type retain
Layer 11 : 293/768 type retain
Layer 12 : 260/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:29:14,912 [trainer.py] => Time:60.005054235458374
1519 1519
1519 1519
2024-08-12 00:29:18,673 [trainer.py] => Time:3.7613861560821533
2024-08-12 00:29:18,674 [inflora.py] => Exemplar size: 0
2024-08-12 00:29:18,674 [trainer.py] => CNN: {'total': 42.13, '00-09': 45.45, '10-19': 44.9, '20-29': 50.51, '30-39': 50.0, '40-49': 46.88, '50-59': 42.31, '60-69': 48.41, '70-79': 41.67, '80-89': 40.74, '90-99': 38.71, '100-109': 46.67, '110-119': 44.59, '120-129': 28.33, '130-139': 51.16, '140-149': 50.0, '150-159': 27.45, '160-169': 30.59, '170-179': 24.19, '180-189': 15.38, '190-199': 43.53, 'old': 42.05, 'new': 43.53}
2024-08-12 00:29:18,674 [trainer.py] => CNN top1 curve: [75.32, 64.57, 64.23, 58.89, 58.25, 57.35, 56.64, 55.87, 54.3, 51.25, 50.28, 49.95, 48.79, 48.35, 47.87, 46.55, 44.79, 43.08, 42.75, 42.13]
2024-08-12 00:29:18,674 [trainer.py] => CNN top1 with task curve: [75.32, 82.29, 84.67, 83.89, 83.49, 83.19, 84.72, 84.67, 84.47, 84.19, 84.13, 84.92, 83.96, 84.12, 83.38, 83.41, 83.5, 83.23, 83.26, 83.15]
2024-08-12 00:29:18,674 [trainer.py] => CNN top1 task curve: [1.0, 0.7371428571428571, 0.6970802919708029, 0.6444444444444445, 0.6320754716981132, 0.6197478991596639, 0.6029900332225914, 0.5859598853868195, 0.5661103979460848, 0.539833531510107, 0.5294117647058824, 0.5230769230769231, 0.5053140096618357, 0.49866190900981266, 0.49122807017543857, 0.47836538461538464, 0.45911477869467365, 0.443010752688172, 0.4393305439330544, 0.43317972350230416]
Traceback (most recent call last):
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/main.py", line 33, in <module>
    main()
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/main.py", line 11, in main
    train(args)
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/trainer.py", line 28, in train
    _set_random(args["seed"])
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/trainer.py", line 101, in _set_random
    torch.manual_seed(args['seed'])
TypeError: 'int' object is not subscriptable
logs/omnibenchmark/10_10_sip/InfLoRA/adam/4/0.95_1.0-0.0005/1993
2024-08-12 00:29:22,407 [trainer.py] => config: ./configs/omn_inflora.json
2024-08-12 00:29:22,419 [trainer.py] => device: [device(type='cuda', index=0)]
2024-08-12 00:29:22,419 [trainer.py] => prefix: reproduce
2024-08-12 00:29:22,419 [trainer.py] => dataset: omnibenchmark
2024-08-12 00:29:22,419 [trainer.py] => data_path: /mnt/mydisk/ruoheng.li/lrh/Dataset
2024-08-12 00:29:22,419 [trainer.py] => memory_size: 0
2024-08-12 00:29:22,419 [trainer.py] => memory_per_class: 0
2024-08-12 00:29:22,419 [trainer.py] => fixed_memory: True
2024-08-12 00:29:22,419 [trainer.py] => shuffle: True
2024-08-12 00:29:22,419 [trainer.py] => init_cls: 10
2024-08-12 00:29:22,419 [trainer.py] => increment: 10
2024-08-12 00:29:22,419 [trainer.py] => model_name: InfLoRA
2024-08-12 00:29:22,419 [trainer.py] => net_type: sip
2024-08-12 00:29:22,419 [trainer.py] => embd_dim: 768
2024-08-12 00:29:22,419 [trainer.py] => num_heads: 12
2024-08-12 00:29:22,419 [trainer.py] => total_sessions: 30
2024-08-12 00:29:22,419 [trainer.py] => seed: 1993
2024-08-12 00:29:22,419 [trainer.py] => EPSILON: 1e-08
2024-08-12 00:29:22,419 [trainer.py] => init_epoch: 20
2024-08-12 00:29:22,419 [trainer.py] => optim: adam
2024-08-12 00:29:22,419 [trainer.py] => init_lr: 0.0005
2024-08-12 00:29:22,419 [trainer.py] => init_lr_decay: 0.1
2024-08-12 00:29:22,419 [trainer.py] => init_weight_decay: 0.0
2024-08-12 00:29:22,419 [trainer.py] => epochs: 20
2024-08-12 00:29:22,419 [trainer.py] => lrate: 0.0005
2024-08-12 00:29:22,419 [trainer.py] => lrate_decay: 0.1
2024-08-12 00:29:22,419 [trainer.py] => batch_size: 48
2024-08-12 00:29:22,419 [trainer.py] => weight_decay: 0.0
2024-08-12 00:29:22,419 [trainer.py] => rank: 4
2024-08-12 00:29:22,420 [trainer.py] => lamb: 0.95
2024-08-12 00:29:22,420 [trainer.py] => lame: 1.0
2024-08-12 00:29:22,420 [trainer.py] => num_workers: 16
2024-08-12 00:29:22,652 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2024-08-12 00:29:25,246 [trainer.py] => All params: 112239531
2024-08-12 00:29:25,251 [trainer.py] => Trainable params: 112239531
2024-08-12 00:29:25,251 [inflora.py] => Learning on 0-10
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_v.0.weight', 'image_encoder.blocks.1.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_k.0.weight', 'classifier_pool.0.bias', 'image_encoder.blocks.11.attn.lora_B_k.0.weight', 'image_encoder.blocks.7.attn.lora_B_k.0.weight', 'image_encoder.blocks.9.attn.lora_B_v.0.weight', 'image_encoder.blocks.8.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_k.0.weight', 'image_encoder.blocks.4.attn.lora_B_v.0.weight', 'image_encoder.blocks.8.attn.lora_B_v.0.weight', 'image_encoder.blocks.7.attn.lora_B_v.0.weight', 'image_encoder.blocks.4.attn.lora_B_k.0.weight', 'image_encoder.blocks.3.attn.lora_B_v.0.weight', 'image_encoder.blocks.5.attn.lora_B_v.0.weight', 'image_encoder.blocks.6.attn.lora_B_k.0.weight', 'image_encoder.blocks.11.attn.lora_B_v.0.weight', 'image_encoder.blocks.1.attn.lora_B_k.0.weight', 'image_encoder.blocks.10.attn.lora_B_k.0.weight', 'image_encoder.blocks.6.attn.lora_B_v.0.weight', 'image_encoder.blocks.5.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_v.0.weight', 'image_encoder.blocks.2.attn.lora_B_k.0.weight', 'classifier_pool.0.weight', 'image_encoder.blocks.2.attn.lora_B_v.0.weight', 'image_encoder.blocks.3.attn.lora_B_k.0.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 0, Epoch 1/20 => Loss 0.518, Train_accy 82.34:   0%|          | 0/20 [00:11<?, ?it/s]
Task 0, Epoch 1/20 => Loss 0.518, Train_accy 82.34:   5%|▌         | 1/20 [00:11<03:31, 11.13s/it]
Task 0, Epoch 2/20 => Loss 0.163, Train_accy 93.17:   5%|▌         | 1/20 [00:22<03:31, 11.13s/it]
Task 0, Epoch 2/20 => Loss 0.163, Train_accy 93.17:  10%|█         | 2/20 [00:22<03:20, 11.17s/it]
Task 0, Epoch 3/20 => Loss 0.137, Train_accy 94.54:  10%|█         | 2/20 [00:33<03:20, 11.17s/it]
Task 0, Epoch 3/20 => Loss 0.137, Train_accy 94.54:  15%|█▌        | 3/20 [00:33<03:08, 11.10s/it]
Task 0, Epoch 4/20 => Loss 0.106, Train_accy 96.40:  15%|█▌        | 3/20 [00:44<03:08, 11.10s/it]
Task 0, Epoch 4/20 => Loss 0.106, Train_accy 96.40:  20%|██        | 4/20 [00:44<02:57, 11.08s/it]
Task 0, Epoch 5/20 => Loss 0.084, Train_accy 96.96:  20%|██        | 4/20 [00:55<02:57, 11.08s/it]
Task 0, Epoch 5/20 => Loss 0.084, Train_accy 96.96:  25%|██▌       | 5/20 [00:55<02:45, 11.05s/it]
Task 0, Epoch 6/20 => Loss 0.099, Train_accy 96.53:  25%|██▌       | 5/20 [01:06<02:45, 11.05s/it]
Task 0, Epoch 6/20 => Loss 0.099, Train_accy 96.53:  30%|███       | 6/20 [01:06<02:34, 11.07s/it]
Task 0, Epoch 7/20 => Loss 0.072, Train_accy 97.42:  30%|███       | 6/20 [01:17<02:34, 11.07s/it]
Task 0, Epoch 7/20 => Loss 0.072, Train_accy 97.42:  35%|███▌      | 7/20 [01:17<02:23, 11.08s/it]
Task 0, Epoch 8/20 => Loss 0.078, Train_accy 97.06:  35%|███▌      | 7/20 [01:28<02:23, 11.08s/it]
Task 0, Epoch 8/20 => Loss 0.078, Train_accy 97.06:  40%|████      | 8/20 [01:28<02:12, 11.05s/it]
Task 0, Epoch 9/20 => Loss 0.085, Train_accy 97.16:  40%|████      | 8/20 [01:39<02:12, 11.05s/it]
Task 0, Epoch 9/20 => Loss 0.085, Train_accy 97.16:  45%|████▌     | 9/20 [01:39<02:01, 11.06s/it]
Task 0, Epoch 10/20 => Loss 0.073, Train_accy 97.42:  45%|████▌     | 9/20 [01:50<02:01, 11.06s/it]
Task 0, Epoch 10/20 => Loss 0.073, Train_accy 97.42:  50%|█████     | 10/20 [01:50<01:50, 11.03s/it]
Task 0, Epoch 11/20 => Loss 0.075, Train_accy 97.42:  50%|█████     | 10/20 [02:01<01:50, 11.03s/it]
Task 0, Epoch 11/20 => Loss 0.075, Train_accy 97.42:  55%|█████▌    | 11/20 [02:01<01:39, 11.06s/it]
Task 0, Epoch 12/20 => Loss 0.063, Train_accy 97.91:  55%|█████▌    | 11/20 [02:12<01:39, 11.06s/it]
Task 0, Epoch 12/20 => Loss 0.063, Train_accy 97.91:  60%|██████    | 12/20 [02:12<01:28, 11.04s/it]
Task 0, Epoch 13/20 => Loss 0.055, Train_accy 98.14:  60%|██████    | 12/20 [02:23<01:28, 11.04s/it]
Task 0, Epoch 13/20 => Loss 0.055, Train_accy 98.14:  65%|██████▌   | 13/20 [02:23<01:17, 11.03s/it]
Task 0, Epoch 14/20 => Loss 0.051, Train_accy 98.10:  65%|██████▌   | 13/20 [02:34<01:17, 11.03s/it]
Task 0, Epoch 14/20 => Loss 0.051, Train_accy 98.10:  70%|███████   | 14/20 [02:34<01:06, 11.03s/it]
Task 0, Epoch 15/20 => Loss 0.052, Train_accy 98.46:  70%|███████   | 14/20 [02:46<01:06, 11.03s/it]
Task 0, Epoch 15/20 => Loss 0.052, Train_accy 98.46:  75%|███████▌  | 15/20 [02:46<00:55, 11.09s/it]
Task 0, Epoch 16/20 => Loss 0.050, Train_accy 97.97:  75%|███████▌  | 15/20 [02:57<00:55, 11.09s/it]
Task 0, Epoch 16/20 => Loss 0.050, Train_accy 97.97:  80%|████████  | 16/20 [02:57<00:44, 11.08s/it]
Task 0, Epoch 17/20 => Loss 0.047, Train_accy 98.17:  80%|████████  | 16/20 [03:08<00:44, 11.08s/it]
Task 0, Epoch 17/20 => Loss 0.047, Train_accy 98.17:  85%|████████▌ | 17/20 [03:08<00:33, 11.07s/it]
Task 0, Epoch 18/20 => Loss 0.045, Train_accy 98.46:  85%|████████▌ | 17/20 [03:19<00:33, 11.07s/it]
Task 0, Epoch 18/20 => Loss 0.045, Train_accy 98.46:  90%|█████████ | 18/20 [03:19<00:22, 11.06s/it]
Task 0, Epoch 19/20 => Loss 0.041, Train_accy 98.59:  90%|█████████ | 18/20 [03:30<00:22, 11.06s/it]
Task 0, Epoch 19/20 => Loss 0.041, Train_accy 98.59:  95%|█████████▌| 19/20 [03:30<00:11, 11.04s/it]
Task 0, Epoch 20/20 => Loss 0.049, Train_accy 98.27:  95%|█████████▌| 19/20 [03:41<00:11, 11.04s/it]
Task 0, Epoch 20/20 => Loss 0.049, Train_accy 98.27: 100%|██████████| 20/20 [03:41<00:00, 11.05s/it]
Task 0, Epoch 20/20 => Loss 0.049, Train_accy 98.27: 100%|██████████| 20/20 [03:41<00:00, 11.06s/it]
2024-08-12 00:33:17,276 [inflora.py] => Task 0, Epoch 20/20 => Loss 0.049, Train_accy 98.27
Threshold:  0.95
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 6/768 type remove
Layer 2 : 9/768 type remove
Layer 3 : 11/768 type remove
Layer 4 : 10/768 type remove
Layer 5 : 16/768 type remove
Layer 6 : 15/768 type remove
Layer 7 : 13/768 type remove
Layer 8 : 14/768 type remove
Layer 9 : 15/768 type remove
Layer 10 : 13/768 type remove
Layer 11 : 3/768 type remove
Layer 12 : 6/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:33:34,078 [trainer.py] => Time:248.82528066635132
200 200
200 200
2024-08-12 00:33:35,755 [trainer.py] => Time:1.675920009613037
2024-08-12 00:33:35,755 [inflora.py] => Exemplar size: 0
2024-08-12 00:33:35,755 [trainer.py] => CNN: {'total': 99.0, '00-09': 99.0, 'old': 0, 'new': 99.0}
2024-08-12 00:33:35,755 [trainer.py] => CNN top1 curve: [99.0]
2024-08-12 00:33:35,755 [trainer.py] => CNN top1 with task curve: [99.0]
2024-08-12 00:33:35,755 [trainer.py] => CNN top1 task curve: [1.0]
2024-08-12 00:33:36,323 [trainer.py] => All params: 112239531
2024-08-12 00:33:36,327 [trainer.py] => Trainable params: 81418
2024-08-12 00:33:36,327 [inflora.py] => Learning on 10-20
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.11.weight', 'classifier_pool.10.bias', 'image_encoder.blocks.0.attn.lora_B_v.15.weight', 'image_encoder.blocks.9.attn.lora_B_v.13.weight', 'image_encoder.blocks.0.attn.lora_B_k.19.weight', 'image_encoder.blocks.5.attn.lora_B_v.11.weight', 'image_encoder.blocks.6.attn.lora_B_v.14.weight', 'image_encoder.blocks.3.attn.lora_B_v.15.weight', 'image_encoder.blocks.6.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.17.weight', 'image_encoder.blocks.1.attn.lora_B_v.14.weight', 'classifier_pool.15.weight', 'image_encoder.blocks.11.attn.lora_B_v.16.weight', 'image_encoder.blocks.8.attn.lora_B_v.12.weight', 'image_encoder.blocks.5.attn.lora_B_k.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.11.weight', 'image_encoder.blocks.11.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.1.weight', 'image_encoder.blocks.8.attn.lora_B_k.13.weight', 'image_encoder.blocks.1.attn.lora_B_k.15.weight', 'image_encoder.blocks.2.attn.lora_B_v.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.16.weight', 'image_encoder.blocks.10.attn.lora_B_k.10.weight', 'image_encoder.blocks.1.attn.lora_B_k.14.weight', 'image_encoder.blocks.9.attn.lora_B_k.1.weight', 'classifier_pool.18.bias', 'image_encoder.blocks.2.attn.lora_B_k.10.weight', 'image_encoder.blocks.2.attn.lora_B_k.15.weight', 'image_encoder.blocks.1.attn.lora_B_k.17.weight', 'classifier_pool.16.weight', 'image_encoder.blocks.6.attn.lora_B_k.11.weight', 'image_encoder.blocks.2.attn.lora_B_v.19.weight', 'image_encoder.blocks.3.attn.lora_B_v.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.1.weight', 'image_encoder.blocks.8.attn.lora_B_k.15.weight', 'image_encoder.blocks.9.attn.lora_B_v.10.weight', 'image_encoder.blocks.9.attn.lora_B_v.12.weight', 'image_encoder.blocks.10.attn.lora_B_v.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.17.weight', 'image_encoder.blocks.6.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_k.19.weight', 'image_encoder.blocks.0.attn.lora_B_k.10.weight', 'classifier_pool.14.weight', 'image_encoder.blocks.4.attn.lora_B_k.12.weight', 'image_encoder.blocks.0.attn.lora_B_v.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.19.weight', 'classifier_pool.19.weight', 'image_encoder.blocks.4.attn.lora_B_v.1.weight', 'image_encoder.blocks.3.attn.lora_B_k.17.weight', 'image_encoder.blocks.1.attn.lora_B_k.18.weight', 'image_encoder.blocks.8.attn.lora_B_v.13.weight', 'image_encoder.blocks.7.attn.lora_B_k.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.10.weight', 'image_encoder.blocks.11.attn.lora_B_k.12.weight', 'image_encoder.blocks.2.attn.lora_B_k.17.weight', 'image_encoder.blocks.9.attn.lora_B_v.14.weight', 'image_encoder.blocks.7.attn.lora_B_v.17.weight', 'classifier_pool.11.weight', 'image_encoder.blocks.1.attn.lora_B_v.12.weight', 'image_encoder.blocks.6.attn.lora_B_v.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.19.weight', 'image_encoder.blocks.6.attn.lora_B_k.10.weight', 'image_encoder.blocks.1.attn.lora_B_v.1.weight', 'image_encoder.blocks.4.attn.lora_B_v.13.weight', 'image_encoder.blocks.5.attn.lora_B_v.15.weight', 'image_encoder.blocks.7.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.18.weight', 'image_encoder.blocks.9.attn.lora_B_v.18.weight', 'image_encoder.blocks.9.attn.lora_B_v.15.weight', 'image_encoder.blocks.3.attn.lora_B_k.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.14.weight', 'image_encoder.blocks.5.attn.lora_B_v.12.weight', 'classifier_pool.18.weight', 'image_encoder.blocks.10.attn.lora_B_v.18.weight', 'image_encoder.blocks.9.attn.lora_B_v.1.weight', 'image_encoder.blocks.0.attn.lora_B_v.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.18.weight', 'image_encoder.blocks.1.attn.lora_B_v.15.weight', 'image_encoder.blocks.11.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_v.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.14.weight', 'classifier_pool.12.bias', 'image_encoder.blocks.10.attn.lora_B_k.14.weight', 'image_encoder.blocks.2.attn.lora_B_k.13.weight', 'image_encoder.blocks.2.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_v.12.weight', 'image_encoder.blocks.6.attn.lora_B_k.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_v.10.weight', 'image_encoder.blocks.7.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.1.weight', 'image_encoder.blocks.9.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.15.weight', 'image_encoder.blocks.4.attn.lora_B_k.15.weight', 'image_encoder.blocks.7.attn.lora_B_v.15.weight', 'image_encoder.blocks.4.attn.lora_B_v.10.weight', 'image_encoder.blocks.11.attn.lora_B_v.11.weight', 'image_encoder.blocks.1.attn.lora_B_k.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.12.weight', 'image_encoder.blocks.2.attn.lora_B_v.14.weight', 'image_encoder.blocks.11.attn.lora_B_v.18.weight', 'classifier_pool.1.bias', 'image_encoder.blocks.4.attn.lora_B_k.1.weight', 'image_encoder.blocks.5.attn.lora_B_k.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.13.weight', 'image_encoder.blocks.11.attn.lora_B_k.14.weight', 'image_encoder.blocks.2.attn.lora_B_k.11.weight', 'image_encoder.blocks.5.attn.lora_B_k.12.weight', 'image_encoder.blocks.0.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_k.11.weight', 'image_encoder.blocks.10.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_k.18.weight', 'image_encoder.blocks.8.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.12.weight', 'image_encoder.blocks.9.attn.lora_B_k.13.weight', 'image_encoder.blocks.9.attn.lora_B_v.11.weight', 'image_encoder.blocks.5.attn.lora_B_k.18.weight', 'image_encoder.blocks.9.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_k.16.weight', 'image_encoder.blocks.10.attn.lora_B_v.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.1.weight', 'image_encoder.blocks.5.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_k.15.weight', 'image_encoder.blocks.7.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_v.19.weight', 'image_encoder.blocks.5.attn.lora_B_v.16.weight', 'image_encoder.blocks.0.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_k.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_v.1.weight', 'image_encoder.blocks.11.attn.lora_B_k.15.weight', 'image_encoder.blocks.11.attn.lora_B_v.12.weight', 'image_encoder.blocks.5.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.10.weight', 'image_encoder.blocks.6.attn.lora_B_k.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.13.weight', 'image_encoder.blocks.6.attn.lora_B_k.16.weight', 'image_encoder.blocks.10.attn.lora_B_k.17.weight', 'image_encoder.blocks.8.attn.lora_B_v.16.weight', 'image_encoder.blocks.6.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_v.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.19.weight', 'image_encoder.blocks.1.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_k.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.11.weight', 'image_encoder.blocks.11.attn.lora_B_k.1.weight', 'image_encoder.blocks.1.attn.lora_B_k.10.weight', 'image_encoder.blocks.10.attn.lora_B_v.10.weight', 'image_encoder.blocks.1.attn.lora_B_k.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_v.10.weight', 'image_encoder.blocks.8.attn.lora_B_v.17.weight', 'classifier_pool.13.weight', 'image_encoder.blocks.7.attn.lora_B_v.13.weight', 'image_encoder.blocks.11.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.18.weight', 'image_encoder.blocks.1.attn.lora_B_k.1.weight', 'image_encoder.blocks.5.attn.lora_B_k.17.weight', 'image_encoder.blocks.7.attn.lora_B_v.11.weight', 'image_encoder.blocks.6.attn.lora_B_v.10.weight', 'image_encoder.blocks.11.attn.lora_B_k.16.weight', 'image_encoder.blocks.3.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.16.weight', 'classifier_pool.16.bias', 'image_encoder.blocks.4.attn.lora_B_v.18.weight', 'image_encoder.blocks.4.attn.lora_B_v.17.weight', 'image_encoder.blocks.7.attn.lora_B_k.17.weight', 'image_encoder.blocks.6.attn.lora_B_k.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.11.weight', 'image_encoder.blocks.3.attn.lora_B_k.13.weight', 'image_encoder.blocks.2.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.14.weight', 'image_encoder.blocks.9.attn.lora_B_k.11.weight', 'image_encoder.blocks.8.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_v.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_v.15.weight', 'image_encoder.blocks.2.attn.lora_B_v.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.13.weight', 'image_encoder.blocks.0.attn.lora_B_k.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.18.weight', 'image_encoder.blocks.6.attn.lora_B_v.12.weight', 'classifier_pool.13.bias', 'image_encoder.blocks.4.attn.lora_B_v.12.weight', 'image_encoder.blocks.7.attn.lora_B_k.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.19.weight', 'image_encoder.blocks.8.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_v.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.12.weight', 'classifier_pool.12.weight', 'image_encoder.blocks.4.attn.lora_B_v.16.weight', 'image_encoder.blocks.0.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_v.15.weight', 'image_encoder.blocks.7.attn.lora_B_k.19.weight', 'image_encoder.blocks.4.attn.lora_B_k.17.weight', 'image_encoder.blocks.6.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_k.17.weight', 'image_encoder.blocks.7.attn.lora_B_v.16.weight', 'image_encoder.blocks.2.attn.lora_B_v.1.weight', 'image_encoder.blocks.5.attn.lora_B_v.14.weight', 'image_encoder.blocks.9.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_k.18.weight', 'image_encoder.blocks.9.attn.lora_B_v.17.weight', 'image_encoder.blocks.8.attn.lora_B_v.19.weight', 'image_encoder.blocks.0.attn.lora_B_v.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.12.weight', 'image_encoder.blocks.0.attn.lora_B_v.19.weight', 'image_encoder.blocks.10.attn.lora_B_k.11.weight', 'classifier_pool.17.weight', 'classifier_pool.17.bias', 'image_encoder.blocks.5.attn.lora_B_v.10.weight', 'image_encoder.blocks.0.attn.lora_B_v.13.weight', 'image_encoder.blocks.3.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.10.weight', 'image_encoder.blocks.7.attn.lora_B_v.19.weight', 'classifier_pool.10.weight', 'image_encoder.blocks.4.attn.lora_B_k.19.weight', 'classifier_pool.11.bias', 'image_encoder.blocks.1.attn.lora_B_v.10.weight', 'image_encoder.blocks.0.attn.lora_B_v.17.weight', 'image_encoder.blocks.0.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_k.13.weight', 'image_encoder.blocks.7.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_k.14.weight', 'image_encoder.blocks.10.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_k.13.weight', 'image_encoder.blocks.1.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.19.weight', 'image_encoder.blocks.8.attn.lora_B_k.12.weight', 'image_encoder.blocks.5.attn.lora_B_k.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.10.weight', 'image_encoder.blocks.6.attn.lora_B_k.13.weight', 'image_encoder.blocks.9.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_k.14.weight', 'image_encoder.blocks.2.attn.lora_B_k.16.weight', 'image_encoder.blocks.5.attn.lora_B_v.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.10.weight', 'image_encoder.blocks.6.attn.lora_B_v.19.weight', 'image_encoder.blocks.4.attn.lora_B_v.11.weight', 'image_encoder.blocks.4.attn.lora_B_v.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.1.weight', 'image_encoder.blocks.8.attn.lora_B_k.10.weight', 'image_encoder.blocks.3.attn.lora_B_k.11.weight', 'classifier_pool.15.bias', 'image_encoder.blocks.8.attn.lora_B_k.14.weight', 'image_encoder.blocks.4.attn.lora_B_k.13.weight', 'image_encoder.blocks.8.attn.lora_B_v.14.weight', 'image_encoder.blocks.9.attn.lora_B_k.10.weight', 'image_encoder.blocks.11.attn.lora_B_v.17.weight', 'image_encoder.blocks.0.attn.lora_B_k.12.weight', 'image_encoder.blocks.0.attn.lora_B_k.18.weight', 'image_encoder.blocks.2.attn.lora_B_v.15.weight', 'image_encoder.blocks.3.attn.lora_B_k.1.weight', 'image_encoder.blocks.2.attn.lora_B_v.16.weight', 'classifier_pool.19.bias', 'image_encoder.blocks.1.attn.lora_B_v.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.11.weight', 'image_encoder.blocks.5.attn.lora_B_v.13.weight', 'image_encoder.blocks.9.attn.lora_B_k.18.weight', 'image_encoder.blocks.4.attn.lora_B_k.18.weight', 'image_encoder.blocks.6.attn.lora_B_v.18.weight', 'image_encoder.blocks.2.attn.lora_B_k.1.weight', 'image_encoder.blocks.2.attn.lora_B_k.12.weight', 'image_encoder.blocks.10.attn.lora_B_v.11.weight', 'image_encoder.blocks.11.attn.lora_B_k.10.weight', 'classifier_pool.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.11.weight', 'image_encoder.blocks.11.attn.lora_B_v.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.18.weight', 'image_encoder.blocks.1.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.14.weight', 'image_encoder.blocks.6.attn.lora_B_v.15.weight', 'classifier_pool.14.bias'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 1, Epoch 1/20 => Loss 0.436, Train_accy 86.20:   0%|          | 0/20 [00:10<?, ?it/s]
Task 1, Epoch 1/20 => Loss 0.436, Train_accy 86.20:   5%|▌         | 1/20 [00:10<03:20, 10.56s/it]
Task 1, Epoch 2/20 => Loss 0.102, Train_accy 96.66:   5%|▌         | 1/20 [00:21<03:20, 10.56s/it]
Task 1, Epoch 2/20 => Loss 0.102, Train_accy 96.66:  10%|█         | 2/20 [00:21<03:12, 10.68s/it]
Task 1, Epoch 3/20 => Loss 0.071, Train_accy 97.76:  10%|█         | 2/20 [00:31<03:12, 10.68s/it]
Task 1, Epoch 3/20 => Loss 0.071, Train_accy 97.76:  15%|█▌        | 3/20 [00:31<03:01, 10.65s/it]
Task 1, Epoch 4/20 => Loss 0.064, Train_accy 97.45:  15%|█▌        | 3/20 [00:42<03:01, 10.65s/it]
Task 1, Epoch 4/20 => Loss 0.064, Train_accy 97.45:  20%|██        | 4/20 [00:42<02:50, 10.67s/it]
Task 1, Epoch 5/20 => Loss 0.058, Train_accy 98.11:  20%|██        | 4/20 [00:53<02:50, 10.67s/it]
Task 1, Epoch 5/20 => Loss 0.058, Train_accy 98.11:  25%|██▌       | 5/20 [00:53<02:40, 10.71s/it]
Task 1, Epoch 6/20 => Loss 0.054, Train_accy 98.38:  25%|██▌       | 5/20 [01:04<02:40, 10.71s/it]
Task 1, Epoch 6/20 => Loss 0.054, Train_accy 98.38:  30%|███       | 6/20 [01:04<02:30, 10.74s/it]
Task 1, Epoch 7/20 => Loss 0.047, Train_accy 98.49:  30%|███       | 6/20 [01:14<02:30, 10.74s/it]
Task 1, Epoch 7/20 => Loss 0.047, Train_accy 98.49:  35%|███▌      | 7/20 [01:14<02:19, 10.71s/it]
Task 1, Epoch 8/20 => Loss 0.056, Train_accy 98.07:  35%|███▌      | 7/20 [01:25<02:19, 10.71s/it]
Task 1, Epoch 8/20 => Loss 0.056, Train_accy 98.07:  40%|████      | 8/20 [01:25<02:08, 10.75s/it]
Task 1, Epoch 9/20 => Loss 0.045, Train_accy 98.38:  40%|████      | 8/20 [01:36<02:08, 10.75s/it]
Task 1, Epoch 9/20 => Loss 0.045, Train_accy 98.38:  45%|████▌     | 9/20 [01:36<01:58, 10.74s/it]
Task 1, Epoch 10/20 => Loss 0.046, Train_accy 98.35:  45%|████▌     | 9/20 [01:47<01:58, 10.74s/it]
Task 1, Epoch 10/20 => Loss 0.046, Train_accy 98.35:  50%|█████     | 10/20 [01:47<01:47, 10.77s/it]
Task 1, Epoch 11/20 => Loss 0.053, Train_accy 98.07:  50%|█████     | 10/20 [01:58<01:47, 10.77s/it]
Task 1, Epoch 11/20 => Loss 0.053, Train_accy 98.07:  55%|█████▌    | 11/20 [01:58<01:37, 10.79s/it]
Task 1, Epoch 12/20 => Loss 0.054, Train_accy 98.25:  55%|█████▌    | 11/20 [02:09<01:37, 10.79s/it]
Task 1, Epoch 12/20 => Loss 0.054, Train_accy 98.25:  60%|██████    | 12/20 [02:09<01:26, 10.86s/it]
Task 1, Epoch 13/20 => Loss 0.038, Train_accy 98.49:  60%|██████    | 12/20 [02:19<01:26, 10.86s/it]
Task 1, Epoch 13/20 => Loss 0.038, Train_accy 98.49:  65%|██████▌   | 13/20 [02:19<01:15, 10.84s/it]
Task 1, Epoch 14/20 => Loss 0.033, Train_accy 98.93:  65%|██████▌   | 13/20 [02:30<01:15, 10.84s/it]
Task 1, Epoch 14/20 => Loss 0.033, Train_accy 98.93:  70%|███████   | 14/20 [02:30<01:04, 10.83s/it]
Task 1, Epoch 15/20 => Loss 0.037, Train_accy 98.73:  70%|███████   | 14/20 [02:41<01:04, 10.83s/it]
Task 1, Epoch 15/20 => Loss 0.037, Train_accy 98.73:  75%|███████▌  | 15/20 [02:41<00:54, 10.81s/it]
Task 1, Epoch 16/20 => Loss 0.034, Train_accy 98.69:  75%|███████▌  | 15/20 [02:52<00:54, 10.81s/it]
Task 1, Epoch 16/20 => Loss 0.034, Train_accy 98.69:  80%|████████  | 16/20 [02:52<00:43, 10.79s/it]
Task 1, Epoch 17/20 => Loss 0.032, Train_accy 98.93:  80%|████████  | 16/20 [03:02<00:43, 10.79s/it]
Task 1, Epoch 17/20 => Loss 0.032, Train_accy 98.93:  85%|████████▌ | 17/20 [03:02<00:32, 10.79s/it]
Task 1, Epoch 18/20 => Loss 0.036, Train_accy 98.62:  85%|████████▌ | 17/20 [03:13<00:32, 10.79s/it]
Task 1, Epoch 18/20 => Loss 0.036, Train_accy 98.62:  90%|█████████ | 18/20 [03:13<00:21, 10.80s/it]
Task 1, Epoch 19/20 => Loss 0.029, Train_accy 99.04:  90%|█████████ | 18/20 [03:24<00:21, 10.80s/it]
Task 1, Epoch 19/20 => Loss 0.029, Train_accy 99.04:  95%|█████████▌| 19/20 [03:24<00:10, 10.78s/it]
Task 1, Epoch 20/20 => Loss 0.031, Train_accy 98.97:  95%|█████████▌| 19/20 [03:35<00:10, 10.78s/it]
Task 1, Epoch 20/20 => Loss 0.031, Train_accy 98.97: 100%|██████████| 20/20 [03:35<00:00, 10.78s/it]
Task 1, Epoch 20/20 => Loss 0.031, Train_accy 98.97: 100%|██████████| 20/20 [03:35<00:00, 10.77s/it]
2024-08-12 00:37:21,773 [inflora.py] => Task 1, Epoch 20/20 => Loss 0.031, Train_accy 98.97
Threshold:  0.9516666666666667
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 10/768 type remove
Layer 3 : 13/768 type remove
Layer 4 : 14/768 type remove
Layer 5 : 21/768 type remove
Layer 6 : 20/768 type remove
Layer 7 : 19/768 type remove
Layer 8 : 21/768 type remove
Layer 9 : 29/768 type remove
Layer 10 : 30/768 type remove
Layer 11 : 9/768 type remove
Layer 12 : 15/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:37:40,249 [trainer.py] => Time:243.92165875434875
400 400
400 400
2024-08-12 00:37:42,120 [trainer.py] => Time:1.870913028717041
2024-08-12 00:37:42,120 [inflora.py] => Exemplar size: 0
2024-08-12 00:37:42,120 [trainer.py] => CNN: {'total': 91.75, '00-09': 86.0, '10-19': 97.5, 'old': 86.0, 'new': 97.5}
2024-08-12 00:37:42,120 [trainer.py] => CNN top1 curve: [99.0, 91.75]
2024-08-12 00:37:42,120 [trainer.py] => CNN top1 with task curve: [99.0, 98.5]
2024-08-12 00:37:42,120 [trainer.py] => CNN top1 task curve: [1.0, 0.9275]
2024-08-12 00:37:42,681 [trainer.py] => All params: 112239531
2024-08-12 00:37:42,686 [trainer.py] => Trainable params: 895598
2024-08-12 00:37:42,686 [inflora.py] => Learning on 20-30
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.22.weight', 'image_encoder.blocks.0.attn.lora_B_v.28.weight', 'image_encoder.blocks.1.attn.lora_B_v.21.weight', 'image_encoder.blocks.0.attn.lora_B_v.2.weight', 'image_encoder.blocks.9.attn.lora_B_k.21.weight', 'classifier_pool.21.weight', 'image_encoder.blocks.1.attn.lora_B_k.28.weight', 'image_encoder.blocks.3.attn.lora_B_v.23.weight', 'classifier_pool.21.bias', 'image_encoder.blocks.10.attn.lora_B_v.26.weight', 'image_encoder.blocks.5.attn.lora_B_v.22.weight', 'image_encoder.blocks.4.attn.lora_B_k.26.weight', 'image_encoder.blocks.8.attn.lora_B_k.27.weight', 'image_encoder.blocks.9.attn.lora_B_v.26.weight', 'image_encoder.blocks.3.attn.lora_B_k.26.weight', 'image_encoder.blocks.4.attn.lora_B_k.22.weight', 'image_encoder.blocks.3.attn.lora_B_v.25.weight', 'image_encoder.blocks.7.attn.lora_B_k.29.weight', 'image_encoder.blocks.6.attn.lora_B_v.25.weight', 'image_encoder.blocks.10.attn.lora_B_v.20.weight', 'image_encoder.blocks.5.attn.lora_B_k.26.weight', 'image_encoder.blocks.7.attn.lora_B_v.2.weight', 'image_encoder.blocks.8.attn.lora_B_v.22.weight', 'image_encoder.blocks.4.attn.lora_B_k.29.weight', 'image_encoder.blocks.2.attn.lora_B_k.25.weight', 'image_encoder.blocks.1.attn.lora_B_v.2.weight', 'image_encoder.blocks.6.attn.lora_B_k.22.weight', 'image_encoder.blocks.6.attn.lora_B_v.28.weight', 'image_encoder.blocks.0.attn.lora_B_v.20.weight', 'image_encoder.blocks.1.attn.lora_B_k.20.weight', 'image_encoder.blocks.7.attn.lora_B_v.23.weight', 'image_encoder.blocks.1.attn.lora_B_v.20.weight', 'image_encoder.blocks.3.attn.lora_B_v.20.weight', 'image_encoder.blocks.4.attn.lora_B_k.21.weight', 'image_encoder.blocks.11.attn.lora_B_v.20.weight', 'image_encoder.blocks.8.attn.lora_B_v.20.weight', 'image_encoder.blocks.10.attn.lora_B_v.29.weight', 'image_encoder.blocks.0.attn.lora_B_k.2.weight', 'image_encoder.blocks.4.attn.lora_B_v.24.weight', 'image_encoder.blocks.3.attn.lora_B_v.26.weight', 'image_encoder.blocks.5.attn.lora_B_v.28.weight', 'image_encoder.blocks.11.attn.lora_B_v.25.weight', 'image_encoder.blocks.3.attn.lora_B_k.28.weight', 'image_encoder.blocks.3.attn.lora_B_k.23.weight', 'image_encoder.blocks.7.attn.lora_B_v.21.weight', 'image_encoder.blocks.9.attn.lora_B_k.25.weight', 'image_encoder.blocks.0.attn.lora_B_k.21.weight', 'image_encoder.blocks.0.attn.lora_B_v.21.weight', 'image_encoder.blocks.9.attn.lora_B_v.27.weight', 'image_encoder.blocks.7.attn.lora_B_k.27.weight', 'image_encoder.blocks.10.attn.lora_B_k.22.weight', 'image_encoder.blocks.8.attn.lora_B_v.21.weight', 'image_encoder.blocks.3.attn.lora_B_k.20.weight', 'image_encoder.blocks.10.attn.lora_B_v.25.weight', 'image_encoder.blocks.3.attn.lora_B_v.28.weight', 'image_encoder.blocks.0.attn.lora_B_k.28.weight', 'classifier_pool.22.bias', 'image_encoder.blocks.3.attn.lora_B_k.27.weight', 'classifier_pool.27.bias', 'image_encoder.blocks.10.attn.lora_B_k.20.weight', 'image_encoder.blocks.1.attn.lora_B_k.22.weight', 'image_encoder.blocks.0.attn.lora_B_k.22.weight', 'image_encoder.blocks.6.attn.lora_B_v.23.weight', 'image_encoder.blocks.7.attn.lora_B_k.26.weight', 'image_encoder.blocks.2.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.24.weight', 'image_encoder.blocks.6.attn.lora_B_k.20.weight', 'image_encoder.blocks.6.attn.lora_B_k.27.weight', 'image_encoder.blocks.9.attn.lora_B_k.27.weight', 'image_encoder.blocks.3.attn.lora_B_k.25.weight', 'image_encoder.blocks.6.attn.lora_B_k.23.weight', 'image_encoder.blocks.8.attn.lora_B_k.29.weight', 'image_encoder.blocks.9.attn.lora_B_v.22.weight', 'image_encoder.blocks.0.attn.lora_B_v.29.weight', 'image_encoder.blocks.11.attn.lora_B_k.28.weight', 'image_encoder.blocks.11.attn.lora_B_v.27.weight', 'image_encoder.blocks.11.attn.lora_B_k.24.weight', 'image_encoder.blocks.1.attn.lora_B_v.24.weight', 'image_encoder.blocks.3.attn.lora_B_k.2.weight', 'image_encoder.blocks.5.attn.lora_B_v.20.weight', 'image_encoder.blocks.2.attn.lora_B_v.22.weight', 'image_encoder.blocks.8.attn.lora_B_v.25.weight', 'image_encoder.blocks.0.attn.lora_B_v.27.weight', 'image_encoder.blocks.8.attn.lora_B_v.2.weight', 'image_encoder.blocks.11.attn.lora_B_v.24.weight', 'image_encoder.blocks.1.attn.lora_B_k.2.weight', 'image_encoder.blocks.5.attn.lora_B_k.25.weight', 'image_encoder.blocks.4.attn.lora_B_v.23.weight', 'image_encoder.blocks.6.attn.lora_B_v.24.weight', 'image_encoder.blocks.2.attn.lora_B_v.26.weight', 'image_encoder.blocks.1.attn.lora_B_k.26.weight', 'image_encoder.blocks.1.attn.lora_B_v.22.weight', 'image_encoder.blocks.11.attn.lora_B_v.22.weight', 'image_encoder.blocks.0.attn.lora_B_k.29.weight', 'image_encoder.blocks.1.attn.lora_B_v.26.weight', 'image_encoder.blocks.5.attn.lora_B_v.23.weight', 'image_encoder.blocks.5.attn.lora_B_v.26.weight', 'image_encoder.blocks.5.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_v.27.weight', 'image_encoder.blocks.2.attn.lora_B_k.24.weight', 'image_encoder.blocks.2.attn.lora_B_v.25.weight', 'image_encoder.blocks.5.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.21.weight', 'image_encoder.blocks.7.attn.lora_B_k.25.weight', 'image_encoder.blocks.2.attn.lora_B_k.2.weight', 'image_encoder.blocks.10.attn.lora_B_v.28.weight', 'image_encoder.blocks.11.attn.lora_B_v.28.weight', 'image_encoder.blocks.8.attn.lora_B_v.24.weight', 'image_encoder.blocks.2.attn.lora_B_k.23.weight', 'image_encoder.blocks.5.attn.lora_B_k.20.weight', 'image_encoder.blocks.11.attn.lora_B_k.29.weight', 'image_encoder.blocks.0.attn.lora_B_v.26.weight', 'image_encoder.blocks.4.attn.lora_B_k.27.weight', 'image_encoder.blocks.7.attn.lora_B_v.27.weight', 'image_encoder.blocks.10.attn.lora_B_k.2.weight', 'image_encoder.blocks.7.attn.lora_B_k.28.weight', 'image_encoder.blocks.4.attn.lora_B_v.20.weight', 'classifier_pool.2.weight', 'image_encoder.blocks.9.attn.lora_B_k.28.weight', 'image_encoder.blocks.4.attn.lora_B_k.23.weight', 'image_encoder.blocks.5.attn.lora_B_k.2.weight', 'image_encoder.blocks.6.attn.lora_B_k.28.weight', 'image_encoder.blocks.7.attn.lora_B_k.23.weight', 'image_encoder.blocks.9.attn.lora_B_k.23.weight', 'image_encoder.blocks.8.attn.lora_B_k.20.weight', 'image_encoder.blocks.8.attn.lora_B_v.23.weight', 'image_encoder.blocks.0.attn.lora_B_v.24.weight', 'image_encoder.blocks.2.attn.lora_B_k.20.weight', 'image_encoder.blocks.3.attn.lora_B_v.27.weight', 'image_encoder.blocks.5.attn.lora_B_k.28.weight', 'image_encoder.blocks.6.attn.lora_B_v.21.weight', 'image_encoder.blocks.10.attn.lora_B_k.24.weight', 'image_encoder.blocks.6.attn.lora_B_v.29.weight', 'image_encoder.blocks.1.attn.lora_B_v.28.weight', 'image_encoder.blocks.3.attn.lora_B_v.24.weight', 'image_encoder.blocks.6.attn.lora_B_k.2.weight', 'image_encoder.blocks.0.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_v.23.weight', 'image_encoder.blocks.3.attn.lora_B_v.21.weight', 'image_encoder.blocks.8.attn.lora_B_k.21.weight', 'image_encoder.blocks.9.attn.lora_B_v.20.weight', 'image_encoder.blocks.10.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_k.26.weight', 'image_encoder.blocks.2.attn.lora_B_v.21.weight', 'image_encoder.blocks.4.attn.lora_B_k.2.weight', 'image_encoder.blocks.10.attn.lora_B_k.21.weight', 'classifier_pool.26.bias', 'image_encoder.blocks.8.attn.lora_B_v.26.weight', 'image_encoder.blocks.6.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_k.24.weight', 'image_encoder.blocks.4.attn.lora_B_v.26.weight', 'image_encoder.blocks.11.attn.lora_B_v.26.weight', 'image_encoder.blocks.0.attn.lora_B_k.20.weight', 'image_encoder.blocks.9.attn.lora_B_k.20.weight', 'image_encoder.blocks.9.attn.lora_B_k.26.weight', 'image_encoder.blocks.1.attn.lora_B_k.29.weight', 'image_encoder.blocks.4.attn.lora_B_k.25.weight', 'image_encoder.blocks.6.attn.lora_B_k.29.weight', 'image_encoder.blocks.6.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_k.22.weight', 'image_encoder.blocks.9.attn.lora_B_v.29.weight', 'image_encoder.blocks.9.attn.lora_B_v.2.weight', 'classifier_pool.20.weight', 'image_encoder.blocks.4.attn.lora_B_k.28.weight', 'classifier_pool.24.weight', 'image_encoder.blocks.7.attn.lora_B_v.28.weight', 'image_encoder.blocks.4.attn.lora_B_v.27.weight', 'image_encoder.blocks.5.attn.lora_B_v.27.weight', 'image_encoder.blocks.5.attn.lora_B_v.29.weight', 'image_encoder.blocks.7.attn.lora_B_v.20.weight', 'image_encoder.blocks.1.attn.lora_B_v.29.weight', 'image_encoder.blocks.11.attn.lora_B_k.23.weight', 'image_encoder.blocks.0.attn.lora_B_k.26.weight', 'image_encoder.blocks.4.attn.lora_B_v.29.weight', 'image_encoder.blocks.7.attn.lora_B_k.2.weight', 'classifier_pool.29.weight', 'image_encoder.blocks.3.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_v.25.weight', 'image_encoder.blocks.9.attn.lora_B_v.28.weight', 'image_encoder.blocks.7.attn.lora_B_k.20.weight', 'image_encoder.blocks.3.attn.lora_B_v.29.weight', 'image_encoder.blocks.7.attn.lora_B_v.24.weight', 'image_encoder.blocks.4.attn.lora_B_v.25.weight', 'image_encoder.blocks.0.attn.lora_B_v.25.weight', 'image_encoder.blocks.9.attn.lora_B_v.21.weight', 'image_encoder.blocks.10.attn.lora_B_k.29.weight', 'image_encoder.blocks.2.attn.lora_B_k.21.weight', 'image_encoder.blocks.6.attn.lora_B_v.27.weight', 'image_encoder.blocks.10.attn.lora_B_k.25.weight', 'image_encoder.blocks.7.attn.lora_B_v.22.weight', 'image_encoder.blocks.10.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_k.23.weight', 'image_encoder.blocks.10.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_k.2.weight', 'image_encoder.blocks.11.attn.lora_B_k.26.weight', 'image_encoder.blocks.5.attn.lora_B_k.24.weight', 'image_encoder.blocks.7.attn.lora_B_k.22.weight', 'image_encoder.blocks.10.attn.lora_B_v.27.weight', 'image_encoder.blocks.8.attn.lora_B_k.23.weight', 'image_encoder.blocks.5.attn.lora_B_k.29.weight', 'image_encoder.blocks.1.attn.lora_B_k.25.weight', 'image_encoder.blocks.4.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_k.21.weight', 'image_encoder.blocks.11.attn.lora_B_v.29.weight', 'classifier_pool.24.bias', 'image_encoder.blocks.2.attn.lora_B_k.29.weight', 'image_encoder.blocks.2.attn.lora_B_k.28.weight', 'classifier_pool.27.weight', 'image_encoder.blocks.4.attn.lora_B_k.24.weight', 'image_encoder.blocks.5.attn.lora_B_k.22.weight', 'classifier_pool.25.weight', 'classifier_pool.28.weight', 'image_encoder.blocks.2.attn.lora_B_v.28.weight', 'image_encoder.blocks.8.attn.lora_B_v.27.weight', 'image_encoder.blocks.11.attn.lora_B_k.25.weight', 'image_encoder.blocks.4.attn.lora_B_v.22.weight', 'image_encoder.blocks.1.attn.lora_B_k.24.weight', 'classifier_pool.26.weight', 'image_encoder.blocks.2.attn.lora_B_v.20.weight', 'image_encoder.blocks.6.attn.lora_B_v.20.weight', 'image_encoder.blocks.2.attn.lora_B_v.24.weight', 'image_encoder.blocks.4.attn.lora_B_v.21.weight', 'image_encoder.blocks.9.attn.lora_B_k.22.weight', 'image_encoder.blocks.9.attn.lora_B_v.23.weight', 'image_encoder.blocks.8.attn.lora_B_k.25.weight', 'image_encoder.blocks.10.attn.lora_B_k.27.weight', 'image_encoder.blocks.6.attn.lora_B_k.25.weight', 'image_encoder.blocks.4.attn.lora_B_v.28.weight', 'image_encoder.blocks.1.attn.lora_B_v.25.weight', 'image_encoder.blocks.2.attn.lora_B_v.27.weight', 'image_encoder.blocks.2.attn.lora_B_k.27.weight', 'image_encoder.blocks.8.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_k.26.weight', 'image_encoder.blocks.10.attn.lora_B_v.24.weight', 'classifier_pool.25.bias', 'image_encoder.blocks.11.attn.lora_B_k.22.weight', 'image_encoder.blocks.2.attn.lora_B_v.29.weight', 'image_encoder.blocks.11.attn.lora_B_k.27.weight', 'classifier_pool.22.weight', 'image_encoder.blocks.6.attn.lora_B_v.22.weight', 'image_encoder.blocks.7.attn.lora_B_v.26.weight', 'image_encoder.blocks.9.attn.lora_B_k.24.weight', 'image_encoder.blocks.2.attn.lora_B_v.23.weight', 'image_encoder.blocks.3.attn.lora_B_k.29.weight', 'classifier_pool.2.bias', 'classifier_pool.28.bias', 'classifier_pool.29.bias', 'classifier_pool.20.bias', 'image_encoder.blocks.8.attn.lora_B_k.28.weight', 'image_encoder.blocks.1.attn.lora_B_k.21.weight', 'image_encoder.blocks.11.attn.lora_B_v.21.weight', 'image_encoder.blocks.4.attn.lora_B_k.20.weight', 'image_encoder.blocks.6.attn.lora_B_k.26.weight', 'image_encoder.blocks.1.attn.lora_B_k.27.weight', 'image_encoder.blocks.8.attn.lora_B_v.28.weight', 'image_encoder.blocks.9.attn.lora_B_k.29.weight', 'image_encoder.blocks.9.attn.lora_B_v.25.weight', 'image_encoder.blocks.5.attn.lora_B_v.24.weight', 'image_encoder.blocks.7.attn.lora_B_k.21.weight', 'image_encoder.blocks.7.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_v.29.weight', 'image_encoder.blocks.10.attn.lora_B_k.28.weight', 'image_encoder.blocks.11.attn.lora_B_k.20.weight', 'image_encoder.blocks.0.attn.lora_B_k.25.weight', 'image_encoder.blocks.7.attn.lora_B_v.29.weight', 'image_encoder.blocks.11.attn.lora_B_v.23.weight', 'image_encoder.blocks.10.attn.lora_B_v.22.weight', 'image_encoder.blocks.6.attn.lora_B_v.26.weight', 'image_encoder.blocks.10.attn.lora_B_k.26.weight', 'image_encoder.blocks.11.attn.lora_B_k.21.weight', 'image_encoder.blocks.2.attn.lora_B_k.22.weight', 'image_encoder.blocks.7.attn.lora_B_v.25.weight', 'image_encoder.blocks.3.attn.lora_B_k.22.weight', 'image_encoder.blocks.6.attn.lora_B_k.21.weight', 'classifier_pool.23.weight', 'image_encoder.blocks.10.attn.lora_B_v.21.weight', 'image_encoder.blocks.0.attn.lora_B_k.27.weight', 'image_encoder.blocks.9.attn.lora_B_k.2.weight', 'image_encoder.blocks.9.attn.lora_B_v.24.weight', 'image_encoder.blocks.0.attn.lora_B_v.22.weight', 'image_encoder.blocks.5.attn.lora_B_v.21.weight', 'image_encoder.blocks.5.attn.lora_B_k.27.weight', 'classifier_pool.23.bias'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 2, Epoch 1/20 => Loss 0.618, Train_accy 78.55:   0%|          | 0/20 [00:11<?, ?it/s]
Task 2, Epoch 1/20 => Loss 0.618, Train_accy 78.55:   5%|▌         | 1/20 [00:11<03:29, 11.01s/it]
Task 2, Epoch 2/20 => Loss 0.244, Train_accy 90.55:   5%|▌         | 1/20 [00:22<03:29, 11.01s/it]
Task 2, Epoch 2/20 => Loss 0.244, Train_accy 90.55:  10%|█         | 2/20 [00:22<03:19, 11.08s/it]
Task 2, Epoch 3/20 => Loss 0.197, Train_accy 92.05:  10%|█         | 2/20 [00:33<03:19, 11.08s/it]
Task 2, Epoch 3/20 => Loss 0.197, Train_accy 92.05:  15%|█▌        | 3/20 [00:33<03:07, 11.05s/it]
Task 2, Epoch 4/20 => Loss 0.168, Train_accy 93.60:  15%|█▌        | 3/20 [00:44<03:07, 11.05s/it]
Task 2, Epoch 4/20 => Loss 0.168, Train_accy 93.60:  20%|██        | 4/20 [00:44<02:57, 11.07s/it]
Task 2, Epoch 5/20 => Loss 0.158, Train_accy 94.20:  20%|██        | 4/20 [00:55<02:57, 11.07s/it]
Task 2, Epoch 5/20 => Loss 0.158, Train_accy 94.20:  25%|██▌       | 5/20 [00:55<02:46, 11.08s/it]
Task 2, Epoch 6/20 => Loss 0.141, Train_accy 94.67:  25%|██▌       | 5/20 [01:06<02:46, 11.08s/it]
Task 2, Epoch 6/20 => Loss 0.141, Train_accy 94.67:  30%|███       | 6/20 [01:06<02:34, 11.05s/it]
Task 2, Epoch 7/20 => Loss 0.134, Train_accy 95.37:  30%|███       | 6/20 [01:17<02:34, 11.05s/it]
Task 2, Epoch 7/20 => Loss 0.134, Train_accy 95.37:  35%|███▌      | 7/20 [01:17<02:23, 11.07s/it]
Task 2, Epoch 8/20 => Loss 0.134, Train_accy 95.34:  35%|███▌      | 7/20 [01:28<02:23, 11.07s/it]
Task 2, Epoch 8/20 => Loss 0.134, Train_accy 95.34:  40%|████      | 8/20 [01:28<02:13, 11.11s/it]
Task 2, Epoch 9/20 => Loss 0.114, Train_accy 95.64:  40%|████      | 8/20 [01:39<02:13, 11.11s/it]
Task 2, Epoch 9/20 => Loss 0.114, Train_accy 95.64:  45%|████▌     | 9/20 [01:39<02:01, 11.08s/it]
Task 2, Epoch 10/20 => Loss 0.114, Train_accy 95.98:  45%|████▌     | 9/20 [01:50<02:01, 11.08s/it]
Task 2, Epoch 10/20 => Loss 0.114, Train_accy 95.98:  50%|█████     | 10/20 [01:50<01:51, 11.11s/it]
Task 2, Epoch 11/20 => Loss 0.109, Train_accy 96.31:  50%|█████     | 10/20 [02:01<01:51, 11.11s/it]
Task 2, Epoch 11/20 => Loss 0.109, Train_accy 96.31:  55%|█████▌    | 11/20 [02:01<01:39, 11.09s/it]
Task 2, Epoch 12/20 => Loss 0.103, Train_accy 96.41:  55%|█████▌    | 11/20 [02:13<01:39, 11.09s/it]
Task 2, Epoch 12/20 => Loss 0.103, Train_accy 96.41:  60%|██████    | 12/20 [02:13<01:28, 11.11s/it]
Task 2, Epoch 13/20 => Loss 0.102, Train_accy 96.41:  60%|██████    | 12/20 [02:24<01:28, 11.11s/it]
Task 2, Epoch 13/20 => Loss 0.102, Train_accy 96.41:  65%|██████▌   | 13/20 [02:24<01:17, 11.14s/it]
Task 2, Epoch 14/20 => Loss 0.097, Train_accy 96.51:  65%|██████▌   | 13/20 [02:35<01:17, 11.14s/it]
Task 2, Epoch 14/20 => Loss 0.097, Train_accy 96.51:  70%|███████   | 14/20 [02:35<01:06, 11.15s/it]
Task 2, Epoch 15/20 => Loss 0.089, Train_accy 96.75:  70%|███████   | 14/20 [02:46<01:06, 11.15s/it]
Task 2, Epoch 15/20 => Loss 0.089, Train_accy 96.75:  75%|███████▌  | 15/20 [02:46<00:55, 11.16s/it]
Task 2, Epoch 16/20 => Loss 0.085, Train_accy 96.92:  75%|███████▌  | 15/20 [02:57<00:55, 11.16s/it]
Task 2, Epoch 16/20 => Loss 0.085, Train_accy 96.92:  80%|████████  | 16/20 [02:57<00:44, 11.17s/it]
Task 2, Epoch 17/20 => Loss 0.079, Train_accy 96.92:  80%|████████  | 16/20 [03:08<00:44, 11.17s/it]
Task 2, Epoch 17/20 => Loss 0.079, Train_accy 96.92:  85%|████████▌ | 17/20 [03:08<00:33, 11.13s/it]
Task 2, Epoch 18/20 => Loss 0.081, Train_accy 97.49:  85%|████████▌ | 17/20 [03:19<00:33, 11.13s/it]
Task 2, Epoch 18/20 => Loss 0.081, Train_accy 97.49:  90%|█████████ | 18/20 [03:19<00:22, 11.13s/it]
Task 2, Epoch 19/20 => Loss 0.079, Train_accy 97.55:  90%|█████████ | 18/20 [03:31<00:22, 11.13s/it]
Task 2, Epoch 19/20 => Loss 0.079, Train_accy 97.55:  95%|█████████▌| 19/20 [03:31<00:11, 11.10s/it]
Task 2, Epoch 20/20 => Loss 0.087, Train_accy 97.18:  95%|█████████▌| 19/20 [03:42<00:11, 11.10s/it]
Task 2, Epoch 20/20 => Loss 0.087, Train_accy 97.18: 100%|██████████| 20/20 [03:42<00:00, 11.08s/it]
Task 2, Epoch 20/20 => Loss 0.087, Train_accy 97.18: 100%|██████████| 20/20 [03:42<00:00, 11.10s/it]
2024-08-12 00:41:39,918 [inflora.py] => Task 2, Epoch 20/20 => Loss 0.087, Train_accy 97.18
Threshold:  0.9533333333333333
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 10/768 type remove
Layer 3 : 15/768 type remove
Layer 4 : 15/768 type remove
Layer 5 : 24/768 type remove
Layer 6 : 23/768 type remove
Layer 7 : 22/768 type remove
Layer 8 : 24/768 type remove
Layer 9 : 35/768 type remove
Layer 10 : 38/768 type remove
Layer 11 : 13/768 type remove
Layer 12 : 20/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:41:59,353 [trainer.py] => Time:256.66662526130676
600 600
600 600
2024-08-12 00:42:01,626 [trainer.py] => Time:2.273141622543335
2024-08-12 00:42:01,626 [inflora.py] => Exemplar size: 0
2024-08-12 00:42:01,627 [trainer.py] => CNN: {'total': 86.83, '00-09': 90.0, '10-19': 94.0, '20-29': 76.5, 'old': 92.0, 'new': 76.5}
2024-08-12 00:42:01,627 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83]
2024-08-12 00:42:01,627 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67]
2024-08-12 00:42:01,627 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333]
2024-08-12 00:42:02,149 [trainer.py] => All params: 112239531
2024-08-12 00:42:02,154 [trainer.py] => Trainable params: 895598
2024-08-12 00:42:02,154 [inflora.py] => Learning on 30-40
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_v.3.weight', 'image_encoder.blocks.2.attn.lora_B_k.3.weight', 'image_encoder.blocks.7.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.3.weight', 'image_encoder.blocks.6.attn.lora_B_k.3.weight', 'image_encoder.blocks.11.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.3.weight', 'image_encoder.blocks.10.attn.lora_B_k.3.weight', 'image_encoder.blocks.2.attn.lora_B_v.3.weight', 'image_encoder.blocks.7.attn.lora_B_v.3.weight', 'image_encoder.blocks.11.attn.lora_B_k.3.weight', 'image_encoder.blocks.5.attn.lora_B_k.3.weight', 'classifier_pool.3.bias', 'image_encoder.blocks.4.attn.lora_B_v.3.weight', 'classifier_pool.3.weight', 'image_encoder.blocks.1.attn.lora_B_k.3.weight', 'image_encoder.blocks.6.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_v.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_v.3.weight', 'image_encoder.blocks.1.attn.lora_B_v.3.weight', 'image_encoder.blocks.3.attn.lora_B_v.3.weight', 'image_encoder.blocks.5.attn.lora_B_v.3.weight', 'image_encoder.blocks.3.attn.lora_B_k.3.weight', 'image_encoder.blocks.8.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_k.3.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 3, Epoch 1/20 => Loss 0.527, Train_accy 82.60:   0%|          | 0/20 [00:11<?, ?it/s]
Task 3, Epoch 1/20 => Loss 0.527, Train_accy 82.60:   5%|▌         | 1/20 [00:11<03:30, 11.10s/it]
Task 3, Epoch 2/20 => Loss 0.152, Train_accy 94.31:   5%|▌         | 1/20 [00:22<03:30, 11.10s/it]
Task 3, Epoch 2/20 => Loss 0.152, Train_accy 94.31:  10%|█         | 2/20 [00:22<03:19, 11.06s/it]
Task 3, Epoch 3/20 => Loss 0.120, Train_accy 95.79:  10%|█         | 2/20 [00:33<03:19, 11.06s/it]
Task 3, Epoch 3/20 => Loss 0.120, Train_accy 95.79:  15%|█▌        | 3/20 [00:33<03:08, 11.06s/it]
Task 3, Epoch 4/20 => Loss 0.125, Train_accy 95.39:  15%|█▌        | 3/20 [00:44<03:08, 11.06s/it]
Task 3, Epoch 4/20 => Loss 0.125, Train_accy 95.39:  20%|██        | 4/20 [00:44<02:56, 11.06s/it]
Task 3, Epoch 5/20 => Loss 0.102, Train_accy 96.40:  20%|██        | 4/20 [00:55<02:56, 11.06s/it]
Task 3, Epoch 5/20 => Loss 0.102, Train_accy 96.40:  25%|██▌       | 5/20 [00:55<02:45, 11.05s/it]
Task 3, Epoch 6/20 => Loss 0.092, Train_accy 96.67:  25%|██▌       | 5/20 [01:06<02:45, 11.05s/it]
Task 3, Epoch 6/20 => Loss 0.092, Train_accy 96.67:  30%|███       | 6/20 [01:06<02:35, 11.08s/it]
Task 3, Epoch 7/20 => Loss 0.079, Train_accy 97.27:  30%|███       | 6/20 [01:17<02:35, 11.08s/it]
Task 3, Epoch 7/20 => Loss 0.079, Train_accy 97.27:  35%|███▌      | 7/20 [01:17<02:24, 11.08s/it]
Task 3, Epoch 8/20 => Loss 0.084, Train_accy 97.24:  35%|███▌      | 7/20 [01:28<02:24, 11.08s/it]
Task 3, Epoch 8/20 => Loss 0.084, Train_accy 97.24:  40%|████      | 8/20 [01:28<02:12, 11.07s/it]
Task 3, Epoch 9/20 => Loss 0.080, Train_accy 97.14:  40%|████      | 8/20 [01:39<02:12, 11.07s/it]
Task 3, Epoch 9/20 => Loss 0.080, Train_accy 97.14:  45%|████▌     | 9/20 [01:39<02:02, 11.11s/it]
Task 3, Epoch 10/20 => Loss 0.072, Train_accy 97.31:  45%|████▌     | 9/20 [01:50<02:02, 11.11s/it]
Task 3, Epoch 10/20 => Loss 0.072, Train_accy 97.31:  50%|█████     | 10/20 [01:50<01:50, 11.09s/it]
Task 3, Epoch 11/20 => Loss 0.065, Train_accy 97.71:  50%|█████     | 10/20 [02:02<01:50, 11.09s/it]
Task 3, Epoch 11/20 => Loss 0.065, Train_accy 97.71:  55%|█████▌    | 11/20 [02:02<01:40, 11.14s/it]
Task 3, Epoch 12/20 => Loss 0.062, Train_accy 97.88:  55%|█████▌    | 11/20 [02:13<01:40, 11.14s/it]
Task 3, Epoch 12/20 => Loss 0.062, Train_accy 97.88:  60%|██████    | 12/20 [02:13<01:29, 11.16s/it]
Task 3, Epoch 13/20 => Loss 0.072, Train_accy 97.51:  60%|██████    | 12/20 [02:24<01:29, 11.16s/it]
Task 3, Epoch 13/20 => Loss 0.072, Train_accy 97.51:  65%|██████▌   | 13/20 [02:24<01:17, 11.13s/it]
Task 3, Epoch 14/20 => Loss 0.068, Train_accy 97.51:  65%|██████▌   | 13/20 [02:35<01:17, 11.13s/it]
Task 3, Epoch 14/20 => Loss 0.068, Train_accy 97.51:  70%|███████   | 14/20 [02:35<01:06, 11.15s/it]
Task 3, Epoch 15/20 => Loss 0.076, Train_accy 97.41:  70%|███████   | 14/20 [02:46<01:06, 11.15s/it]
Task 3, Epoch 15/20 => Loss 0.076, Train_accy 97.41:  75%|███████▌  | 15/20 [02:46<00:55, 11.15s/it]
Task 3, Epoch 16/20 => Loss 0.067, Train_accy 97.58:  75%|███████▌  | 15/20 [02:57<00:55, 11.15s/it]
Task 3, Epoch 16/20 => Loss 0.067, Train_accy 97.58:  80%|████████  | 16/20 [02:57<00:44, 11.16s/it]
Task 3, Epoch 17/20 => Loss 0.063, Train_accy 97.81:  80%|████████  | 16/20 [03:09<00:44, 11.16s/it]
Task 3, Epoch 17/20 => Loss 0.063, Train_accy 97.81:  85%|████████▌ | 17/20 [03:09<00:33, 11.17s/it]
Task 3, Epoch 18/20 => Loss 0.080, Train_accy 97.27:  85%|████████▌ | 17/20 [03:20<00:33, 11.17s/it]
Task 3, Epoch 18/20 => Loss 0.080, Train_accy 97.27:  90%|█████████ | 18/20 [03:20<00:22, 11.15s/it]
Task 3, Epoch 19/20 => Loss 0.060, Train_accy 98.01:  90%|█████████ | 18/20 [03:31<00:22, 11.15s/it]
Task 3, Epoch 19/20 => Loss 0.060, Train_accy 98.01:  95%|█████████▌| 19/20 [03:31<00:11, 11.16s/it]
Task 3, Epoch 20/20 => Loss 0.058, Train_accy 97.81:  95%|█████████▌| 19/20 [03:42<00:11, 11.16s/it]
Task 3, Epoch 20/20 => Loss 0.058, Train_accy 97.81: 100%|██████████| 20/20 [03:42<00:00, 11.14s/it]
Task 3, Epoch 20/20 => Loss 0.058, Train_accy 97.81: 100%|██████████| 20/20 [03:42<00:00, 11.12s/it]
2024-08-12 00:45:55,449 [inflora.py] => Task 3, Epoch 20/20 => Loss 0.058, Train_accy 97.81
Threshold:  0.955
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 11/768 type remove
Layer 3 : 18/768 type remove
Layer 4 : 18/768 type remove
Layer 5 : 28/768 type remove
Layer 6 : 28/768 type remove
Layer 7 : 28/768 type remove
Layer 8 : 33/768 type remove
Layer 9 : 42/768 type remove
Layer 10 : 45/768 type remove
Layer 11 : 15/768 type remove
Layer 12 : 24/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:46:14,700 [trainer.py] => Time:252.54561710357666
800 800
800 800
2024-08-12 00:46:17,287 [trainer.py] => Time:2.5870771408081055
2024-08-12 00:46:17,287 [inflora.py] => Exemplar size: 0
2024-08-12 00:46:17,287 [trainer.py] => CNN: {'total': 86.62, '00-09': 86.5, '10-19': 92.5, '20-29': 73.5, '30-39': 94.0, 'old': 84.17, 'new': 94.0}
2024-08-12 00:46:17,287 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62]
2024-08-12 00:46:17,287 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75]
2024-08-12 00:46:17,287 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87]
2024-08-12 00:46:17,962 [trainer.py] => All params: 112239531
2024-08-12 00:46:17,966 [trainer.py] => Trainable params: 81418
2024-08-12 00:46:17,966 [inflora.py] => Learning on 40-50
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_v.4.weight', 'image_encoder.blocks.0.attn.lora_B_k.4.weight', 'image_encoder.blocks.1.attn.lora_B_v.4.weight', 'image_encoder.blocks.5.attn.lora_B_k.4.weight', 'image_encoder.blocks.11.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_v.4.weight', 'image_encoder.blocks.8.attn.lora_B_k.4.weight', 'image_encoder.blocks.0.attn.lora_B_v.4.weight', 'image_encoder.blocks.11.attn.lora_B_k.4.weight', 'image_encoder.blocks.7.attn.lora_B_k.4.weight', 'image_encoder.blocks.8.attn.lora_B_v.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.4.weight', 'image_encoder.blocks.4.attn.lora_B_v.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.4.weight', 'classifier_pool.4.weight', 'image_encoder.blocks.6.attn.lora_B_v.4.weight', 'image_encoder.blocks.6.attn.lora_B_k.4.weight', 'classifier_pool.4.bias', 'image_encoder.blocks.3.attn.lora_B_k.4.weight', 'image_encoder.blocks.3.attn.lora_B_v.4.weight', 'image_encoder.blocks.1.attn.lora_B_k.4.weight', 'image_encoder.blocks.2.attn.lora_B_v.4.weight', 'image_encoder.blocks.5.attn.lora_B_v.4.weight', 'image_encoder.blocks.2.attn.lora_B_k.4.weight', 'image_encoder.blocks.4.attn.lora_B_k.4.weight', 'image_encoder.blocks.10.attn.lora_B_k.4.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 4, Epoch 1/20 => Loss 0.591, Train_accy 83.12:   0%|          | 0/20 [00:10<?, ?it/s]
Task 4, Epoch 1/20 => Loss 0.591, Train_accy 83.12:   5%|▌         | 1/20 [00:10<03:27, 10.91s/it]
Task 4, Epoch 2/20 => Loss 0.116, Train_accy 95.80:   5%|▌         | 1/20 [00:21<03:27, 10.91s/it]
Task 4, Epoch 2/20 => Loss 0.116, Train_accy 95.80:  10%|█         | 2/20 [00:21<03:17, 11.00s/it]
Task 4, Epoch 3/20 => Loss 0.108, Train_accy 96.44:  10%|█         | 2/20 [00:33<03:17, 11.00s/it]
Task 4, Epoch 3/20 => Loss 0.108, Train_accy 96.44:  15%|█▌        | 3/20 [00:33<03:08, 11.08s/it]
Task 4, Epoch 4/20 => Loss 0.085, Train_accy 97.02:  15%|█▌        | 3/20 [00:44<03:08, 11.08s/it]
Task 4, Epoch 4/20 => Loss 0.085, Train_accy 97.02:  20%|██        | 4/20 [00:44<02:56, 11.04s/it]
Task 4, Epoch 5/20 => Loss 0.073, Train_accy 97.56:  20%|██        | 4/20 [00:55<02:56, 11.04s/it]
Task 4, Epoch 5/20 => Loss 0.073, Train_accy 97.56:  25%|██▌       | 5/20 [00:55<02:45, 11.01s/it]
Task 4, Epoch 6/20 => Loss 0.071, Train_accy 97.63:  25%|██▌       | 5/20 [01:06<02:45, 11.01s/it]
Task 4, Epoch 6/20 => Loss 0.071, Train_accy 97.63:  30%|███       | 6/20 [01:06<02:34, 11.06s/it]
Task 4, Epoch 7/20 => Loss 0.058, Train_accy 97.90:  30%|███       | 6/20 [01:17<02:34, 11.06s/it]
Task 4, Epoch 7/20 => Loss 0.058, Train_accy 97.90:  35%|███▌      | 7/20 [01:17<02:23, 11.03s/it]
Task 4, Epoch 8/20 => Loss 0.069, Train_accy 97.53:  35%|███▌      | 7/20 [01:28<02:23, 11.03s/it]
Task 4, Epoch 8/20 => Loss 0.069, Train_accy 97.53:  40%|████      | 8/20 [01:28<02:13, 11.09s/it]
Task 4, Epoch 9/20 => Loss 0.064, Train_accy 97.53:  40%|████      | 8/20 [01:39<02:13, 11.09s/it]
Task 4, Epoch 9/20 => Loss 0.064, Train_accy 97.53:  45%|████▌     | 9/20 [01:39<02:01, 11.08s/it]
Task 4, Epoch 10/20 => Loss 0.060, Train_accy 97.76:  45%|████▌     | 9/20 [01:50<02:01, 11.08s/it]
Task 4, Epoch 10/20 => Loss 0.060, Train_accy 97.76:  50%|█████     | 10/20 [01:50<01:50, 11.10s/it]
Task 4, Epoch 11/20 => Loss 0.046, Train_accy 98.37:  50%|█████     | 10/20 [02:01<01:50, 11.10s/it]
Task 4, Epoch 11/20 => Loss 0.046, Train_accy 98.37:  55%|█████▌    | 11/20 [02:01<01:39, 11.08s/it]
Task 4, Epoch 12/20 => Loss 0.047, Train_accy 98.27:  55%|█████▌    | 11/20 [02:12<01:39, 11.08s/it]
Task 4, Epoch 12/20 => Loss 0.047, Train_accy 98.27:  60%|██████    | 12/20 [02:12<01:28, 11.09s/it]
Task 4, Epoch 13/20 => Loss 0.050, Train_accy 98.27:  60%|██████    | 12/20 [02:23<01:28, 11.09s/it]
Task 4, Epoch 13/20 => Loss 0.050, Train_accy 98.27:  65%|██████▌   | 13/20 [02:23<01:17, 11.08s/it]
Task 4, Epoch 14/20 => Loss 0.043, Train_accy 98.54:  65%|██████▌   | 13/20 [02:34<01:17, 11.08s/it]
Task 4, Epoch 14/20 => Loss 0.043, Train_accy 98.54:  70%|███████   | 14/20 [02:34<01:06, 11.05s/it]
Task 4, Epoch 15/20 => Loss 0.050, Train_accy 98.27:  70%|███████   | 14/20 [02:45<01:06, 11.05s/it]
Task 4, Epoch 15/20 => Loss 0.050, Train_accy 98.27:  75%|███████▌  | 15/20 [02:45<00:55, 11.02s/it]
Task 4, Epoch 16/20 => Loss 0.048, Train_accy 98.41:  75%|███████▌  | 15/20 [02:56<00:55, 11.02s/it]
Task 4, Epoch 16/20 => Loss 0.048, Train_accy 98.41:  80%|████████  | 16/20 [02:56<00:44, 11.05s/it]
Task 4, Epoch 17/20 => Loss 0.050, Train_accy 98.20:  80%|████████  | 16/20 [03:07<00:44, 11.05s/it]
Task 4, Epoch 17/20 => Loss 0.050, Train_accy 98.20:  85%|████████▌ | 17/20 [03:07<00:33, 11.02s/it]
Task 4, Epoch 18/20 => Loss 0.037, Train_accy 98.58:  85%|████████▌ | 17/20 [03:18<00:33, 11.02s/it]
Task 4, Epoch 18/20 => Loss 0.037, Train_accy 98.58:  90%|█████████ | 18/20 [03:18<00:22, 11.01s/it]
Task 4, Epoch 19/20 => Loss 0.046, Train_accy 98.44:  90%|█████████ | 18/20 [03:29<00:22, 11.01s/it]
Task 4, Epoch 19/20 => Loss 0.046, Train_accy 98.44:  95%|█████████▌| 19/20 [03:29<00:10, 10.98s/it]
Task 4, Epoch 20/20 => Loss 0.044, Train_accy 98.27:  95%|█████████▌| 19/20 [03:40<00:10, 10.98s/it]
Task 4, Epoch 20/20 => Loss 0.044, Train_accy 98.27: 100%|██████████| 20/20 [03:40<00:00, 10.99s/it]
Task 4, Epoch 20/20 => Loss 0.044, Train_accy 98.27: 100%|██████████| 20/20 [03:40<00:00, 11.04s/it]
2024-08-12 00:50:09,549 [inflora.py] => Task 4, Epoch 20/20 => Loss 0.044, Train_accy 98.27
Threshold:  0.9566666666666667
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 11/768 type remove
Layer 3 : 19/768 type remove
Layer 4 : 19/768 type remove
Layer 5 : 29/768 type remove
Layer 6 : 29/768 type remove
Layer 7 : 29/768 type remove
Layer 8 : 34/768 type remove
Layer 9 : 43/768 type remove
Layer 10 : 49/768 type remove
Layer 11 : 17/768 type remove
Layer 12 : 30/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:50:29,728 [trainer.py] => Time:251.76149129867554
1000 1000
1000 1000
2024-08-12 00:50:32,767 [trainer.py] => Time:3.039025068283081
2024-08-12 00:50:32,767 [inflora.py] => Exemplar size: 0
2024-08-12 00:50:32,767 [trainer.py] => CNN: {'total': 81.5, '00-09': 84.5, '10-19': 85.5, '20-29': 70.0, '30-39': 84.5, '40-49': 83.0, 'old': 81.12, 'new': 83.0}
2024-08-12 00:50:32,767 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5]
2024-08-12 00:50:32,767 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8]
2024-08-12 00:50:32,767 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819]
2024-08-12 00:50:33,298 [trainer.py] => All params: 112239531
2024-08-12 00:50:33,302 [trainer.py] => Trainable params: 81418
2024-08-12 00:50:33,302 [inflora.py] => Learning on 50-60
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_k.5.weight', 'image_encoder.blocks.6.attn.lora_B_k.5.weight', 'image_encoder.blocks.7.attn.lora_B_k.5.weight', 'image_encoder.blocks.1.attn.lora_B_v.5.weight', 'image_encoder.blocks.0.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_v.5.weight', 'image_encoder.blocks.10.attn.lora_B_k.5.weight', 'image_encoder.blocks.10.attn.lora_B_v.5.weight', 'image_encoder.blocks.8.attn.lora_B_k.5.weight', 'image_encoder.blocks.0.attn.lora_B_k.5.weight', 'image_encoder.blocks.9.attn.lora_B_v.5.weight', 'classifier_pool.5.weight', 'image_encoder.blocks.11.attn.lora_B_k.5.weight', 'classifier_pool.5.bias', 'image_encoder.blocks.4.attn.lora_B_k.5.weight', 'image_encoder.blocks.2.attn.lora_B_v.5.weight', 'image_encoder.blocks.11.attn.lora_B_v.5.weight', 'image_encoder.blocks.2.attn.lora_B_k.5.weight', 'image_encoder.blocks.1.attn.lora_B_k.5.weight', 'image_encoder.blocks.6.attn.lora_B_v.5.weight', 'image_encoder.blocks.3.attn.lora_B_k.5.weight', 'image_encoder.blocks.3.attn.lora_B_v.5.weight', 'image_encoder.blocks.4.attn.lora_B_v.5.weight', 'image_encoder.blocks.5.attn.lora_B_k.5.weight', 'image_encoder.blocks.7.attn.lora_B_v.5.weight', 'image_encoder.blocks.5.attn.lora_B_v.5.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 5, Epoch 1/20 => Loss 0.570, Train_accy 82.71:   0%|          | 0/20 [00:11<?, ?it/s]
Task 5, Epoch 1/20 => Loss 0.570, Train_accy 82.71:   5%|▌         | 1/20 [00:11<03:34, 11.31s/it]
Task 5, Epoch 2/20 => Loss 0.141, Train_accy 94.82:   5%|▌         | 1/20 [00:22<03:34, 11.31s/it]
Task 5, Epoch 2/20 => Loss 0.141, Train_accy 94.82:  10%|█         | 2/20 [00:22<03:25, 11.40s/it]
Task 5, Epoch 3/20 => Loss 0.109, Train_accy 96.69:  10%|█         | 2/20 [00:34<03:25, 11.40s/it]
Task 5, Epoch 3/20 => Loss 0.109, Train_accy 96.69:  15%|█▌        | 3/20 [00:34<03:14, 11.42s/it]
Task 5, Epoch 4/20 => Loss 0.106, Train_accy 96.59:  15%|█▌        | 3/20 [00:45<03:14, 11.42s/it]
Task 5, Epoch 4/20 => Loss 0.106, Train_accy 96.59:  20%|██        | 4/20 [00:45<03:02, 11.41s/it]
Task 5, Epoch 5/20 => Loss 0.083, Train_accy 97.35:  20%|██        | 4/20 [00:57<03:02, 11.41s/it]
Task 5, Epoch 5/20 => Loss 0.083, Train_accy 97.35:  25%|██▌       | 5/20 [00:57<02:52, 11.49s/it]
Task 5, Epoch 6/20 => Loss 0.089, Train_accy 97.18:  25%|██▌       | 5/20 [01:08<02:52, 11.49s/it]
Task 5, Epoch 6/20 => Loss 0.089, Train_accy 97.18:  30%|███       | 6/20 [01:08<02:40, 11.47s/it]
Task 5, Epoch 7/20 => Loss 0.076, Train_accy 97.45:  30%|███       | 6/20 [01:20<02:40, 11.47s/it]
Task 5, Epoch 7/20 => Loss 0.076, Train_accy 97.45:  35%|███▌      | 7/20 [01:20<02:29, 11.47s/it]
Task 5, Epoch 8/20 => Loss 0.078, Train_accy 97.45:  35%|███▌      | 7/20 [01:31<02:29, 11.47s/it]
Task 5, Epoch 8/20 => Loss 0.078, Train_accy 97.45:  40%|████      | 8/20 [01:31<02:17, 11.49s/it]
Task 5, Epoch 9/20 => Loss 0.072, Train_accy 97.54:  40%|████      | 8/20 [01:43<02:17, 11.49s/it]
Task 5, Epoch 9/20 => Loss 0.072, Train_accy 97.54:  45%|████▌     | 9/20 [01:43<02:06, 11.49s/it]
Task 5, Epoch 10/20 => Loss 0.072, Train_accy 97.51:  45%|████▌     | 9/20 [01:54<02:06, 11.49s/it]
Task 5, Epoch 10/20 => Loss 0.072, Train_accy 97.51:  50%|█████     | 10/20 [01:54<01:54, 11.42s/it]
Task 5, Epoch 11/20 => Loss 0.066, Train_accy 97.38:  50%|█████     | 10/20 [02:05<01:54, 11.42s/it]
Task 5, Epoch 11/20 => Loss 0.066, Train_accy 97.38:  55%|█████▌    | 11/20 [02:05<01:42, 11.41s/it]
Task 5, Epoch 12/20 => Loss 0.072, Train_accy 97.48:  55%|█████▌    | 11/20 [02:17<01:42, 11.41s/it]
Task 5, Epoch 12/20 => Loss 0.072, Train_accy 97.48:  60%|██████    | 12/20 [02:17<01:31, 11.40s/it]
Task 5, Epoch 13/20 => Loss 0.064, Train_accy 97.84:  60%|██████    | 12/20 [02:28<01:31, 11.40s/it]
Task 5, Epoch 13/20 => Loss 0.064, Train_accy 97.84:  65%|██████▌   | 13/20 [02:28<01:19, 11.41s/it]
Task 5, Epoch 14/20 => Loss 0.066, Train_accy 97.48:  65%|██████▌   | 13/20 [02:39<01:19, 11.41s/it]
Task 5, Epoch 14/20 => Loss 0.066, Train_accy 97.48:  70%|███████   | 14/20 [02:39<01:08, 11.38s/it]
Task 5, Epoch 15/20 => Loss 0.056, Train_accy 98.10:  70%|███████   | 14/20 [02:51<01:08, 11.38s/it]
Task 5, Epoch 15/20 => Loss 0.056, Train_accy 98.10:  75%|███████▌  | 15/20 [02:51<00:57, 11.41s/it]
Task 5, Epoch 16/20 => Loss 0.050, Train_accy 98.33:  75%|███████▌  | 15/20 [03:02<00:57, 11.41s/it]
Task 5, Epoch 16/20 => Loss 0.050, Train_accy 98.33:  80%|████████  | 16/20 [03:02<00:45, 11.43s/it]
Task 5, Epoch 17/20 => Loss 0.057, Train_accy 98.07:  80%|████████  | 16/20 [03:14<00:45, 11.43s/it]
Task 5, Epoch 17/20 => Loss 0.057, Train_accy 98.07:  85%|████████▌ | 17/20 [03:14<00:34, 11.44s/it]
Task 5, Epoch 18/20 => Loss 0.050, Train_accy 98.43:  85%|████████▌ | 17/20 [03:25<00:34, 11.44s/it]
Task 5, Epoch 18/20 => Loss 0.050, Train_accy 98.43:  90%|█████████ | 18/20 [03:25<00:22, 11.45s/it]
Task 5, Epoch 19/20 => Loss 0.054, Train_accy 98.33:  90%|█████████ | 18/20 [03:37<00:22, 11.45s/it]
Task 5, Epoch 19/20 => Loss 0.054, Train_accy 98.33:  95%|█████████▌| 19/20 [03:37<00:11, 11.43s/it]
Task 5, Epoch 20/20 => Loss 0.055, Train_accy 98.07:  95%|█████████▌| 19/20 [03:48<00:11, 11.43s/it]
Task 5, Epoch 20/20 => Loss 0.055, Train_accy 98.07: 100%|██████████| 20/20 [03:48<00:00, 11.45s/it]
Task 5, Epoch 20/20 => Loss 0.055, Train_accy 98.07: 100%|██████████| 20/20 [03:48<00:00, 11.44s/it]
2024-08-12 00:54:33,715 [inflora.py] => Task 5, Epoch 20/20 => Loss 0.055, Train_accy 98.07
Threshold:  0.9583333333333333
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 11/768 type remove
Layer 3 : 20/768 type remove
Layer 4 : 21/768 type remove
Layer 5 : 32/768 type remove
Layer 6 : 33/768 type remove
Layer 7 : 33/768 type remove
Layer 8 : 36/768 type remove
Layer 9 : 47/768 type remove
Layer 10 : 54/768 type remove
Layer 11 : 20/768 type remove
Layer 12 : 36/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:54:53,927 [trainer.py] => Time:260.62486243247986
1199 1199
1199 1199
2024-08-12 00:54:57,221 [trainer.py] => Time:3.2938718795776367
2024-08-12 00:54:57,221 [inflora.py] => Exemplar size: 0
2024-08-12 00:54:57,221 [trainer.py] => CNN: {'total': 81.32, '00-09': 81.5, '10-19': 87.0, '20-29': 70.5, '30-39': 78.0, '40-49': 82.0, '50-59': 88.94, 'old': 79.8, 'new': 88.94}
2024-08-12 00:54:57,222 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32]
2024-08-12 00:54:57,222 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17]
2024-08-12 00:54:57,222 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258]
2024-08-12 00:54:57,754 [trainer.py] => All params: 112239531
2024-08-12 00:54:57,758 [trainer.py] => Trainable params: 81418
2024-08-12 00:54:57,758 [inflora.py] => Learning on 60-70
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_v.6.weight', 'image_encoder.blocks.7.attn.lora_B_v.6.weight', 'image_encoder.blocks.6.attn.lora_B_k.6.weight', 'image_encoder.blocks.8.attn.lora_B_k.6.weight', 'image_encoder.blocks.8.attn.lora_B_v.6.weight', 'image_encoder.blocks.11.attn.lora_B_v.6.weight', 'image_encoder.blocks.5.attn.lora_B_k.6.weight', 'image_encoder.blocks.2.attn.lora_B_v.6.weight', 'image_encoder.blocks.10.attn.lora_B_v.6.weight', 'image_encoder.blocks.11.attn.lora_B_k.6.weight', 'image_encoder.blocks.5.attn.lora_B_v.6.weight', 'image_encoder.blocks.9.attn.lora_B_v.6.weight', 'image_encoder.blocks.0.attn.lora_B_k.6.weight', 'image_encoder.blocks.6.attn.lora_B_v.6.weight', 'image_encoder.blocks.2.attn.lora_B_k.6.weight', 'image_encoder.blocks.1.attn.lora_B_k.6.weight', 'image_encoder.blocks.4.attn.lora_B_v.6.weight', 'image_encoder.blocks.3.attn.lora_B_k.6.weight', 'image_encoder.blocks.10.attn.lora_B_k.6.weight', 'classifier_pool.6.bias', 'classifier_pool.6.weight', 'image_encoder.blocks.0.attn.lora_B_v.6.weight', 'image_encoder.blocks.7.attn.lora_B_k.6.weight', 'image_encoder.blocks.4.attn.lora_B_k.6.weight', 'image_encoder.blocks.9.attn.lora_B_k.6.weight', 'image_encoder.blocks.3.attn.lora_B_v.6.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 6, Epoch 1/20 => Loss 0.506, Train_accy 84.99:   0%|          | 0/20 [00:11<?, ?it/s]
Task 6, Epoch 1/20 => Loss 0.506, Train_accy 84.99:   5%|▌         | 1/20 [00:11<03:32, 11.18s/it]
Task 6, Epoch 2/20 => Loss 0.103, Train_accy 96.59:   5%|▌         | 1/20 [00:22<03:32, 11.18s/it]
Task 6, Epoch 2/20 => Loss 0.103, Train_accy 96.59:  10%|█         | 2/20 [00:22<03:21, 11.21s/it]
Task 6, Epoch 3/20 => Loss 0.072, Train_accy 97.68:  10%|█         | 2/20 [00:33<03:21, 11.21s/it]
Task 6, Epoch 3/20 => Loss 0.072, Train_accy 97.68:  15%|█▌        | 3/20 [00:33<03:10, 11.23s/it]
Task 6, Epoch 4/20 => Loss 0.073, Train_accy 97.78:  15%|█▌        | 3/20 [00:45<03:10, 11.23s/it]
Task 6, Epoch 4/20 => Loss 0.073, Train_accy 97.78:  20%|██        | 4/20 [00:45<03:00, 11.27s/it]
Task 6, Epoch 5/20 => Loss 0.059, Train_accy 98.01:  20%|██        | 4/20 [00:56<03:00, 11.27s/it]
Task 6, Epoch 5/20 => Loss 0.059, Train_accy 98.01:  25%|██▌       | 5/20 [00:56<02:49, 11.27s/it]
Task 6, Epoch 6/20 => Loss 0.059, Train_accy 97.91:  25%|██▌       | 5/20 [01:07<02:49, 11.27s/it]
Task 6, Epoch 6/20 => Loss 0.059, Train_accy 97.91:  30%|███       | 6/20 [01:07<02:37, 11.27s/it]
Task 6, Epoch 7/20 => Loss 0.050, Train_accy 98.05:  30%|███       | 6/20 [01:18<02:37, 11.27s/it]
Task 6, Epoch 7/20 => Loss 0.050, Train_accy 98.05:  35%|███▌      | 7/20 [01:18<02:26, 11.29s/it]
Task 6, Epoch 8/20 => Loss 0.053, Train_accy 98.28:  35%|███▌      | 7/20 [01:30<02:26, 11.29s/it]
Task 6, Epoch 8/20 => Loss 0.053, Train_accy 98.28:  40%|████      | 8/20 [01:30<02:15, 11.30s/it]
Task 6, Epoch 9/20 => Loss 0.057, Train_accy 97.91:  40%|████      | 8/20 [01:41<02:15, 11.30s/it]
Task 6, Epoch 9/20 => Loss 0.057, Train_accy 97.91:  45%|████▌     | 9/20 [01:41<02:04, 11.29s/it]
Task 6, Epoch 10/20 => Loss 0.049, Train_accy 98.24:  45%|████▌     | 9/20 [01:52<02:04, 11.29s/it]
Task 6, Epoch 10/20 => Loss 0.049, Train_accy 98.24:  50%|█████     | 10/20 [01:52<01:52, 11.28s/it]
Task 6, Epoch 11/20 => Loss 0.059, Train_accy 97.95:  50%|█████     | 10/20 [02:04<01:52, 11.28s/it]
Task 6, Epoch 11/20 => Loss 0.059, Train_accy 97.95:  55%|█████▌    | 11/20 [02:04<01:41, 11.32s/it]
Task 6, Epoch 12/20 => Loss 0.045, Train_accy 98.38:  55%|█████▌    | 11/20 [02:15<01:41, 11.32s/it]
Task 6, Epoch 12/20 => Loss 0.045, Train_accy 98.38:  60%|██████    | 12/20 [02:15<01:30, 11.29s/it]
Task 6, Epoch 13/20 => Loss 0.047, Train_accy 98.34:  60%|██████    | 12/20 [02:26<01:30, 11.29s/it]
Task 6, Epoch 13/20 => Loss 0.047, Train_accy 98.34:  65%|██████▌   | 13/20 [02:26<01:19, 11.33s/it]
Task 6, Epoch 14/20 => Loss 0.042, Train_accy 98.38:  65%|██████▌   | 13/20 [02:38<01:19, 11.33s/it]
Task 6, Epoch 14/20 => Loss 0.042, Train_accy 98.38:  70%|███████   | 14/20 [02:38<01:07, 11.33s/it]
Task 6, Epoch 15/20 => Loss 0.042, Train_accy 98.51:  70%|███████   | 14/20 [02:49<01:07, 11.33s/it]
Task 6, Epoch 15/20 => Loss 0.042, Train_accy 98.51:  75%|███████▌  | 15/20 [02:49<00:56, 11.35s/it]
Task 6, Epoch 16/20 => Loss 0.037, Train_accy 98.64:  75%|███████▌  | 15/20 [03:00<00:56, 11.35s/it]
Task 6, Epoch 16/20 => Loss 0.037, Train_accy 98.64:  80%|████████  | 16/20 [03:00<00:45, 11.33s/it]
Task 6, Epoch 17/20 => Loss 0.035, Train_accy 98.94:  80%|████████  | 16/20 [03:12<00:45, 11.33s/it]
Task 6, Epoch 17/20 => Loss 0.035, Train_accy 98.94:  85%|████████▌ | 17/20 [03:12<00:33, 11.32s/it]
Task 6, Epoch 18/20 => Loss 0.041, Train_accy 98.58:  85%|████████▌ | 17/20 [03:23<00:33, 11.32s/it]
Task 6, Epoch 18/20 => Loss 0.041, Train_accy 98.58:  90%|█████████ | 18/20 [03:23<00:22, 11.36s/it]
Task 6, Epoch 19/20 => Loss 0.033, Train_accy 98.91:  90%|█████████ | 18/20 [03:34<00:22, 11.36s/it]
Task 6, Epoch 19/20 => Loss 0.033, Train_accy 98.91:  95%|█████████▌| 19/20 [03:34<00:11, 11.37s/it]
Task 6, Epoch 20/20 => Loss 0.041, Train_accy 98.58:  95%|█████████▌| 19/20 [03:46<00:11, 11.37s/it]
Task 6, Epoch 20/20 => Loss 0.041, Train_accy 98.58: 100%|██████████| 20/20 [03:46<00:00, 11.37s/it]
Task 6, Epoch 20/20 => Loss 0.041, Train_accy 98.58: 100%|██████████| 20/20 [03:46<00:00, 11.32s/it]
2024-08-12 00:58:55,145 [inflora.py] => Task 6, Epoch 20/20 => Loss 0.041, Train_accy 98.58
Threshold:  0.96
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 22/768 type remove
Layer 4 : 23/768 type remove
Layer 5 : 36/768 type remove
Layer 6 : 38/768 type remove
Layer 7 : 37/768 type remove
Layer 8 : 41/768 type remove
Layer 9 : 52/768 type remove
Layer 10 : 60/768 type remove
Layer 11 : 22/768 type remove
Layer 12 : 43/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 00:59:14,844 [trainer.py] => Time:257.08569955825806
1399 1399
1399 1399
2024-08-12 00:59:18,440 [trainer.py] => Time:3.594986915588379
2024-08-12 00:59:18,440 [inflora.py] => Exemplar size: 0
2024-08-12 00:59:18,440 [trainer.py] => CNN: {'total': 80.2, '00-09': 80.0, '10-19': 85.0, '20-29': 70.0, '30-39': 73.5, '40-49': 80.5, '50-59': 84.92, '60-69': 87.5, 'old': 78.98, 'new': 87.5}
2024-08-12 00:59:18,440 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2]
2024-08-12 00:59:18,440 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21]
2024-08-12 00:59:18,440 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442]
2024-08-12 00:59:18,969 [trainer.py] => All params: 112239531
2024-08-12 00:59:18,973 [trainer.py] => Trainable params: 81418
2024-08-12 00:59:18,974 [inflora.py] => Learning on 70-80
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.7.weight', 'image_encoder.blocks.10.attn.lora_B_v.7.weight', 'image_encoder.blocks.9.attn.lora_B_v.7.weight', 'classifier_pool.7.bias', 'image_encoder.blocks.6.attn.lora_B_v.7.weight', 'image_encoder.blocks.1.attn.lora_B_v.7.weight', 'image_encoder.blocks.2.attn.lora_B_k.7.weight', 'image_encoder.blocks.3.attn.lora_B_v.7.weight', 'classifier_pool.7.weight', 'image_encoder.blocks.0.attn.lora_B_v.7.weight', 'image_encoder.blocks.11.attn.lora_B_v.7.weight', 'image_encoder.blocks.7.attn.lora_B_k.7.weight', 'image_encoder.blocks.0.attn.lora_B_k.7.weight', 'image_encoder.blocks.8.attn.lora_B_k.7.weight', 'image_encoder.blocks.2.attn.lora_B_v.7.weight', 'image_encoder.blocks.4.attn.lora_B_v.7.weight', 'image_encoder.blocks.11.attn.lora_B_k.7.weight', 'image_encoder.blocks.4.attn.lora_B_k.7.weight', 'image_encoder.blocks.8.attn.lora_B_v.7.weight', 'image_encoder.blocks.9.attn.lora_B_k.7.weight', 'image_encoder.blocks.6.attn.lora_B_k.7.weight', 'image_encoder.blocks.3.attn.lora_B_k.7.weight', 'image_encoder.blocks.7.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_k.7.weight', 'image_encoder.blocks.5.attn.lora_B_v.7.weight', 'image_encoder.blocks.1.attn.lora_B_k.7.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 7, Epoch 1/20 => Loss 0.527, Train_accy 83.89:   0%|          | 0/20 [00:10<?, ?it/s]
Task 7, Epoch 1/20 => Loss 0.527, Train_accy 83.89:   5%|▌         | 1/20 [00:10<03:27, 10.92s/it]
Task 7, Epoch 2/20 => Loss 0.190, Train_accy 92.97:   5%|▌         | 1/20 [00:21<03:27, 10.92s/it]
Task 7, Epoch 2/20 => Loss 0.190, Train_accy 92.97:  10%|█         | 2/20 [00:21<03:15, 10.86s/it]
Task 7, Epoch 3/20 => Loss 0.137, Train_accy 95.81:  10%|█         | 2/20 [00:32<03:15, 10.86s/it]
Task 7, Epoch 3/20 => Loss 0.137, Train_accy 95.81:  15%|█▌        | 3/20 [00:32<03:04, 10.87s/it]
Task 7, Epoch 4/20 => Loss 0.122, Train_accy 95.81:  15%|█▌        | 3/20 [00:43<03:04, 10.87s/it]
Task 7, Epoch 4/20 => Loss 0.122, Train_accy 95.81:  20%|██        | 4/20 [00:43<02:53, 10.87s/it]
Task 7, Epoch 5/20 => Loss 0.122, Train_accy 95.81:  20%|██        | 4/20 [00:54<02:53, 10.87s/it]
Task 7, Epoch 5/20 => Loss 0.122, Train_accy 95.81:  25%|██▌       | 5/20 [00:54<02:43, 10.87s/it]
Task 7, Epoch 6/20 => Loss 0.106, Train_accy 96.60:  25%|██▌       | 5/20 [01:05<02:43, 10.87s/it]
Task 7, Epoch 6/20 => Loss 0.106, Train_accy 96.60:  30%|███       | 6/20 [01:05<02:32, 10.92s/it]
Task 7, Epoch 7/20 => Loss 0.106, Train_accy 96.29:  30%|███       | 6/20 [01:16<02:32, 10.92s/it]
Task 7, Epoch 7/20 => Loss 0.106, Train_accy 96.29:  35%|███▌      | 7/20 [01:16<02:22, 10.94s/it]
Task 7, Epoch 8/20 => Loss 0.103, Train_accy 96.26:  35%|███▌      | 7/20 [01:27<02:22, 10.94s/it]
Task 7, Epoch 8/20 => Loss 0.103, Train_accy 96.26:  40%|████      | 8/20 [01:27<02:11, 10.96s/it]
Task 7, Epoch 9/20 => Loss 0.080, Train_accy 96.81:  40%|████      | 8/20 [01:38<02:11, 10.96s/it]
Task 7, Epoch 9/20 => Loss 0.080, Train_accy 96.81:  45%|████▌     | 9/20 [01:38<02:00, 10.91s/it]
Task 7, Epoch 10/20 => Loss 0.087, Train_accy 96.78:  45%|████▌     | 9/20 [01:49<02:00, 10.91s/it]
Task 7, Epoch 10/20 => Loss 0.087, Train_accy 96.78:  50%|█████     | 10/20 [01:49<01:48, 10.89s/it]
Task 7, Epoch 11/20 => Loss 0.086, Train_accy 96.92:  50%|█████     | 10/20 [01:59<01:48, 10.89s/it]
Task 7, Epoch 11/20 => Loss 0.086, Train_accy 96.92:  55%|█████▌    | 11/20 [01:59<01:37, 10.89s/it]
Task 7, Epoch 12/20 => Loss 0.085, Train_accy 96.99:  55%|█████▌    | 11/20 [02:10<01:37, 10.89s/it]
Task 7, Epoch 12/20 => Loss 0.085, Train_accy 96.99:  60%|██████    | 12/20 [02:10<01:27, 10.89s/it]
Task 7, Epoch 13/20 => Loss 0.085, Train_accy 96.92:  60%|██████    | 12/20 [02:21<01:27, 10.89s/it]
Task 7, Epoch 13/20 => Loss 0.085, Train_accy 96.92:  65%|██████▌   | 13/20 [02:21<01:16, 10.88s/it]
Task 7, Epoch 14/20 => Loss 0.075, Train_accy 97.23:  65%|██████▌   | 13/20 [02:32<01:16, 10.88s/it]
Task 7, Epoch 14/20 => Loss 0.075, Train_accy 97.23:  70%|███████   | 14/20 [02:32<01:05, 10.91s/it]
Task 7, Epoch 15/20 => Loss 0.065, Train_accy 97.82:  70%|███████   | 14/20 [02:43<01:05, 10.91s/it]
Task 7, Epoch 15/20 => Loss 0.065, Train_accy 97.82:  75%|███████▌  | 15/20 [02:43<00:54, 10.88s/it]
Task 7, Epoch 16/20 => Loss 0.064, Train_accy 97.85:  75%|███████▌  | 15/20 [02:54<00:54, 10.88s/it]
Task 7, Epoch 16/20 => Loss 0.064, Train_accy 97.85:  80%|████████  | 16/20 [02:54<00:43, 10.88s/it]
Task 7, Epoch 17/20 => Loss 0.068, Train_accy 97.54:  80%|████████  | 16/20 [03:05<00:43, 10.88s/it]
Task 7, Epoch 17/20 => Loss 0.068, Train_accy 97.54:  85%|████████▌ | 17/20 [03:05<00:32, 10.91s/it]
Task 7, Epoch 18/20 => Loss 0.062, Train_accy 97.75:  85%|████████▌ | 17/20 [03:16<00:32, 10.91s/it]
Task 7, Epoch 18/20 => Loss 0.062, Train_accy 97.75:  90%|█████████ | 18/20 [03:16<00:21, 10.87s/it]
Task 7, Epoch 19/20 => Loss 0.068, Train_accy 97.26:  90%|█████████ | 18/20 [03:26<00:21, 10.87s/it]
Task 7, Epoch 19/20 => Loss 0.068, Train_accy 97.26:  95%|█████████▌| 19/20 [03:26<00:10, 10.88s/it]
Task 7, Epoch 20/20 => Loss 0.058, Train_accy 98.20:  95%|█████████▌| 19/20 [03:37<00:10, 10.88s/it]
Task 7, Epoch 20/20 => Loss 0.058, Train_accy 98.20: 100%|██████████| 20/20 [03:37<00:00, 10.89s/it]
Task 7, Epoch 20/20 => Loss 0.058, Train_accy 98.20: 100%|██████████| 20/20 [03:37<00:00, 10.89s/it]
2024-08-12 01:03:08,307 [inflora.py] => Task 7, Epoch 20/20 => Loss 0.058, Train_accy 98.20
Threshold:  0.9616666666666667
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 25/768 type remove
Layer 4 : 26/768 type remove
Layer 5 : 42/768 type remove
Layer 6 : 45/768 type remove
Layer 7 : 43/768 type remove
Layer 8 : 49/768 type remove
Layer 9 : 65/768 type remove
Layer 10 : 72/768 type remove
Layer 11 : 28/768 type remove
Layer 12 : 48/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 01:03:27,047 [trainer.py] => Time:248.07340931892395
1599 1599
1599 1599
2024-08-12 01:03:31,181 [trainer.py] => Time:4.132906913757324
2024-08-12 01:03:31,181 [inflora.py] => Exemplar size: 0
2024-08-12 01:03:31,181 [trainer.py] => CNN: {'total': 80.74, '00-09': 79.5, '10-19': 85.5, '20-29': 72.0, '30-39': 73.0, '40-49': 80.0, '50-59': 86.43, '60-69': 86.0, '70-79': 83.5, 'old': 80.34, 'new': 83.5}
2024-08-12 01:03:31,181 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74]
2024-08-12 01:03:31,181 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25]
2024-08-12 01:03:31,181 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477]
2024-08-12 01:03:31,728 [trainer.py] => All params: 112239531
2024-08-12 01:03:31,732 [trainer.py] => Trainable params: 81418
2024-08-12 01:03:31,732 [inflora.py] => Learning on 80-90
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_v.8.weight', 'image_encoder.blocks.9.attn.lora_B_k.8.weight', 'image_encoder.blocks.4.attn.lora_B_v.8.weight', 'image_encoder.blocks.4.attn.lora_B_k.8.weight', 'image_encoder.blocks.11.attn.lora_B_v.8.weight', 'image_encoder.blocks.2.attn.lora_B_v.8.weight', 'classifier_pool.8.bias', 'image_encoder.blocks.3.attn.lora_B_v.8.weight', 'image_encoder.blocks.7.attn.lora_B_k.8.weight', 'classifier_pool.8.weight', 'image_encoder.blocks.10.attn.lora_B_k.8.weight', 'image_encoder.blocks.8.attn.lora_B_k.8.weight', 'image_encoder.blocks.9.attn.lora_B_v.8.weight', 'image_encoder.blocks.2.attn.lora_B_k.8.weight', 'image_encoder.blocks.7.attn.lora_B_v.8.weight', 'image_encoder.blocks.10.attn.lora_B_v.8.weight', 'image_encoder.blocks.1.attn.lora_B_k.8.weight', 'image_encoder.blocks.6.attn.lora_B_k.8.weight', 'image_encoder.blocks.1.attn.lora_B_v.8.weight', 'image_encoder.blocks.8.attn.lora_B_v.8.weight', 'image_encoder.blocks.5.attn.lora_B_v.8.weight', 'image_encoder.blocks.11.attn.lora_B_k.8.weight', 'image_encoder.blocks.0.attn.lora_B_k.8.weight', 'image_encoder.blocks.5.attn.lora_B_k.8.weight', 'image_encoder.blocks.6.attn.lora_B_v.8.weight', 'image_encoder.blocks.3.attn.lora_B_k.8.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 8, Epoch 1/20 => Loss 0.614, Train_accy 79.68:   0%|          | 0/20 [00:10<?, ?it/s]
Task 8, Epoch 1/20 => Loss 0.614, Train_accy 79.68:   5%|▌         | 1/20 [00:10<03:20, 10.57s/it]
Task 8, Epoch 2/20 => Loss 0.197, Train_accy 91.88:   5%|▌         | 1/20 [00:21<03:20, 10.57s/it]
Task 8, Epoch 2/20 => Loss 0.197, Train_accy 91.88:  10%|█         | 2/20 [00:21<03:14, 10.79s/it]
Task 8, Epoch 3/20 => Loss 0.158, Train_accy 93.74:  10%|█         | 2/20 [00:32<03:14, 10.79s/it]
Task 8, Epoch 3/20 => Loss 0.158, Train_accy 93.74:  15%|█▌        | 3/20 [00:32<03:03, 10.79s/it]
Task 8, Epoch 4/20 => Loss 0.139, Train_accy 94.80:  15%|█▌        | 3/20 [00:43<03:03, 10.79s/it]
Task 8, Epoch 4/20 => Loss 0.139, Train_accy 94.80:  20%|██        | 4/20 [00:43<02:52, 10.78s/it]
Task 8, Epoch 5/20 => Loss 0.145, Train_accy 94.55:  20%|██        | 4/20 [00:53<02:52, 10.78s/it]
Task 8, Epoch 5/20 => Loss 0.145, Train_accy 94.55:  25%|██▌       | 5/20 [00:53<02:42, 10.80s/it]
Task 8, Epoch 6/20 => Loss 0.130, Train_accy 95.04:  25%|██▌       | 5/20 [01:04<02:42, 10.80s/it]
Task 8, Epoch 6/20 => Loss 0.130, Train_accy 95.04:  30%|███       | 6/20 [01:04<02:30, 10.77s/it]
Task 8, Epoch 7/20 => Loss 0.114, Train_accy 95.78:  30%|███       | 6/20 [01:15<02:30, 10.77s/it]
Task 8, Epoch 7/20 => Loss 0.114, Train_accy 95.78:  35%|███▌      | 7/20 [01:15<02:20, 10.82s/it]
Task 8, Epoch 8/20 => Loss 0.107, Train_accy 95.47:  35%|███▌      | 7/20 [01:26<02:20, 10.82s/it]
Task 8, Epoch 8/20 => Loss 0.107, Train_accy 95.47:  40%|████      | 8/20 [01:26<02:09, 10.79s/it]
Task 8, Epoch 9/20 => Loss 0.087, Train_accy 96.49:  40%|████      | 8/20 [01:37<02:09, 10.79s/it]
Task 8, Epoch 9/20 => Loss 0.087, Train_accy 96.49:  45%|████▌     | 9/20 [01:37<01:58, 10.79s/it]
Task 8, Epoch 10/20 => Loss 0.085, Train_accy 96.77:  45%|████▌     | 9/20 [01:47<01:58, 10.79s/it]
Task 8, Epoch 10/20 => Loss 0.085, Train_accy 96.77:  50%|█████     | 10/20 [01:47<01:48, 10.82s/it]
Task 8, Epoch 11/20 => Loss 0.105, Train_accy 95.85:  50%|█████     | 10/20 [01:58<01:48, 10.82s/it]
Task 8, Epoch 11/20 => Loss 0.105, Train_accy 95.85:  55%|█████▌    | 11/20 [01:58<01:37, 10.78s/it]
Task 8, Epoch 12/20 => Loss 0.082, Train_accy 96.98:  55%|█████▌    | 11/20 [02:09<01:37, 10.78s/it]
Task 8, Epoch 12/20 => Loss 0.082, Train_accy 96.98:  60%|██████    | 12/20 [02:09<01:26, 10.81s/it]
Task 8, Epoch 13/20 => Loss 0.080, Train_accy 96.59:  60%|██████    | 12/20 [02:20<01:26, 10.81s/it]
Task 8, Epoch 13/20 => Loss 0.080, Train_accy 96.59:  65%|██████▌   | 13/20 [02:20<01:15, 10.82s/it]
Task 8, Epoch 14/20 => Loss 0.081, Train_accy 97.19:  65%|██████▌   | 13/20 [02:31<01:15, 10.82s/it]
Task 8, Epoch 14/20 => Loss 0.081, Train_accy 97.19:  70%|███████   | 14/20 [02:31<01:04, 10.82s/it]
Task 8, Epoch 15/20 => Loss 0.078, Train_accy 96.98:  70%|███████   | 14/20 [02:42<01:04, 10.82s/it]
Task 8, Epoch 15/20 => Loss 0.078, Train_accy 96.98:  75%|███████▌  | 15/20 [02:42<00:54, 10.83s/it]
Task 8, Epoch 16/20 => Loss 0.077, Train_accy 97.47:  75%|███████▌  | 15/20 [02:52<00:54, 10.83s/it]
Task 8, Epoch 16/20 => Loss 0.077, Train_accy 97.47:  80%|████████  | 16/20 [02:52<00:43, 10.85s/it]
Task 8, Epoch 17/20 => Loss 0.067, Train_accy 97.19:  80%|████████  | 16/20 [03:03<00:43, 10.85s/it]
Task 8, Epoch 17/20 => Loss 0.067, Train_accy 97.19:  85%|████████▌ | 17/20 [03:03<00:32, 10.82s/it]
Task 8, Epoch 18/20 => Loss 0.073, Train_accy 97.29:  85%|████████▌ | 17/20 [03:14<00:32, 10.82s/it]
Task 8, Epoch 18/20 => Loss 0.073, Train_accy 97.29:  90%|█████████ | 18/20 [03:14<00:21, 10.81s/it]
Task 8, Epoch 19/20 => Loss 0.071, Train_accy 97.43:  90%|█████████ | 18/20 [03:25<00:21, 10.81s/it]
Task 8, Epoch 19/20 => Loss 0.071, Train_accy 97.43:  95%|█████████▌| 19/20 [03:25<00:10, 10.79s/it]
Task 8, Epoch 20/20 => Loss 0.071, Train_accy 97.36:  95%|█████████▌| 19/20 [03:36<00:10, 10.79s/it]
Task 8, Epoch 20/20 => Loss 0.071, Train_accy 97.36: 100%|██████████| 20/20 [03:36<00:00, 10.81s/it]
Task 8, Epoch 20/20 => Loss 0.071, Train_accy 97.36: 100%|██████████| 20/20 [03:36<00:00, 10.80s/it]
2024-08-12 01:07:19,234 [inflora.py] => Task 8, Epoch 20/20 => Loss 0.071, Train_accy 97.36
Threshold:  0.9633333333333333
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 26/768 type remove
Layer 4 : 28/768 type remove
Layer 5 : 44/768 type remove
Layer 6 : 47/768 type remove
Layer 7 : 46/768 type remove
Layer 8 : 52/768 type remove
Layer 9 : 69/768 type remove
Layer 10 : 81/768 type remove
Layer 11 : 35/768 type remove
Layer 12 : 54/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 01:07:38,677 [trainer.py] => Time:246.94527077674866
1798 1798
1798 1798
2024-08-12 01:07:43,085 [trainer.py] => Time:4.407105207443237
2024-08-12 01:07:43,085 [inflora.py] => Exemplar size: 0
2024-08-12 01:07:43,085 [trainer.py] => CNN: {'total': 78.7, '00-09': 80.5, '10-19': 81.0, '20-29': 69.5, '30-39': 72.0, '40-49': 80.0, '50-59': 85.93, '60-69': 85.0, '70-79': 82.0, '80-89': 72.36, 'old': 79.49, 'new': 72.36}
2024-08-12 01:07:43,085 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7]
2024-08-12 01:07:43,085 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33]
2024-08-12 01:07:43,085 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408]
2024-08-12 01:07:43,637 [trainer.py] => All params: 112239531
2024-08-12 01:07:43,641 [trainer.py] => Trainable params: 81418
2024-08-12 01:07:43,641 [inflora.py] => Learning on 90-100
Parameters to be updated: {'image_encoder.blocks.10.attn.lora_B_k.9.weight', 'image_encoder.blocks.5.attn.lora_B_k.9.weight', 'image_encoder.blocks.7.attn.lora_B_k.9.weight', 'image_encoder.blocks.10.attn.lora_B_v.9.weight', 'image_encoder.blocks.6.attn.lora_B_v.9.weight', 'image_encoder.blocks.6.attn.lora_B_k.9.weight', 'image_encoder.blocks.9.attn.lora_B_k.9.weight', 'image_encoder.blocks.4.attn.lora_B_v.9.weight', 'image_encoder.blocks.1.attn.lora_B_v.9.weight', 'image_encoder.blocks.5.attn.lora_B_v.9.weight', 'image_encoder.blocks.8.attn.lora_B_v.9.weight', 'classifier_pool.9.weight', 'classifier_pool.9.bias', 'image_encoder.blocks.9.attn.lora_B_v.9.weight', 'image_encoder.blocks.8.attn.lora_B_k.9.weight', 'image_encoder.blocks.11.attn.lora_B_v.9.weight', 'image_encoder.blocks.0.attn.lora_B_v.9.weight', 'image_encoder.blocks.3.attn.lora_B_v.9.weight', 'image_encoder.blocks.7.attn.lora_B_v.9.weight', 'image_encoder.blocks.11.attn.lora_B_k.9.weight', 'image_encoder.blocks.0.attn.lora_B_k.9.weight', 'image_encoder.blocks.4.attn.lora_B_k.9.weight', 'image_encoder.blocks.1.attn.lora_B_k.9.weight', 'image_encoder.blocks.2.attn.lora_B_k.9.weight', 'image_encoder.blocks.2.attn.lora_B_v.9.weight', 'image_encoder.blocks.3.attn.lora_B_k.9.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 9, Epoch 1/20 => Loss 0.393, Train_accy 88.04:   0%|          | 0/20 [00:11<?, ?it/s]
Task 9, Epoch 1/20 => Loss 0.393, Train_accy 88.04:   5%|▌         | 1/20 [00:11<03:35, 11.34s/it]
Task 9, Epoch 2/20 => Loss 0.089, Train_accy 97.34:   5%|▌         | 1/20 [00:22<03:35, 11.34s/it]
Task 9, Epoch 2/20 => Loss 0.089, Train_accy 97.34:  10%|█         | 2/20 [00:22<03:23, 11.32s/it]
Task 9, Epoch 3/20 => Loss 0.073, Train_accy 97.64:  10%|█         | 2/20 [00:34<03:23, 11.32s/it]
Task 9, Epoch 3/20 => Loss 0.073, Train_accy 97.64:  15%|█▌        | 3/20 [00:34<03:12, 11.35s/it]
Task 9, Epoch 4/20 => Loss 0.069, Train_accy 97.84:  15%|█▌        | 3/20 [00:45<03:12, 11.35s/it]
Task 9, Epoch 4/20 => Loss 0.069, Train_accy 97.84:  20%|██        | 4/20 [00:45<03:01, 11.32s/it]
Task 9, Epoch 5/20 => Loss 0.063, Train_accy 97.94:  20%|██        | 4/20 [00:56<03:01, 11.32s/it]
Task 9, Epoch 5/20 => Loss 0.063, Train_accy 97.94:  25%|██▌       | 5/20 [00:56<02:50, 11.35s/it]
Task 9, Epoch 6/20 => Loss 0.058, Train_accy 98.14:  25%|██▌       | 5/20 [01:08<02:50, 11.35s/it]
Task 9, Epoch 6/20 => Loss 0.058, Train_accy 98.14:  30%|███       | 6/20 [01:08<02:38, 11.35s/it]
Task 9, Epoch 7/20 => Loss 0.052, Train_accy 98.41:  30%|███       | 6/20 [01:19<02:38, 11.35s/it]
Task 9, Epoch 7/20 => Loss 0.052, Train_accy 98.41:  35%|███▌      | 7/20 [01:19<02:27, 11.36s/it]
Task 9, Epoch 8/20 => Loss 0.051, Train_accy 98.11:  35%|███▌      | 7/20 [01:30<02:27, 11.36s/it]
Task 9, Epoch 8/20 => Loss 0.051, Train_accy 98.11:  40%|████      | 8/20 [01:30<02:16, 11.34s/it]
Task 9, Epoch 9/20 => Loss 0.047, Train_accy 98.47:  40%|████      | 8/20 [01:42<02:16, 11.34s/it]
Task 9, Epoch 9/20 => Loss 0.047, Train_accy 98.47:  45%|████▌     | 9/20 [01:42<02:04, 11.33s/it]
Task 9, Epoch 10/20 => Loss 0.054, Train_accy 98.24:  45%|████▌     | 9/20 [01:53<02:04, 11.33s/it]
Task 9, Epoch 10/20 => Loss 0.054, Train_accy 98.24:  50%|█████     | 10/20 [01:53<01:53, 11.32s/it]
Task 9, Epoch 11/20 => Loss 0.050, Train_accy 98.34:  50%|█████     | 10/20 [02:04<01:53, 11.32s/it]
Task 9, Epoch 11/20 => Loss 0.050, Train_accy 98.34:  55%|█████▌    | 11/20 [02:04<01:42, 11.34s/it]
Task 9, Epoch 12/20 => Loss 0.053, Train_accy 98.17:  55%|█████▌    | 11/20 [02:16<01:42, 11.34s/it]
Task 9, Epoch 12/20 => Loss 0.053, Train_accy 98.17:  60%|██████    | 12/20 [02:16<01:30, 11.36s/it]
Task 9, Epoch 13/20 => Loss 0.041, Train_accy 98.51:  60%|██████    | 12/20 [02:27<01:30, 11.36s/it]
Task 9, Epoch 13/20 => Loss 0.041, Train_accy 98.51:  65%|██████▌   | 13/20 [02:27<01:19, 11.38s/it]
Task 9, Epoch 14/20 => Loss 0.036, Train_accy 98.77:  65%|██████▌   | 13/20 [02:38<01:19, 11.38s/it]
Task 9, Epoch 14/20 => Loss 0.036, Train_accy 98.77:  70%|███████   | 14/20 [02:38<01:08, 11.35s/it]
Task 9, Epoch 15/20 => Loss 0.046, Train_accy 98.37:  70%|███████   | 14/20 [02:50<01:08, 11.35s/it]
Task 9, Epoch 15/20 => Loss 0.046, Train_accy 98.37:  75%|███████▌  | 15/20 [02:50<00:56, 11.34s/it]
Task 9, Epoch 16/20 => Loss 0.032, Train_accy 98.87:  75%|███████▌  | 15/20 [03:01<00:56, 11.34s/it]
Task 9, Epoch 16/20 => Loss 0.032, Train_accy 98.87:  80%|████████  | 16/20 [03:01<00:45, 11.34s/it]
Task 9, Epoch 17/20 => Loss 0.043, Train_accy 98.64:  80%|████████  | 16/20 [03:12<00:45, 11.34s/it]
Task 9, Epoch 17/20 => Loss 0.043, Train_accy 98.64:  85%|████████▌ | 17/20 [03:12<00:33, 11.33s/it]
Task 9, Epoch 18/20 => Loss 0.040, Train_accy 98.57:  85%|████████▌ | 17/20 [03:24<00:33, 11.33s/it]
Task 9, Epoch 18/20 => Loss 0.040, Train_accy 98.57:  90%|█████████ | 18/20 [03:24<00:22, 11.38s/it]
Task 9, Epoch 19/20 => Loss 0.039, Train_accy 98.77:  90%|█████████ | 18/20 [03:35<00:22, 11.38s/it]
Task 9, Epoch 19/20 => Loss 0.039, Train_accy 98.77:  95%|█████████▌| 19/20 [03:35<00:11, 11.35s/it]
Task 9, Epoch 20/20 => Loss 0.031, Train_accy 98.87:  95%|█████████▌| 19/20 [03:46<00:11, 11.35s/it]
Task 9, Epoch 20/20 => Loss 0.031, Train_accy 98.87: 100%|██████████| 20/20 [03:46<00:00, 11.34s/it]
Task 9, Epoch 20/20 => Loss 0.031, Train_accy 98.87: 100%|██████████| 20/20 [03:46<00:00, 11.35s/it]
2024-08-12 01:11:43,357 [inflora.py] => Task 9, Epoch 20/20 => Loss 0.031, Train_accy 98.87
Threshold:  0.965
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 28/768 type remove
Layer 4 : 30/768 type remove
Layer 5 : 46/768 type remove
Layer 6 : 50/768 type remove
Layer 7 : 49/768 type remove
Layer 8 : 56/768 type remove
Layer 9 : 73/768 type remove
Layer 10 : 87/768 type remove
Layer 11 : 38/768 type remove
Layer 12 : 59/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 01:12:03,078 [trainer.py] => Time:259.4361708164215
1997 1997
1997 1997
2024-08-12 01:12:07,707 [trainer.py] => Time:4.628757476806641
2024-08-12 01:12:07,707 [inflora.py] => Exemplar size: 0
2024-08-12 01:12:07,707 [trainer.py] => CNN: {'total': 76.87, '00-09': 81.0, '10-19': 80.0, '20-29': 66.5, '30-39': 72.5, '40-49': 79.5, '50-59': 85.43, '60-69': 85.0, '70-79': 80.5, '80-89': 70.85, '90-99': 67.34, 'old': 77.92, 'new': 67.34}
2024-08-12 01:12:07,707 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87]
2024-08-12 01:12:07,707 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15]
2024-08-12 01:12:07,707 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739]
2024-08-12 01:12:08,450 [trainer.py] => All params: 112239531
2024-08-12 01:12:08,455 [trainer.py] => Trainable params: 81418
2024-08-12 01:12:08,455 [inflora.py] => Learning on 100-110
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_v.10.weight', 'classifier_pool.10.bias', 'image_encoder.blocks.1.attn.lora_B_k.10.weight', 'image_encoder.blocks.10.attn.lora_B_v.10.weight', 'image_encoder.blocks.5.attn.lora_B_v.10.weight', 'image_encoder.blocks.8.attn.lora_B_v.10.weight', 'classifier_pool.10.weight', 'image_encoder.blocks.1.attn.lora_B_v.10.weight', 'image_encoder.blocks.7.attn.lora_B_v.10.weight', 'image_encoder.blocks.10.attn.lora_B_k.10.weight', 'image_encoder.blocks.6.attn.lora_B_v.10.weight', 'image_encoder.blocks.0.attn.lora_B_v.10.weight', 'image_encoder.blocks.4.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_k.10.weight', 'image_encoder.blocks.3.attn.lora_B_k.10.weight', 'image_encoder.blocks.8.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_v.10.weight', 'image_encoder.blocks.0.attn.lora_B_k.10.weight', 'image_encoder.blocks.9.attn.lora_B_k.10.weight', 'image_encoder.blocks.7.attn.lora_B_k.10.weight', 'image_encoder.blocks.3.attn.lora_B_v.10.weight', 'image_encoder.blocks.2.attn.lora_B_v.10.weight', 'image_encoder.blocks.6.attn.lora_B_k.10.weight', 'image_encoder.blocks.11.attn.lora_B_k.10.weight', 'image_encoder.blocks.5.attn.lora_B_k.10.weight', 'image_encoder.blocks.4.attn.lora_B_k.10.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 10, Epoch 1/20 => Loss 0.504, Train_accy 84.87:   0%|          | 0/20 [00:11<?, ?it/s]
Task 10, Epoch 1/20 => Loss 0.504, Train_accy 84.87:   5%|▌         | 1/20 [00:11<03:35, 11.33s/it]
Task 10, Epoch 2/20 => Loss 0.149, Train_accy 94.72:   5%|▌         | 1/20 [00:22<03:35, 11.33s/it]
Task 10, Epoch 2/20 => Loss 0.149, Train_accy 94.72:  10%|█         | 2/20 [00:22<03:26, 11.48s/it]
Task 10, Epoch 3/20 => Loss 0.156, Train_accy 95.45:  10%|█         | 2/20 [00:34<03:26, 11.48s/it]
Task 10, Epoch 3/20 => Loss 0.156, Train_accy 95.45:  15%|█▌        | 3/20 [00:34<03:16, 11.54s/it]
Task 10, Epoch 4/20 => Loss 0.112, Train_accy 96.11:  15%|█▌        | 3/20 [00:46<03:16, 11.54s/it]
Task 10, Epoch 4/20 => Loss 0.112, Train_accy 96.11:  20%|██        | 4/20 [00:46<03:05, 11.59s/it]
Task 10, Epoch 5/20 => Loss 0.116, Train_accy 96.04:  20%|██        | 4/20 [00:57<03:05, 11.59s/it]
Task 10, Epoch 5/20 => Loss 0.116, Train_accy 96.04:  25%|██▌       | 5/20 [00:57<02:52, 11.51s/it]
Task 10, Epoch 6/20 => Loss 0.107, Train_accy 96.04:  25%|██▌       | 5/20 [01:09<02:52, 11.51s/it]
Task 10, Epoch 6/20 => Loss 0.107, Train_accy 96.04:  30%|███       | 6/20 [01:09<02:41, 11.54s/it]
Task 10, Epoch 7/20 => Loss 0.095, Train_accy 96.90:  30%|███       | 6/20 [01:20<02:41, 11.54s/it]
Task 10, Epoch 7/20 => Loss 0.095, Train_accy 96.90:  35%|███▌      | 7/20 [01:20<02:29, 11.51s/it]
Task 10, Epoch 8/20 => Loss 0.086, Train_accy 96.87:  35%|███▌      | 7/20 [01:32<02:29, 11.51s/it]
Task 10, Epoch 8/20 => Loss 0.086, Train_accy 96.87:  40%|████      | 8/20 [01:32<02:18, 11.54s/it]
Task 10, Epoch 9/20 => Loss 0.091, Train_accy 96.90:  40%|████      | 8/20 [01:43<02:18, 11.54s/it]
Task 10, Epoch 9/20 => Loss 0.091, Train_accy 96.90:  45%|████▌     | 9/20 [01:43<02:06, 11.50s/it]
Task 10, Epoch 10/20 => Loss 0.083, Train_accy 97.30:  45%|████▌     | 9/20 [01:55<02:06, 11.50s/it]
Task 10, Epoch 10/20 => Loss 0.083, Train_accy 97.30:  50%|█████     | 10/20 [01:55<01:55, 11.54s/it]
Task 10, Epoch 11/20 => Loss 0.086, Train_accy 96.97:  50%|█████     | 10/20 [02:06<01:55, 11.54s/it]
Task 10, Epoch 11/20 => Loss 0.086, Train_accy 96.97:  55%|█████▌    | 11/20 [02:06<01:43, 11.51s/it]
Task 10, Epoch 12/20 => Loss 0.083, Train_accy 97.13:  55%|█████▌    | 11/20 [02:18<01:43, 11.51s/it]
Task 10, Epoch 12/20 => Loss 0.083, Train_accy 97.13:  60%|██████    | 12/20 [02:18<01:31, 11.50s/it]
Task 10, Epoch 13/20 => Loss 0.078, Train_accy 97.40:  60%|██████    | 12/20 [02:29<01:31, 11.50s/it]
Task 10, Epoch 13/20 => Loss 0.078, Train_accy 97.40:  65%|██████▌   | 13/20 [02:29<01:20, 11.55s/it]
Task 10, Epoch 14/20 => Loss 0.079, Train_accy 97.46:  65%|██████▌   | 13/20 [02:41<01:20, 11.55s/it]
Task 10, Epoch 14/20 => Loss 0.079, Train_accy 97.46:  70%|███████   | 14/20 [02:41<01:09, 11.51s/it]
Task 10, Epoch 15/20 => Loss 0.051, Train_accy 98.22:  70%|███████   | 14/20 [02:52<01:09, 11.51s/it]
Task 10, Epoch 15/20 => Loss 0.051, Train_accy 98.22:  75%|███████▌  | 15/20 [02:52<00:57, 11.50s/it]
Task 10, Epoch 16/20 => Loss 0.063, Train_accy 97.76:  75%|███████▌  | 15/20 [03:04<00:57, 11.50s/it]
Task 10, Epoch 16/20 => Loss 0.063, Train_accy 97.76:  80%|████████  | 16/20 [03:04<00:46, 11.53s/it]
Task 10, Epoch 17/20 => Loss 0.065, Train_accy 97.46:  80%|████████  | 16/20 [03:15<00:46, 11.53s/it]
Task 10, Epoch 17/20 => Loss 0.065, Train_accy 97.46:  85%|████████▌ | 17/20 [03:15<00:34, 11.53s/it]
Task 10, Epoch 18/20 => Loss 0.058, Train_accy 97.92:  85%|████████▌ | 17/20 [03:27<00:34, 11.53s/it]
Task 10, Epoch 18/20 => Loss 0.058, Train_accy 97.92:  90%|█████████ | 18/20 [03:27<00:23, 11.55s/it]
Task 10, Epoch 19/20 => Loss 0.072, Train_accy 97.59:  90%|█████████ | 18/20 [03:38<00:23, 11.55s/it]
Task 10, Epoch 19/20 => Loss 0.072, Train_accy 97.59:  95%|█████████▌| 19/20 [03:38<00:11, 11.51s/it]
Task 10, Epoch 20/20 => Loss 0.058, Train_accy 98.09:  95%|█████████▌| 19/20 [03:50<00:11, 11.51s/it]
Task 10, Epoch 20/20 => Loss 0.058, Train_accy 98.09: 100%|██████████| 20/20 [03:50<00:00, 11.49s/it]
Task 10, Epoch 20/20 => Loss 0.058, Train_accy 98.09: 100%|██████████| 20/20 [03:50<00:00, 11.52s/it]
2024-08-12 01:16:10,353 [inflora.py] => Task 10, Epoch 20/20 => Loss 0.058, Train_accy 98.09
Threshold:  0.9666666666666667
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 29/768 type remove
Layer 4 : 31/768 type remove
Layer 5 : 49/768 type remove
Layer 6 : 53/768 type remove
Layer 7 : 53/768 type remove
Layer 8 : 61/768 type remove
Layer 9 : 78/768 type remove
Layer 10 : 91/768 type remove
Layer 11 : 40/768 type remove
Layer 12 : 65/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 01:16:30,166 [trainer.py] => Time:261.7108197212219
2196 2196
2196 2196
2024-08-12 01:16:35,255 [trainer.py] => Time:5.0887556076049805
2024-08-12 01:16:35,255 [inflora.py] => Exemplar size: 0
2024-08-12 01:16:35,255 [trainer.py] => CNN: {'total': 77.14, '00-09': 81.0, '10-19': 80.5, '20-29': 68.0, '30-39': 71.5, '40-49': 80.0, '50-59': 84.42, '60-69': 85.5, '70-79': 79.5, '80-89': 70.35, '90-99': 65.33, '100-109': 82.41, 'old': 76.61, 'new': 82.41}
2024-08-12 01:16:35,255 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14]
2024-08-12 01:16:35,255 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13]
2024-08-12 01:16:35,255 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608]
2024-08-12 01:16:35,800 [trainer.py] => All params: 112239531
2024-08-12 01:16:35,804 [trainer.py] => Trainable params: 81418
2024-08-12 01:16:35,804 [inflora.py] => Learning on 110-120
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_k.11.weight', 'image_encoder.blocks.0.attn.lora_B_v.11.weight', 'image_encoder.blocks.10.attn.lora_B_k.11.weight', 'image_encoder.blocks.5.attn.lora_B_v.11.weight', 'classifier_pool.11.bias', 'image_encoder.blocks.2.attn.lora_B_v.11.weight', 'image_encoder.blocks.8.attn.lora_B_k.11.weight', 'image_encoder.blocks.7.attn.lora_B_v.11.weight', 'image_encoder.blocks.5.attn.lora_B_k.11.weight', 'image_encoder.blocks.11.attn.lora_B_v.11.weight', 'image_encoder.blocks.1.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_v.11.weight', 'image_encoder.blocks.6.attn.lora_B_k.11.weight', 'image_encoder.blocks.3.attn.lora_B_k.11.weight', 'image_encoder.blocks.2.attn.lora_B_k.11.weight', 'image_encoder.blocks.0.attn.lora_B_k.11.weight', 'image_encoder.blocks.7.attn.lora_B_k.11.weight', 'image_encoder.blocks.4.attn.lora_B_k.11.weight', 'image_encoder.blocks.9.attn.lora_B_k.11.weight', 'image_encoder.blocks.8.attn.lora_B_v.11.weight', 'image_encoder.blocks.9.attn.lora_B_v.11.weight', 'image_encoder.blocks.3.attn.lora_B_v.11.weight', 'classifier_pool.11.weight', 'image_encoder.blocks.10.attn.lora_B_v.11.weight', 'image_encoder.blocks.6.attn.lora_B_v.11.weight', 'image_encoder.blocks.1.attn.lora_B_v.11.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 11, Epoch 1/20 => Loss 0.509, Train_accy 82.37:   0%|          | 0/20 [00:11<?, ?it/s]
Task 11, Epoch 1/20 => Loss 0.509, Train_accy 82.37:   5%|▌         | 1/20 [00:11<03:41, 11.68s/it]
Task 11, Epoch 2/20 => Loss 0.215, Train_accy 92.41:   5%|▌         | 1/20 [00:23<03:41, 11.68s/it]
Task 11, Epoch 2/20 => Loss 0.215, Train_accy 92.41:  10%|█         | 2/20 [00:23<03:29, 11.66s/it]
Task 11, Epoch 3/20 => Loss 0.154, Train_accy 94.33:  10%|█         | 2/20 [00:35<03:29, 11.66s/it]
Task 11, Epoch 3/20 => Loss 0.154, Train_accy 94.33:  15%|█▌        | 3/20 [00:35<03:18, 11.70s/it]
Task 11, Epoch 4/20 => Loss 0.131, Train_accy 94.98:  15%|█▌        | 3/20 [00:46<03:18, 11.70s/it]
Task 11, Epoch 4/20 => Loss 0.131, Train_accy 94.98:  20%|██        | 4/20 [00:46<03:06, 11.63s/it]
Task 11, Epoch 5/20 => Loss 0.132, Train_accy 95.28:  20%|██        | 4/20 [00:58<03:06, 11.63s/it]
Task 11, Epoch 5/20 => Loss 0.132, Train_accy 95.28:  25%|██▌       | 5/20 [00:58<02:54, 11.66s/it]
Task 11, Epoch 6/20 => Loss 0.112, Train_accy 95.99:  25%|██▌       | 5/20 [01:09<02:54, 11.66s/it]
Task 11, Epoch 6/20 => Loss 0.112, Train_accy 95.99:  30%|███       | 6/20 [01:09<02:43, 11.65s/it]
Task 11, Epoch 7/20 => Loss 0.113, Train_accy 95.96:  30%|███       | 6/20 [01:21<02:43, 11.65s/it]
Task 11, Epoch 7/20 => Loss 0.113, Train_accy 95.96:  35%|███▌      | 7/20 [01:21<02:31, 11.62s/it]
Task 11, Epoch 8/20 => Loss 0.108, Train_accy 96.06:  35%|███▌      | 7/20 [01:33<02:31, 11.62s/it]
Task 11, Epoch 8/20 => Loss 0.108, Train_accy 96.06:  40%|████      | 8/20 [01:33<02:19, 11.65s/it]
Task 11, Epoch 9/20 => Loss 0.098, Train_accy 96.71:  40%|████      | 8/20 [01:44<02:19, 11.65s/it]
Task 11, Epoch 9/20 => Loss 0.098, Train_accy 96.71:  45%|████▌     | 9/20 [01:44<02:07, 11.59s/it]
Task 11, Epoch 10/20 => Loss 0.087, Train_accy 97.03:  45%|████▌     | 9/20 [01:56<02:07, 11.59s/it]
Task 11, Epoch 10/20 => Loss 0.087, Train_accy 97.03:  50%|█████     | 10/20 [01:56<01:56, 11.62s/it]
Task 11, Epoch 11/20 => Loss 0.100, Train_accy 96.09:  50%|█████     | 10/20 [02:08<01:56, 11.62s/it]
Task 11, Epoch 11/20 => Loss 0.100, Train_accy 96.09:  55%|█████▌    | 11/20 [02:08<01:45, 11.67s/it]
Task 11, Epoch 12/20 => Loss 0.082, Train_accy 96.81:  55%|█████▌    | 11/20 [02:19<01:45, 11.67s/it]
Task 11, Epoch 12/20 => Loss 0.082, Train_accy 96.81:  60%|██████    | 12/20 [02:19<01:33, 11.63s/it]
Task 11, Epoch 13/20 => Loss 0.084, Train_accy 97.10:  60%|██████    | 12/20 [02:31<01:33, 11.63s/it]
Task 11, Epoch 13/20 => Loss 0.084, Train_accy 97.10:  65%|██████▌   | 13/20 [02:31<01:21, 11.66s/it]
Task 11, Epoch 14/20 => Loss 0.083, Train_accy 96.74:  65%|██████▌   | 13/20 [02:43<01:21, 11.66s/it]
Task 11, Epoch 14/20 => Loss 0.083, Train_accy 96.74:  70%|███████   | 14/20 [02:43<01:09, 11.66s/it]
Task 11, Epoch 15/20 => Loss 0.083, Train_accy 97.03:  70%|███████   | 14/20 [02:54<01:09, 11.66s/it]
Task 11, Epoch 15/20 => Loss 0.083, Train_accy 97.03:  75%|███████▌  | 15/20 [02:54<00:58, 11.73s/it]
Task 11, Epoch 16/20 => Loss 0.082, Train_accy 97.03:  75%|███████▌  | 15/20 [03:06<00:58, 11.73s/it]
Task 11, Epoch 16/20 => Loss 0.082, Train_accy 97.03:  80%|████████  | 16/20 [03:06<00:46, 11.67s/it]
Task 11, Epoch 17/20 => Loss 0.082, Train_accy 96.94:  80%|████████  | 16/20 [03:18<00:46, 11.67s/it]
Task 11, Epoch 17/20 => Loss 0.082, Train_accy 96.94:  85%|████████▌ | 17/20 [03:18<00:34, 11.63s/it]
Task 11, Epoch 18/20 => Loss 0.084, Train_accy 96.77:  85%|████████▌ | 17/20 [03:29<00:34, 11.63s/it]
Task 11, Epoch 18/20 => Loss 0.084, Train_accy 96.77:  90%|█████████ | 18/20 [03:29<00:23, 11.66s/it]
Task 11, Epoch 19/20 => Loss 0.077, Train_accy 97.00:  90%|█████████ | 18/20 [03:41<00:23, 11.66s/it]
Task 11, Epoch 19/20 => Loss 0.077, Train_accy 97.00:  95%|█████████▌| 19/20 [03:41<00:11, 11.61s/it]
Task 11, Epoch 20/20 => Loss 0.070, Train_accy 97.33:  95%|█████████▌| 19/20 [03:52<00:11, 11.61s/it]
Task 11, Epoch 20/20 => Loss 0.070, Train_accy 97.33: 100%|██████████| 20/20 [03:52<00:00, 11.60s/it]
Task 11, Epoch 20/20 => Loss 0.070, Train_accy 97.33: 100%|██████████| 20/20 [03:52<00:00, 11.64s/it]
2024-08-12 01:20:40,681 [inflora.py] => Task 11, Epoch 20/20 => Loss 0.070, Train_accy 97.33
Threshold:  0.9683333333333333
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 32/768 type remove
Layer 4 : 34/768 type remove
Layer 5 : 52/768 type remove
Layer 6 : 57/768 type remove
Layer 7 : 57/768 type remove
Layer 8 : 68/768 type remove
Layer 9 : 88/768 type remove
Layer 10 : 104/768 type remove
Layer 11 : 46/768 type remove
Layer 12 : 71/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 01:21:00,735 [trainer.py] => Time:264.9306757450104
2395 2395
2395 2395
2024-08-12 01:21:06,176 [trainer.py] => Time:5.44111442565918
2024-08-12 01:21:06,176 [inflora.py] => Exemplar size: 0
2024-08-12 01:21:06,176 [trainer.py] => CNN: {'total': 75.82, '00-09': 82.5, '10-19': 80.0, '20-29': 68.0, '30-39': 69.5, '40-49': 77.5, '50-59': 85.93, '60-69': 83.5, '70-79': 80.0, '80-89': 70.85, '90-99': 67.34, '100-109': 79.9, '110-119': 64.82, 'old': 76.82, 'new': 64.82}
2024-08-12 01:21:06,176 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82]
2024-08-12 01:21:06,176 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16]
2024-08-12 01:21:06,176 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172]
2024-08-12 01:21:06,716 [trainer.py] => All params: 112239531
2024-08-12 01:21:06,721 [trainer.py] => Trainable params: 81418
2024-08-12 01:21:06,721 [inflora.py] => Learning on 120-130
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_v.12.weight', 'image_encoder.blocks.0.attn.lora_B_v.12.weight', 'image_encoder.blocks.9.attn.lora_B_k.12.weight', 'classifier_pool.12.bias', 'image_encoder.blocks.1.attn.lora_B_k.12.weight', 'image_encoder.blocks.10.attn.lora_B_v.12.weight', 'image_encoder.blocks.6.attn.lora_B_k.12.weight', 'image_encoder.blocks.8.attn.lora_B_v.12.weight', 'image_encoder.blocks.8.attn.lora_B_k.12.weight', 'image_encoder.blocks.10.attn.lora_B_k.12.weight', 'image_encoder.blocks.9.attn.lora_B_v.12.weight', 'image_encoder.blocks.5.attn.lora_B_k.12.weight', 'image_encoder.blocks.4.attn.lora_B_k.12.weight', 'image_encoder.blocks.7.attn.lora_B_k.12.weight', 'image_encoder.blocks.0.attn.lora_B_k.12.weight', 'image_encoder.blocks.6.attn.lora_B_v.12.weight', 'image_encoder.blocks.4.attn.lora_B_v.12.weight', 'image_encoder.blocks.11.attn.lora_B_k.12.weight', 'image_encoder.blocks.3.attn.lora_B_v.12.weight', 'classifier_pool.12.weight', 'image_encoder.blocks.1.attn.lora_B_v.12.weight', 'image_encoder.blocks.2.attn.lora_B_k.12.weight', 'image_encoder.blocks.7.attn.lora_B_v.12.weight', 'image_encoder.blocks.11.attn.lora_B_v.12.weight', 'image_encoder.blocks.3.attn.lora_B_k.12.weight', 'image_encoder.blocks.5.attn.lora_B_v.12.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 12, Epoch 1/20 => Loss 0.531, Train_accy 83.41:   0%|          | 0/20 [00:11<?, ?it/s]
Task 12, Epoch 1/20 => Loss 0.531, Train_accy 83.41:   5%|▌         | 1/20 [00:11<03:33, 11.24s/it]
Task 12, Epoch 2/20 => Loss 0.119, Train_accy 95.87:   5%|▌         | 1/20 [00:22<03:33, 11.24s/it]
Task 12, Epoch 2/20 => Loss 0.119, Train_accy 95.87:  10%|█         | 2/20 [00:22<03:20, 11.16s/it]
Task 12, Epoch 3/20 => Loss 0.090, Train_accy 97.02:  10%|█         | 2/20 [00:33<03:20, 11.16s/it]
Task 12, Epoch 3/20 => Loss 0.090, Train_accy 97.02:  15%|█▌        | 3/20 [00:33<03:10, 11.23s/it]
Task 12, Epoch 4/20 => Loss 0.082, Train_accy 97.09:  15%|█▌        | 3/20 [00:44<03:10, 11.23s/it]
Task 12, Epoch 4/20 => Loss 0.082, Train_accy 97.09:  20%|██        | 4/20 [00:44<02:59, 11.23s/it]
Task 12, Epoch 5/20 => Loss 0.076, Train_accy 97.97:  20%|██        | 4/20 [00:56<02:59, 11.23s/it]
Task 12, Epoch 5/20 => Loss 0.076, Train_accy 97.97:  25%|██▌       | 5/20 [00:56<02:48, 11.21s/it]
Task 12, Epoch 6/20 => Loss 0.062, Train_accy 98.17:  25%|██▌       | 5/20 [01:07<02:48, 11.21s/it]
Task 12, Epoch 6/20 => Loss 0.062, Train_accy 98.17:  30%|███       | 6/20 [01:07<02:36, 11.20s/it]
Task 12, Epoch 7/20 => Loss 0.066, Train_accy 97.90:  30%|███       | 6/20 [01:18<02:36, 11.20s/it]
Task 12, Epoch 7/20 => Loss 0.066, Train_accy 97.90:  35%|███▌      | 7/20 [01:18<02:25, 11.19s/it]
Task 12, Epoch 8/20 => Loss 0.051, Train_accy 98.38:  35%|███▌      | 7/20 [01:29<02:25, 11.19s/it]
Task 12, Epoch 8/20 => Loss 0.051, Train_accy 98.38:  40%|████      | 8/20 [01:29<02:15, 11.25s/it]
Task 12, Epoch 9/20 => Loss 0.052, Train_accy 98.38:  40%|████      | 8/20 [01:40<02:15, 11.25s/it]
Task 12, Epoch 9/20 => Loss 0.052, Train_accy 98.38:  45%|████▌     | 9/20 [01:40<02:03, 11.20s/it]
Task 12, Epoch 10/20 => Loss 0.052, Train_accy 98.34:  45%|████▌     | 9/20 [01:52<02:03, 11.20s/it]
Task 12, Epoch 10/20 => Loss 0.052, Train_accy 98.34:  50%|█████     | 10/20 [01:52<01:52, 11.22s/it]
Task 12, Epoch 11/20 => Loss 0.051, Train_accy 98.34:  50%|█████     | 10/20 [02:03<01:52, 11.22s/it]
Task 12, Epoch 11/20 => Loss 0.051, Train_accy 98.34:  55%|█████▌    | 11/20 [02:03<01:41, 11.26s/it]
Task 12, Epoch 12/20 => Loss 0.041, Train_accy 98.68:  55%|█████▌    | 11/20 [02:14<01:41, 11.26s/it]
Task 12, Epoch 12/20 => Loss 0.041, Train_accy 98.68:  60%|██████    | 12/20 [02:14<01:29, 11.24s/it]
Task 12, Epoch 13/20 => Loss 0.042, Train_accy 98.54:  60%|██████    | 12/20 [02:25<01:29, 11.24s/it]
Task 12, Epoch 13/20 => Loss 0.042, Train_accy 98.54:  65%|██████▌   | 13/20 [02:25<01:18, 11.24s/it]
Task 12, Epoch 14/20 => Loss 0.052, Train_accy 98.34:  65%|██████▌   | 13/20 [02:37<01:18, 11.24s/it]
Task 12, Epoch 14/20 => Loss 0.052, Train_accy 98.34:  70%|███████   | 14/20 [02:37<01:07, 11.23s/it]
Task 12, Epoch 15/20 => Loss 0.054, Train_accy 98.04:  70%|███████   | 14/20 [02:48<01:07, 11.23s/it]
Task 12, Epoch 15/20 => Loss 0.054, Train_accy 98.04:  75%|███████▌  | 15/20 [02:48<00:56, 11.21s/it]
Task 12, Epoch 16/20 => Loss 0.043, Train_accy 98.31:  75%|███████▌  | 15/20 [02:59<00:56, 11.21s/it]
Task 12, Epoch 16/20 => Loss 0.043, Train_accy 98.31:  80%|████████  | 16/20 [02:59<00:45, 11.26s/it]
Task 12, Epoch 17/20 => Loss 0.041, Train_accy 98.71:  80%|████████  | 16/20 [03:10<00:45, 11.26s/it]
Task 12, Epoch 17/20 => Loss 0.041, Train_accy 98.71:  85%|████████▌ | 17/20 [03:10<00:33, 11.23s/it]
Task 12, Epoch 18/20 => Loss 0.036, Train_accy 98.61:  85%|████████▌ | 17/20 [03:22<00:33, 11.23s/it]
Task 12, Epoch 18/20 => Loss 0.036, Train_accy 98.61:  90%|█████████ | 18/20 [03:22<00:22, 11.28s/it]
Task 12, Epoch 19/20 => Loss 0.043, Train_accy 98.65:  90%|█████████ | 18/20 [03:33<00:22, 11.28s/it]
Task 12, Epoch 19/20 => Loss 0.043, Train_accy 98.65:  95%|█████████▌| 19/20 [03:33<00:11, 11.30s/it]
Task 12, Epoch 20/20 => Loss 0.041, Train_accy 98.44:  95%|█████████▌| 19/20 [03:44<00:11, 11.30s/it]
Task 12, Epoch 20/20 => Loss 0.041, Train_accy 98.44: 100%|██████████| 20/20 [03:44<00:00, 11.29s/it]
Task 12, Epoch 20/20 => Loss 0.041, Train_accy 98.44: 100%|██████████| 20/20 [03:44<00:00, 11.24s/it]
2024-08-12 01:25:03,062 [inflora.py] => Task 12, Epoch 20/20 => Loss 0.041, Train_accy 98.44
Threshold:  0.97
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 34/768 type remove
Layer 4 : 37/768 type remove
Layer 5 : 56/768 type remove
Layer 6 : 61/768 type remove
Layer 7 : 62/768 type remove
Layer 8 : 73/768 type remove
Layer 9 : 94/768 type remove
Layer 10 : 111/768 type remove
Layer 11 : 51/768 type remove
Layer 12 : 75/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 01:25:23,653 [trainer.py] => Time:256.93163537979126
2595 2595
2595 2595
2024-08-12 01:25:29,537 [trainer.py] => Time:5.883269786834717
2024-08-12 01:25:29,537 [inflora.py] => Exemplar size: 0
2024-08-12 01:25:29,537 [trainer.py] => CNN: {'total': 75.68, '00-09': 83.0, '10-19': 79.0, '20-29': 70.5, '30-39': 70.0, '40-49': 75.5, '50-59': 80.9, '60-69': 82.5, '70-79': 81.5, '80-89': 69.85, '90-99': 67.84, '100-109': 79.9, '110-119': 65.33, '120-129': 78.0, 'old': 75.49, 'new': 78.0}
2024-08-12 01:25:29,537 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68]
2024-08-12 01:25:29,537 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11]
2024-08-12 01:25:29,538 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734]
2024-08-12 01:25:30,072 [trainer.py] => All params: 112239531
2024-08-12 01:25:30,077 [trainer.py] => Trainable params: 81418
2024-08-12 01:25:30,077 [inflora.py] => Learning on 130-140
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.13.weight', 'image_encoder.blocks.0.attn.lora_B_v.13.weight', 'image_encoder.blocks.11.attn.lora_B_v.13.weight', 'classifier_pool.13.weight', 'image_encoder.blocks.2.attn.lora_B_k.13.weight', 'image_encoder.blocks.7.attn.lora_B_v.13.weight', 'image_encoder.blocks.11.attn.lora_B_k.13.weight', 'image_encoder.blocks.7.attn.lora_B_k.13.weight', 'image_encoder.blocks.10.attn.lora_B_k.13.weight', 'image_encoder.blocks.8.attn.lora_B_k.13.weight', 'image_encoder.blocks.6.attn.lora_B_k.13.weight', 'image_encoder.blocks.10.attn.lora_B_v.13.weight', 'image_encoder.blocks.1.attn.lora_B_k.13.weight', 'image_encoder.blocks.3.attn.lora_B_k.13.weight', 'image_encoder.blocks.4.attn.lora_B_k.13.weight', 'image_encoder.blocks.9.attn.lora_B_k.13.weight', 'image_encoder.blocks.8.attn.lora_B_v.13.weight', 'image_encoder.blocks.2.attn.lora_B_v.13.weight', 'image_encoder.blocks.3.attn.lora_B_v.13.weight', 'classifier_pool.13.bias', 'image_encoder.blocks.1.attn.lora_B_v.13.weight', 'image_encoder.blocks.5.attn.lora_B_v.13.weight', 'image_encoder.blocks.6.attn.lora_B_v.13.weight', 'image_encoder.blocks.0.attn.lora_B_k.13.weight', 'image_encoder.blocks.4.attn.lora_B_v.13.weight', 'image_encoder.blocks.5.attn.lora_B_k.13.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 13, Epoch 1/20 => Loss 0.595, Train_accy 79.60:   0%|          | 0/20 [00:11<?, ?it/s]
Task 13, Epoch 1/20 => Loss 0.595, Train_accy 79.60:   5%|▌         | 1/20 [00:11<03:42, 11.71s/it]
Task 13, Epoch 2/20 => Loss 0.233, Train_accy 90.88:   5%|▌         | 1/20 [00:23<03:42, 11.71s/it]
Task 13, Epoch 2/20 => Loss 0.233, Train_accy 90.88:  10%|█         | 2/20 [00:23<03:28, 11.58s/it]
Task 13, Epoch 3/20 => Loss 0.198, Train_accy 92.34:  10%|█         | 2/20 [00:34<03:28, 11.58s/it]
Task 13, Epoch 3/20 => Loss 0.198, Train_accy 92.34:  15%|█▌        | 3/20 [00:34<03:17, 11.59s/it]
Task 13, Epoch 4/20 => Loss 0.198, Train_accy 92.15:  15%|█▌        | 3/20 [00:46<03:17, 11.59s/it]
Task 13, Epoch 4/20 => Loss 0.198, Train_accy 92.15:  20%|██        | 4/20 [00:46<03:05, 11.59s/it]
Task 13, Epoch 5/20 => Loss 0.176, Train_accy 93.26:  20%|██        | 4/20 [00:58<03:05, 11.59s/it]
Task 13, Epoch 5/20 => Loss 0.176, Train_accy 93.26:  25%|██▌       | 5/20 [00:58<02:54, 11.66s/it]
Task 13, Epoch 6/20 => Loss 0.143, Train_accy 94.49:  25%|██▌       | 5/20 [01:09<02:54, 11.66s/it]
Task 13, Epoch 6/20 => Loss 0.143, Train_accy 94.49:  30%|███       | 6/20 [01:09<02:43, 11.68s/it]
Task 13, Epoch 7/20 => Loss 0.132, Train_accy 94.85:  30%|███       | 6/20 [01:21<02:43, 11.68s/it]
Task 13, Epoch 7/20 => Loss 0.132, Train_accy 94.85:  35%|███▌      | 7/20 [01:21<02:31, 11.62s/it]
Task 13, Epoch 8/20 => Loss 0.132, Train_accy 94.40:  35%|███▌      | 7/20 [01:33<02:31, 11.62s/it]
Task 13, Epoch 8/20 => Loss 0.132, Train_accy 94.40:  40%|████      | 8/20 [01:33<02:20, 11.67s/it]
Task 13, Epoch 9/20 => Loss 0.135, Train_accy 94.82:  40%|████      | 8/20 [01:44<02:20, 11.67s/it]
Task 13, Epoch 9/20 => Loss 0.135, Train_accy 94.82:  45%|████▌     | 9/20 [01:44<02:07, 11.63s/it]
Task 13, Epoch 10/20 => Loss 0.127, Train_accy 95.11:  45%|████▌     | 9/20 [01:56<02:07, 11.63s/it]
Task 13, Epoch 10/20 => Loss 0.127, Train_accy 95.11:  50%|█████     | 10/20 [01:56<01:56, 11.67s/it]
Task 13, Epoch 11/20 => Loss 0.132, Train_accy 94.69:  50%|█████     | 10/20 [02:08<01:56, 11.67s/it]
Task 13, Epoch 11/20 => Loss 0.132, Train_accy 94.69:  55%|█████▌    | 11/20 [02:08<01:44, 11.66s/it]
Task 13, Epoch 12/20 => Loss 0.121, Train_accy 95.37:  55%|█████▌    | 11/20 [02:19<01:44, 11.66s/it]
Task 13, Epoch 12/20 => Loss 0.121, Train_accy 95.37:  60%|██████    | 12/20 [02:19<01:33, 11.70s/it]
Task 13, Epoch 13/20 => Loss 0.119, Train_accy 95.31:  60%|██████    | 12/20 [02:31<01:33, 11.70s/it]
Task 13, Epoch 13/20 => Loss 0.119, Train_accy 95.31:  65%|██████▌   | 13/20 [02:31<01:21, 11.70s/it]
Task 13, Epoch 14/20 => Loss 0.108, Train_accy 95.70:  65%|██████▌   | 13/20 [02:43<01:21, 11.70s/it]
Task 13, Epoch 14/20 => Loss 0.108, Train_accy 95.70:  70%|███████   | 14/20 [02:43<01:09, 11.63s/it]
Task 13, Epoch 15/20 => Loss 0.114, Train_accy 95.57:  70%|███████   | 14/20 [02:54<01:09, 11.63s/it]
Task 13, Epoch 15/20 => Loss 0.114, Train_accy 95.57:  75%|███████▌  | 15/20 [02:54<00:58, 11.66s/it]
Task 13, Epoch 16/20 => Loss 0.101, Train_accy 95.93:  75%|███████▌  | 15/20 [03:06<00:58, 11.66s/it]
Task 13, Epoch 16/20 => Loss 0.101, Train_accy 95.93:  80%|████████  | 16/20 [03:06<00:46, 11.66s/it]
Task 13, Epoch 17/20 => Loss 0.113, Train_accy 95.54:  80%|████████  | 16/20 [03:18<00:46, 11.66s/it]
Task 13, Epoch 17/20 => Loss 0.113, Train_accy 95.54:  85%|████████▌ | 17/20 [03:18<00:34, 11.65s/it]
Task 13, Epoch 18/20 => Loss 0.098, Train_accy 96.02:  85%|████████▌ | 17/20 [03:29<00:34, 11.65s/it]
Task 13, Epoch 18/20 => Loss 0.098, Train_accy 96.02:  90%|█████████ | 18/20 [03:29<00:23, 11.62s/it]
Task 13, Epoch 19/20 => Loss 0.109, Train_accy 95.93:  90%|█████████ | 18/20 [03:41<00:23, 11.62s/it]
Task 13, Epoch 19/20 => Loss 0.109, Train_accy 95.93:  95%|█████████▌| 19/20 [03:41<00:11, 11.63s/it]
Task 13, Epoch 20/20 => Loss 0.101, Train_accy 96.38:  95%|█████████▌| 19/20 [03:52<00:11, 11.63s/it]
Task 13, Epoch 20/20 => Loss 0.101, Train_accy 96.38: 100%|██████████| 20/20 [03:52<00:00, 11.63s/it]
Task 13, Epoch 20/20 => Loss 0.101, Train_accy 96.38: 100%|██████████| 20/20 [03:52<00:00, 11.65s/it]
2024-08-12 01:29:34,310 [inflora.py] => Task 13, Epoch 20/20 => Loss 0.101, Train_accy 96.38
Threshold:  0.9716666666666667
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 37/768 type remove
Layer 4 : 39/768 type remove
Layer 5 : 59/768 type remove
Layer 6 : 66/768 type remove
Layer 7 : 67/768 type remove
Layer 8 : 79/768 type remove
Layer 9 : 100/768 type remove
Layer 10 : 116/768 type remove
Layer 11 : 54/768 type remove
Layer 12 : 79/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 01:29:55,169 [trainer.py] => Time:265.0920946598053
2795 2795
2795 2795
2024-08-12 01:30:01,523 [trainer.py] => Time:6.353673219680786
2024-08-12 01:30:01,524 [inflora.py] => Exemplar size: 0
2024-08-12 01:30:01,524 [trainer.py] => CNN: {'total': 74.49, '00-09': 81.0, '10-19': 81.0, '20-29': 67.5, '30-39': 69.5, '40-49': 75.0, '50-59': 77.39, '60-69': 83.0, '70-79': 80.5, '80-89': 68.34, '90-99': 66.83, '100-109': 79.4, '110-119': 64.82, '120-129': 74.5, '130-139': 74.0, 'old': 74.53, 'new': 74.0}
2024-08-12 01:30:01,524 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68, 74.49]
2024-08-12 01:30:01,524 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11, 97.89]
2024-08-12 01:30:01,524 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734, 0.7456171735241502]
2024-08-12 01:30:02,057 [trainer.py] => All params: 112239531
2024-08-12 01:30:02,062 [trainer.py] => Trainable params: 81418
2024-08-12 01:30:02,062 [inflora.py] => Learning on 140-150
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.14.weight', 'image_encoder.blocks.11.attn.lora_B_v.14.weight', 'image_encoder.blocks.6.attn.lora_B_v.14.weight', 'image_encoder.blocks.10.attn.lora_B_k.14.weight', 'image_encoder.blocks.1.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_k.14.weight', 'image_encoder.blocks.9.attn.lora_B_k.14.weight', 'image_encoder.blocks.1.attn.lora_B_k.14.weight', 'image_encoder.blocks.2.attn.lora_B_k.14.weight', 'image_encoder.blocks.2.attn.lora_B_v.14.weight', 'image_encoder.blocks.4.attn.lora_B_v.14.weight', 'image_encoder.blocks.5.attn.lora_B_k.14.weight', 'image_encoder.blocks.11.attn.lora_B_k.14.weight', 'image_encoder.blocks.8.attn.lora_B_k.14.weight', 'image_encoder.blocks.5.attn.lora_B_v.14.weight', 'image_encoder.blocks.3.attn.lora_B_k.14.weight', 'image_encoder.blocks.10.attn.lora_B_v.14.weight', 'image_encoder.blocks.0.attn.lora_B_v.14.weight', 'classifier_pool.14.weight', 'image_encoder.blocks.8.attn.lora_B_v.14.weight', 'image_encoder.blocks.7.attn.lora_B_k.14.weight', 'image_encoder.blocks.9.attn.lora_B_v.14.weight', 'image_encoder.blocks.4.attn.lora_B_k.14.weight', 'image_encoder.blocks.6.attn.lora_B_k.14.weight', 'image_encoder.blocks.7.attn.lora_B_v.14.weight', 'classifier_pool.14.bias'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 14, Epoch 1/20 => Loss 0.526, Train_accy 82.42:   0%|          | 0/20 [00:11<?, ?it/s]
Task 14, Epoch 1/20 => Loss 0.526, Train_accy 82.42:   5%|▌         | 1/20 [00:11<03:44, 11.83s/it]
Task 14, Epoch 2/20 => Loss 0.173, Train_accy 93.11:   5%|▌         | 1/20 [00:23<03:44, 11.83s/it]
Task 14, Epoch 2/20 => Loss 0.173, Train_accy 93.11:  10%|█         | 2/20 [00:23<03:30, 11.72s/it]
Task 14, Epoch 3/20 => Loss 0.149, Train_accy 94.24:  10%|█         | 2/20 [00:35<03:30, 11.72s/it]
Task 14, Epoch 3/20 => Loss 0.149, Train_accy 94.24:  15%|█▌        | 3/20 [00:35<03:19, 11.73s/it]
Task 14, Epoch 4/20 => Loss 0.132, Train_accy 94.98:  15%|█▌        | 3/20 [00:46<03:19, 11.73s/it]
Task 14, Epoch 4/20 => Loss 0.132, Train_accy 94.98:  20%|██        | 4/20 [00:46<03:07, 11.73s/it]
Task 14, Epoch 5/20 => Loss 0.135, Train_accy 95.07:  20%|██        | 4/20 [00:58<03:07, 11.73s/it]
Task 14, Epoch 5/20 => Loss 0.135, Train_accy 95.07:  25%|██▌       | 5/20 [00:58<02:56, 11.75s/it]
Task 14, Epoch 6/20 => Loss 0.118, Train_accy 95.62:  25%|██▌       | 5/20 [01:10<02:56, 11.75s/it]
Task 14, Epoch 6/20 => Loss 0.118, Train_accy 95.62:  30%|███       | 6/20 [01:10<02:45, 11.79s/it]
Task 14, Epoch 7/20 => Loss 0.115, Train_accy 95.46:  30%|███       | 6/20 [01:22<02:45, 11.79s/it]
Task 14, Epoch 7/20 => Loss 0.115, Train_accy 95.46:  35%|███▌      | 7/20 [01:22<02:33, 11.81s/it]
Task 14, Epoch 8/20 => Loss 0.100, Train_accy 96.36:  35%|███▌      | 7/20 [01:34<02:33, 11.81s/it]
Task 14, Epoch 8/20 => Loss 0.100, Train_accy 96.36:  40%|████      | 8/20 [01:34<02:22, 11.84s/it]
Task 14, Epoch 9/20 => Loss 0.103, Train_accy 96.10:  40%|████      | 8/20 [01:46<02:22, 11.84s/it]
Task 14, Epoch 9/20 => Loss 0.103, Train_accy 96.10:  45%|████▌     | 9/20 [01:46<02:10, 11.87s/it]
Task 14, Epoch 10/20 => Loss 0.092, Train_accy 96.75:  45%|████▌     | 9/20 [01:58<02:10, 11.87s/it]
Task 14, Epoch 10/20 => Loss 0.092, Train_accy 96.75:  50%|█████     | 10/20 [01:58<01:59, 11.91s/it]
Task 14, Epoch 11/20 => Loss 0.090, Train_accy 96.91:  50%|█████     | 10/20 [02:10<01:59, 11.91s/it]
Task 14, Epoch 11/20 => Loss 0.090, Train_accy 96.91:  55%|█████▌    | 11/20 [02:10<01:47, 11.89s/it]
Task 14, Epoch 12/20 => Loss 0.084, Train_accy 97.07:  55%|█████▌    | 11/20 [02:21<01:47, 11.89s/it]
Task 14, Epoch 12/20 => Loss 0.084, Train_accy 97.07:  60%|██████    | 12/20 [02:21<01:34, 11.82s/it]
Task 14, Epoch 13/20 => Loss 0.078, Train_accy 97.17:  60%|██████    | 12/20 [02:33<01:34, 11.82s/it]
Task 14, Epoch 13/20 => Loss 0.078, Train_accy 97.17:  65%|██████▌   | 13/20 [02:33<01:22, 11.82s/it]
Task 14, Epoch 14/20 => Loss 0.090, Train_accy 97.10:  65%|██████▌   | 13/20 [02:45<01:22, 11.82s/it]
Task 14, Epoch 14/20 => Loss 0.090, Train_accy 97.10:  70%|███████   | 14/20 [02:45<01:11, 11.84s/it]
Task 14, Epoch 15/20 => Loss 0.091, Train_accy 96.78:  70%|███████   | 14/20 [02:57<01:11, 11.84s/it]
Task 14, Epoch 15/20 => Loss 0.091, Train_accy 96.78:  75%|███████▌  | 15/20 [02:57<00:58, 11.79s/it]
Task 14, Epoch 16/20 => Loss 0.091, Train_accy 96.94:  75%|███████▌  | 15/20 [03:09<00:58, 11.79s/it]
Task 14, Epoch 16/20 => Loss 0.091, Train_accy 96.94:  80%|████████  | 16/20 [03:09<00:47, 11.81s/it]
Task 14, Epoch 17/20 => Loss 0.082, Train_accy 97.17:  80%|████████  | 16/20 [03:20<00:47, 11.81s/it]
Task 14, Epoch 17/20 => Loss 0.082, Train_accy 97.17:  85%|████████▌ | 17/20 [03:20<00:35, 11.85s/it]
Task 14, Epoch 18/20 => Loss 0.083, Train_accy 96.91:  85%|████████▌ | 17/20 [03:32<00:35, 11.85s/it]
Task 14, Epoch 18/20 => Loss 0.083, Train_accy 96.91:  90%|█████████ | 18/20 [03:32<00:23, 11.83s/it]
Task 14, Epoch 19/20 => Loss 0.077, Train_accy 97.52:  90%|█████████ | 18/20 [03:44<00:23, 11.83s/it]
Task 14, Epoch 19/20 => Loss 0.077, Train_accy 97.52:  95%|█████████▌| 19/20 [03:44<00:11, 11.81s/it]
Task 14, Epoch 20/20 => Loss 0.084, Train_accy 97.10:  95%|█████████▌| 19/20 [03:56<00:11, 11.81s/it]
Task 14, Epoch 20/20 => Loss 0.084, Train_accy 97.10: 100%|██████████| 20/20 [03:56<00:00, 11.78s/it]
Task 14, Epoch 20/20 => Loss 0.084, Train_accy 97.10: 100%|██████████| 20/20 [03:56<00:00, 11.81s/it]
2024-08-12 01:34:09,794 [inflora.py] => Task 14, Epoch 20/20 => Loss 0.084, Train_accy 97.10
Threshold:  0.9733333333333333
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 38/768 type remove
Layer 4 : 40/768 type remove
Layer 5 : 60/768 type remove
Layer 6 : 67/768 type remove
Layer 7 : 68/768 type remove
Layer 8 : 80/768 type remove
Layer 9 : 101/768 type remove
Layer 10 : 117/768 type remove
Layer 11 : 56/768 type remove
Layer 12 : 83/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 01:34:30,487 [trainer.py] => Time:268.4244043827057
2994 2994
2994 2994
2024-08-12 01:34:36,933 [trainer.py] => Time:6.445671558380127
2024-08-12 01:34:36,933 [inflora.py] => Exemplar size: 0
2024-08-12 01:34:36,933 [trainer.py] => CNN: {'total': 73.85, '00-09': 77.0, '10-19': 81.0, '20-29': 68.5, '30-39': 68.5, '40-49': 75.5, '50-59': 78.39, '60-69': 84.0, '70-79': 81.5, '80-89': 68.34, '90-99': 67.84, '100-109': 74.37, '110-119': 63.82, '120-129': 75.0, '130-139': 73.5, '140-149': 70.35, 'old': 74.1, 'new': 70.35}
2024-08-12 01:34:36,933 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68, 74.49, 73.85]
2024-08-12 01:34:36,933 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11, 97.89, 97.76]
2024-08-12 01:34:36,933 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734, 0.7456171735241502, 0.7391449565798264]
2024-08-12 01:34:37,473 [trainer.py] => All params: 112239531
2024-08-12 01:34:37,477 [trainer.py] => Trainable params: 81418
2024-08-12 01:34:37,477 [inflora.py] => Learning on 150-160
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_k.15.weight', 'image_encoder.blocks.0.attn.lora_B_v.15.weight', 'image_encoder.blocks.1.attn.lora_B_v.15.weight', 'image_encoder.blocks.3.attn.lora_B_v.15.weight', 'image_encoder.blocks.9.attn.lora_B_v.15.weight', 'image_encoder.blocks.3.attn.lora_B_k.15.weight', 'classifier_pool.15.weight', 'image_encoder.blocks.1.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_v.15.weight', 'image_encoder.blocks.4.attn.lora_B_k.15.weight', 'image_encoder.blocks.7.attn.lora_B_v.15.weight', 'image_encoder.blocks.2.attn.lora_B_k.15.weight', 'image_encoder.blocks.8.attn.lora_B_k.15.weight', 'classifier_pool.15.bias', 'image_encoder.blocks.0.attn.lora_B_k.15.weight', 'image_encoder.blocks.11.attn.lora_B_v.15.weight', 'image_encoder.blocks.2.attn.lora_B_v.15.weight', 'image_encoder.blocks.9.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_k.15.weight', 'image_encoder.blocks.10.attn.lora_B_k.15.weight', 'image_encoder.blocks.4.attn.lora_B_v.15.weight', 'image_encoder.blocks.5.attn.lora_B_v.15.weight', 'image_encoder.blocks.6.attn.lora_B_k.15.weight', 'image_encoder.blocks.11.attn.lora_B_k.15.weight', 'image_encoder.blocks.6.attn.lora_B_v.15.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 15, Epoch 1/20 => Loss 0.488, Train_accy 84.38:   0%|          | 0/20 [00:11<?, ?it/s]
Task 15, Epoch 1/20 => Loss 0.488, Train_accy 84.38:   5%|▌         | 1/20 [00:11<03:38, 11.50s/it]
Task 15, Epoch 2/20 => Loss 0.165, Train_accy 94.15:   5%|▌         | 1/20 [00:23<03:38, 11.50s/it]
Task 15, Epoch 2/20 => Loss 0.165, Train_accy 94.15:  10%|█         | 2/20 [00:23<03:30, 11.70s/it]
Task 15, Epoch 3/20 => Loss 0.160, Train_accy 94.25:  10%|█         | 2/20 [00:35<03:30, 11.70s/it]
Task 15, Epoch 3/20 => Loss 0.160, Train_accy 94.25:  15%|█▌        | 3/20 [00:35<03:19, 11.74s/it]
Task 15, Epoch 4/20 => Loss 0.129, Train_accy 95.39:  15%|█▌        | 3/20 [00:46<03:19, 11.74s/it]
Task 15, Epoch 4/20 => Loss 0.129, Train_accy 95.39:  20%|██        | 4/20 [00:46<03:08, 11.76s/it]
Task 15, Epoch 5/20 => Loss 0.127, Train_accy 95.52:  20%|██        | 4/20 [00:58<03:08, 11.76s/it]
Task 15, Epoch 5/20 => Loss 0.127, Train_accy 95.52:  25%|██▌       | 5/20 [00:58<02:56, 11.77s/it]
Task 15, Epoch 6/20 => Loss 0.114, Train_accy 95.85:  25%|██▌       | 5/20 [01:10<02:56, 11.77s/it]
Task 15, Epoch 6/20 => Loss 0.114, Train_accy 95.85:  30%|███       | 6/20 [01:10<02:44, 11.75s/it]
Task 15, Epoch 7/20 => Loss 0.109, Train_accy 96.31:  30%|███       | 6/20 [01:22<02:44, 11.75s/it]
Task 15, Epoch 7/20 => Loss 0.109, Train_accy 96.31:  35%|███▌      | 7/20 [01:22<02:32, 11.73s/it]
Task 15, Epoch 8/20 => Loss 0.096, Train_accy 96.90:  35%|███▌      | 7/20 [01:33<02:32, 11.73s/it]
Task 15, Epoch 8/20 => Loss 0.096, Train_accy 96.90:  40%|████      | 8/20 [01:33<02:20, 11.72s/it]
Task 15, Epoch 9/20 => Loss 0.118, Train_accy 95.88:  40%|████      | 8/20 [01:45<02:20, 11.72s/it]
Task 15, Epoch 9/20 => Loss 0.118, Train_accy 95.88:  45%|████▌     | 9/20 [01:45<02:08, 11.68s/it]
Task 15, Epoch 10/20 => Loss 0.091, Train_accy 96.83:  45%|████▌     | 9/20 [01:57<02:08, 11.68s/it]
Task 15, Epoch 10/20 => Loss 0.091, Train_accy 96.83:  50%|█████     | 10/20 [01:57<01:57, 11.71s/it]
Task 15, Epoch 11/20 => Loss 0.093, Train_accy 96.73:  50%|█████     | 10/20 [02:08<01:57, 11.71s/it]
Task 15, Epoch 11/20 => Loss 0.093, Train_accy 96.73:  55%|█████▌    | 11/20 [02:08<01:45, 11.74s/it]
Task 15, Epoch 12/20 => Loss 0.080, Train_accy 97.06:  55%|█████▌    | 11/20 [02:20<01:45, 11.74s/it]
Task 15, Epoch 12/20 => Loss 0.080, Train_accy 97.06:  60%|██████    | 12/20 [02:20<01:33, 11.71s/it]
Task 15, Epoch 13/20 => Loss 0.102, Train_accy 96.41:  60%|██████    | 12/20 [02:32<01:33, 11.71s/it]
Task 15, Epoch 13/20 => Loss 0.102, Train_accy 96.41:  65%|██████▌   | 13/20 [02:32<01:21, 11.68s/it]
Task 15, Epoch 14/20 => Loss 0.080, Train_accy 97.29:  65%|██████▌   | 13/20 [02:43<01:21, 11.68s/it]
Task 15, Epoch 14/20 => Loss 0.080, Train_accy 97.29:  70%|███████   | 14/20 [02:43<01:10, 11.67s/it]
Task 15, Epoch 15/20 => Loss 0.084, Train_accy 96.99:  70%|███████   | 14/20 [02:55<01:10, 11.67s/it]
Task 15, Epoch 15/20 => Loss 0.084, Train_accy 96.99:  75%|███████▌  | 15/20 [02:55<00:58, 11.70s/it]
Task 15, Epoch 16/20 => Loss 0.083, Train_accy 97.03:  75%|███████▌  | 15/20 [03:07<00:58, 11.70s/it]
Task 15, Epoch 16/20 => Loss 0.083, Train_accy 97.03:  80%|████████  | 16/20 [03:07<00:46, 11.74s/it]
Task 15, Epoch 17/20 => Loss 0.086, Train_accy 96.76:  80%|████████  | 16/20 [03:19<00:46, 11.74s/it]
Task 15, Epoch 17/20 => Loss 0.086, Train_accy 96.76:  85%|████████▌ | 17/20 [03:19<00:35, 11.70s/it]
Task 15, Epoch 18/20 => Loss 0.076, Train_accy 97.12:  85%|████████▌ | 17/20 [03:30<00:35, 11.70s/it]
Task 15, Epoch 18/20 => Loss 0.076, Train_accy 97.12:  90%|█████████ | 18/20 [03:30<00:23, 11.74s/it]
Task 15, Epoch 19/20 => Loss 0.072, Train_accy 97.52:  90%|█████████ | 18/20 [03:42<00:23, 11.74s/it]
Task 15, Epoch 19/20 => Loss 0.072, Train_accy 97.52:  95%|█████████▌| 19/20 [03:42<00:11, 11.77s/it]
Task 15, Epoch 20/20 => Loss 0.082, Train_accy 97.32:  95%|█████████▌| 19/20 [03:54<00:11, 11.77s/it]
Task 15, Epoch 20/20 => Loss 0.082, Train_accy 97.32: 100%|██████████| 20/20 [03:54<00:00, 11.72s/it]
Task 15, Epoch 20/20 => Loss 0.082, Train_accy 97.32: 100%|██████████| 20/20 [03:54<00:00, 11.72s/it]
2024-08-12 01:38:43,452 [inflora.py] => Task 15, Epoch 20/20 => Loss 0.082, Train_accy 97.32
Threshold:  0.975
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 15/768 type remove
Layer 3 : 40/768 type remove
Layer 4 : 43/768 type remove
Layer 5 : 65/768 type remove
Layer 6 : 74/768 type remove
Layer 7 : 76/768 type remove
Layer 8 : 91/768 type remove
Layer 9 : 114/768 type remove
Layer 10 : 126/768 type remove
Layer 11 : 63/768 type remove
Layer 12 : 88/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 01:39:03,770 [trainer.py] => Time:266.2923517227173
3193 3193
3193 3193
2024-08-12 01:39:10,718 [trainer.py] => Time:6.947611570358276
2024-08-12 01:39:10,718 [inflora.py] => Exemplar size: 0
2024-08-12 01:39:10,718 [trainer.py] => CNN: {'total': 72.44, '00-09': 76.5, '10-19': 80.5, '20-29': 66.0, '30-39': 68.5, '40-49': 71.0, '50-59': 80.4, '60-69': 83.0, '70-79': 80.0, '80-89': 69.85, '90-99': 65.33, '100-109': 71.36, '110-119': 59.8, '120-129': 73.0, '130-139': 73.0, '140-149': 67.34, '150-159': 73.37, 'old': 72.38, 'new': 73.37}
2024-08-12 01:39:10,719 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68, 74.49, 73.85, 72.44]
2024-08-12 01:39:10,719 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11, 97.89, 97.76, 97.56]
2024-08-12 01:39:10,719 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734, 0.7456171735241502, 0.7391449565798264, 0.7256498590667084]
2024-08-12 01:39:11,259 [trainer.py] => All params: 112239531
2024-08-12 01:39:11,264 [trainer.py] => Trainable params: 81418
2024-08-12 01:39:11,264 [inflora.py] => Learning on 160-170
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.16.weight', 'image_encoder.blocks.3.attn.lora_B_k.16.weight', 'image_encoder.blocks.0.attn.lora_B_v.16.weight', 'image_encoder.blocks.8.attn.lora_B_k.16.weight', 'image_encoder.blocks.3.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_v.16.weight', 'image_encoder.blocks.5.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_v.16.weight', 'image_encoder.blocks.11.attn.lora_B_k.16.weight', 'image_encoder.blocks.4.attn.lora_B_k.16.weight', 'classifier_pool.16.bias', 'image_encoder.blocks.2.attn.lora_B_k.16.weight', 'classifier_pool.16.weight', 'image_encoder.blocks.6.attn.lora_B_v.16.weight', 'image_encoder.blocks.10.attn.lora_B_v.16.weight', 'image_encoder.blocks.1.attn.lora_B_v.16.weight', 'image_encoder.blocks.2.attn.lora_B_v.16.weight', 'image_encoder.blocks.10.attn.lora_B_k.16.weight', 'image_encoder.blocks.7.attn.lora_B_k.16.weight', 'image_encoder.blocks.5.attn.lora_B_v.16.weight', 'image_encoder.blocks.4.attn.lora_B_v.16.weight', 'image_encoder.blocks.0.attn.lora_B_k.16.weight', 'image_encoder.blocks.7.attn.lora_B_v.16.weight', 'image_encoder.blocks.1.attn.lora_B_k.16.weight', 'image_encoder.blocks.6.attn.lora_B_k.16.weight', 'image_encoder.blocks.9.attn.lora_B_k.16.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 16, Epoch 1/20 => Loss 0.559, Train_accy 81.90:   0%|          | 0/20 [00:11<?, ?it/s]
Task 16, Epoch 1/20 => Loss 0.559, Train_accy 81.90:   5%|▌         | 1/20 [00:11<03:35, 11.34s/it]
Task 16, Epoch 2/20 => Loss 0.159, Train_accy 93.92:   5%|▌         | 1/20 [00:22<03:35, 11.34s/it]
Task 16, Epoch 2/20 => Loss 0.159, Train_accy 93.92:  10%|█         | 2/20 [00:22<03:24, 11.36s/it]
Task 16, Epoch 3/20 => Loss 0.136, Train_accy 94.82:  10%|█         | 2/20 [00:34<03:24, 11.36s/it]
Task 16, Epoch 3/20 => Loss 0.136, Train_accy 94.82:  15%|█▌        | 3/20 [00:34<03:13, 11.37s/it]
Task 16, Epoch 4/20 => Loss 0.109, Train_accy 95.66:  15%|█▌        | 3/20 [00:45<03:13, 11.37s/it]
Task 16, Epoch 4/20 => Loss 0.109, Train_accy 95.66:  20%|██        | 4/20 [00:45<03:03, 11.46s/it]
Task 16, Epoch 5/20 => Loss 0.093, Train_accy 96.73:  20%|██        | 4/20 [00:57<03:03, 11.46s/it]
Task 16, Epoch 5/20 => Loss 0.093, Train_accy 96.73:  25%|██▌       | 5/20 [00:57<02:51, 11.44s/it]
Task 16, Epoch 6/20 => Loss 0.088, Train_accy 96.89:  25%|██▌       | 5/20 [01:08<02:51, 11.44s/it]
Task 16, Epoch 6/20 => Loss 0.088, Train_accy 96.89:  30%|███       | 6/20 [01:08<02:40, 11.44s/it]
Task 16, Epoch 7/20 => Loss 0.088, Train_accy 96.83:  30%|███       | 6/20 [01:19<02:40, 11.44s/it]
Task 16, Epoch 7/20 => Loss 0.088, Train_accy 96.83:  35%|███▌      | 7/20 [01:19<02:28, 11.44s/it]
Task 16, Epoch 8/20 => Loss 0.082, Train_accy 96.73:  35%|███▌      | 7/20 [01:31<02:28, 11.44s/it]
Task 16, Epoch 8/20 => Loss 0.082, Train_accy 96.73:  40%|████      | 8/20 [01:31<02:17, 11.46s/it]
Task 16, Epoch 9/20 => Loss 0.078, Train_accy 96.96:  40%|████      | 8/20 [01:43<02:17, 11.46s/it]
Task 16, Epoch 9/20 => Loss 0.078, Train_accy 96.96:  45%|████▌     | 9/20 [01:43<02:07, 11.61s/it]
Task 16, Epoch 10/20 => Loss 0.064, Train_accy 97.66:  45%|████▌     | 9/20 [01:54<02:07, 11.61s/it]
Task 16, Epoch 10/20 => Loss 0.064, Train_accy 97.66:  50%|█████     | 10/20 [01:54<01:55, 11.55s/it]
Task 16, Epoch 11/20 => Loss 0.065, Train_accy 97.50:  50%|█████     | 10/20 [02:06<01:55, 11.55s/it]
Task 16, Epoch 11/20 => Loss 0.065, Train_accy 97.50:  55%|█████▌    | 11/20 [02:06<01:43, 11.54s/it]
Task 16, Epoch 12/20 => Loss 0.071, Train_accy 97.10:  55%|█████▌    | 11/20 [02:17<01:43, 11.54s/it]
Task 16, Epoch 12/20 => Loss 0.071, Train_accy 97.10:  60%|██████    | 12/20 [02:17<01:32, 11.52s/it]
Task 16, Epoch 13/20 => Loss 0.061, Train_accy 97.90:  60%|██████    | 12/20 [02:29<01:32, 11.52s/it]
Task 16, Epoch 13/20 => Loss 0.061, Train_accy 97.90:  65%|██████▌   | 13/20 [02:29<01:20, 11.52s/it]
Task 16, Epoch 14/20 => Loss 0.071, Train_accy 97.50:  65%|██████▌   | 13/20 [02:40<01:20, 11.52s/it]
Task 16, Epoch 14/20 => Loss 0.071, Train_accy 97.50:  70%|███████   | 14/20 [02:40<01:09, 11.52s/it]
Task 16, Epoch 15/20 => Loss 0.058, Train_accy 97.66:  70%|███████   | 14/20 [02:52<01:09, 11.52s/it]
Task 16, Epoch 15/20 => Loss 0.058, Train_accy 97.66:  75%|███████▌  | 15/20 [02:52<00:57, 11.53s/it]
Task 16, Epoch 16/20 => Loss 0.072, Train_accy 97.10:  75%|███████▌  | 15/20 [03:03<00:57, 11.53s/it]
Task 16, Epoch 16/20 => Loss 0.072, Train_accy 97.10:  80%|████████  | 16/20 [03:03<00:46, 11.54s/it]
Task 16, Epoch 17/20 => Loss 0.056, Train_accy 97.83:  80%|████████  | 16/20 [03:15<00:46, 11.54s/it]
Task 16, Epoch 17/20 => Loss 0.056, Train_accy 97.83:  85%|████████▌ | 17/20 [03:15<00:34, 11.54s/it]
Task 16, Epoch 18/20 => Loss 0.061, Train_accy 97.80:  85%|████████▌ | 17/20 [03:27<00:34, 11.54s/it]
Task 16, Epoch 18/20 => Loss 0.061, Train_accy 97.80:  90%|█████████ | 18/20 [03:27<00:23, 11.53s/it]
Task 16, Epoch 19/20 => Loss 0.053, Train_accy 97.96:  90%|█████████ | 18/20 [03:38<00:23, 11.53s/it]
Task 16, Epoch 19/20 => Loss 0.053, Train_accy 97.96:  95%|█████████▌| 19/20 [03:38<00:11, 11.49s/it]
Task 16, Epoch 20/20 => Loss 0.059, Train_accy 97.96:  95%|█████████▌| 19/20 [03:50<00:11, 11.49s/it]
Task 16, Epoch 20/20 => Loss 0.059, Train_accy 97.96: 100%|██████████| 20/20 [03:50<00:00, 11.52s/it]
Task 16, Epoch 20/20 => Loss 0.059, Train_accy 97.96: 100%|██████████| 20/20 [03:50<00:00, 11.50s/it]
2024-08-12 01:43:12,563 [inflora.py] => Task 16, Epoch 20/20 => Loss 0.059, Train_accy 97.96
Threshold:  0.9766666666666667
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 45/768 type remove
Layer 4 : 48/768 type remove
Layer 5 : 72/768 type remove
Layer 6 : 79/768 type remove
Layer 7 : 81/768 type remove
Layer 8 : 96/768 type remove
Layer 9 : 129/768 type remove
Layer 10 : 147/768 type remove
Layer 11 : 75/768 type remove
Layer 12 : 94/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 01:43:32,736 [trainer.py] => Time:261.47241711616516
3392 3392
3392 3392
2024-08-12 01:43:39,879 [trainer.py] => Time:7.142471790313721
2024-08-12 01:43:39,880 [inflora.py] => Exemplar size: 0
2024-08-12 01:43:39,880 [trainer.py] => CNN: {'total': 71.08, '00-09': 75.0, '10-19': 78.5, '20-29': 67.0, '30-39': 66.5, '40-49': 72.0, '50-59': 75.88, '60-69': 83.5, '70-79': 74.5, '80-89': 66.33, '90-99': 65.33, '100-109': 73.87, '110-119': 60.8, '120-129': 73.0, '130-139': 71.5, '140-149': 66.33, '150-159': 72.36, '160-169': 65.83, 'old': 71.41, 'new': 65.83}
2024-08-12 01:43:39,880 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68, 74.49, 73.85, 72.44, 71.08]
2024-08-12 01:43:39,880 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11, 97.89, 97.76, 97.56, 97.7]
2024-08-12 01:43:39,880 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734, 0.7456171735241502, 0.7391449565798264, 0.7256498590667084, 0.7122641509433962]
2024-08-12 01:43:40,420 [trainer.py] => All params: 112239531
2024-08-12 01:43:40,424 [trainer.py] => Trainable params: 81418
2024-08-12 01:43:40,424 [inflora.py] => Learning on 170-180
Parameters to be updated: {'image_encoder.blocks.9.attn.lora_B_v.17.weight', 'image_encoder.blocks.8.attn.lora_B_k.17.weight', 'image_encoder.blocks.1.attn.lora_B_v.17.weight', 'classifier_pool.17.bias', 'classifier_pool.17.weight', 'image_encoder.blocks.5.attn.lora_B_v.17.weight', 'image_encoder.blocks.6.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_k.17.weight', 'image_encoder.blocks.6.attn.lora_B_v.17.weight', 'image_encoder.blocks.8.attn.lora_B_v.17.weight', 'image_encoder.blocks.0.attn.lora_B_v.17.weight', 'image_encoder.blocks.5.attn.lora_B_k.17.weight', 'image_encoder.blocks.2.attn.lora_B_v.17.weight', 'image_encoder.blocks.3.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_v.17.weight', 'image_encoder.blocks.7.attn.lora_B_k.17.weight', 'image_encoder.blocks.1.attn.lora_B_k.17.weight', 'image_encoder.blocks.10.attn.lora_B_v.17.weight', 'image_encoder.blocks.3.attn.lora_B_k.17.weight', 'image_encoder.blocks.2.attn.lora_B_k.17.weight', 'image_encoder.blocks.7.attn.lora_B_v.17.weight', 'image_encoder.blocks.4.attn.lora_B_k.17.weight', 'image_encoder.blocks.0.attn.lora_B_k.17.weight', 'image_encoder.blocks.9.attn.lora_B_k.17.weight', 'image_encoder.blocks.11.attn.lora_B_v.17.weight', 'image_encoder.blocks.10.attn.lora_B_k.17.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 17, Epoch 1/20 => Loss 0.581, Train_accy 81.42:   0%|          | 0/20 [00:10<?, ?it/s]
Task 17, Epoch 1/20 => Loss 0.581, Train_accy 81.42:   5%|▌         | 1/20 [00:10<03:24, 10.74s/it]
Task 17, Epoch 2/20 => Loss 0.156, Train_accy 94.39:   5%|▌         | 1/20 [00:21<03:24, 10.74s/it]
Task 17, Epoch 2/20 => Loss 0.156, Train_accy 94.39:  10%|█         | 2/20 [00:21<03:15, 10.85s/it]
Task 17, Epoch 3/20 => Loss 0.122, Train_accy 95.60:  10%|█         | 2/20 [00:32<03:15, 10.85s/it]
Task 17, Epoch 3/20 => Loss 0.122, Train_accy 95.60:  15%|█▌        | 3/20 [00:32<03:04, 10.83s/it]
Task 17, Epoch 4/20 => Loss 0.107, Train_accy 96.16:  15%|█▌        | 3/20 [00:43<03:04, 10.83s/it]
Task 17, Epoch 4/20 => Loss 0.107, Train_accy 96.16:  20%|██        | 4/20 [00:43<02:53, 10.83s/it]
Task 17, Epoch 5/20 => Loss 0.086, Train_accy 97.05:  20%|██        | 4/20 [00:54<02:53, 10.83s/it]
Task 17, Epoch 5/20 => Loss 0.086, Train_accy 97.05:  25%|██▌       | 5/20 [00:54<02:42, 10.83s/it]
Task 17, Epoch 6/20 => Loss 0.079, Train_accy 96.98:  25%|██▌       | 5/20 [01:05<02:42, 10.83s/it]
Task 17, Epoch 6/20 => Loss 0.079, Train_accy 96.98:  30%|███       | 6/20 [01:05<02:32, 10.89s/it]
Task 17, Epoch 7/20 => Loss 0.069, Train_accy 97.55:  30%|███       | 6/20 [01:16<02:32, 10.89s/it]
Task 17, Epoch 7/20 => Loss 0.069, Train_accy 97.55:  35%|███▌      | 7/20 [01:16<02:21, 10.88s/it]
Task 17, Epoch 8/20 => Loss 0.062, Train_accy 97.83:  35%|███▌      | 7/20 [01:26<02:21, 10.88s/it]
Task 17, Epoch 8/20 => Loss 0.062, Train_accy 97.83:  40%|████      | 8/20 [01:26<02:10, 10.86s/it]
Task 17, Epoch 9/20 => Loss 0.067, Train_accy 97.76:  40%|████      | 8/20 [01:37<02:10, 10.86s/it]
Task 17, Epoch 9/20 => Loss 0.067, Train_accy 97.76:  45%|████▌     | 9/20 [01:37<01:59, 10.84s/it]
Task 17, Epoch 10/20 => Loss 0.063, Train_accy 97.83:  45%|████▌     | 9/20 [01:48<01:59, 10.84s/it]
Task 17, Epoch 10/20 => Loss 0.063, Train_accy 97.83:  50%|█████     | 10/20 [01:48<01:48, 10.84s/it]
Task 17, Epoch 11/20 => Loss 0.064, Train_accy 97.51:  50%|█████     | 10/20 [01:59<01:48, 10.84s/it]
Task 17, Epoch 11/20 => Loss 0.064, Train_accy 97.51:  55%|█████▌    | 11/20 [01:59<01:38, 10.89s/it]
Task 17, Epoch 12/20 => Loss 0.072, Train_accy 97.62:  55%|█████▌    | 11/20 [02:10<01:38, 10.89s/it]
Task 17, Epoch 12/20 => Loss 0.072, Train_accy 97.62:  60%|██████    | 12/20 [02:10<01:26, 10.86s/it]
Task 17, Epoch 13/20 => Loss 0.054, Train_accy 98.15:  60%|██████    | 12/20 [02:21<01:26, 10.86s/it]
Task 17, Epoch 13/20 => Loss 0.054, Train_accy 98.15:  65%|██████▌   | 13/20 [02:21<01:15, 10.85s/it]
Task 17, Epoch 14/20 => Loss 0.051, Train_accy 98.54:  65%|██████▌   | 13/20 [02:31<01:15, 10.85s/it]
Task 17, Epoch 14/20 => Loss 0.051, Train_accy 98.54:  70%|███████   | 14/20 [02:31<01:05, 10.85s/it]
Task 17, Epoch 15/20 => Loss 0.047, Train_accy 98.40:  70%|███████   | 14/20 [02:42<01:05, 10.85s/it]
Task 17, Epoch 15/20 => Loss 0.047, Train_accy 98.40:  75%|███████▌  | 15/20 [02:42<00:54, 10.87s/it]
Task 17, Epoch 16/20 => Loss 0.048, Train_accy 98.29:  75%|███████▌  | 15/20 [02:53<00:54, 10.87s/it]
Task 17, Epoch 16/20 => Loss 0.048, Train_accy 98.29:  80%|████████  | 16/20 [02:53<00:43, 10.88s/it]
Task 17, Epoch 17/20 => Loss 0.048, Train_accy 98.47:  80%|████████  | 16/20 [03:04<00:43, 10.88s/it]
Task 17, Epoch 17/20 => Loss 0.048, Train_accy 98.47:  85%|████████▌ | 17/20 [03:04<00:32, 10.92s/it]
Task 17, Epoch 18/20 => Loss 0.051, Train_accy 98.19:  85%|████████▌ | 17/20 [03:15<00:32, 10.92s/it]
Task 17, Epoch 18/20 => Loss 0.051, Train_accy 98.19:  90%|█████████ | 18/20 [03:15<00:21, 10.94s/it]
Task 17, Epoch 19/20 => Loss 0.042, Train_accy 98.58:  90%|█████████ | 18/20 [03:26<00:21, 10.94s/it]
Task 17, Epoch 19/20 => Loss 0.042, Train_accy 98.58:  95%|█████████▌| 19/20 [03:26<00:10, 10.91s/it]
Task 17, Epoch 20/20 => Loss 0.052, Train_accy 98.40:  95%|█████████▌| 19/20 [03:37<00:10, 10.91s/it]
Task 17, Epoch 20/20 => Loss 0.052, Train_accy 98.40: 100%|██████████| 20/20 [03:37<00:00, 10.89s/it]
Task 17, Epoch 20/20 => Loss 0.052, Train_accy 98.40: 100%|██████████| 20/20 [03:37<00:00, 10.87s/it]
2024-08-12 01:47:28,650 [inflora.py] => Task 17, Epoch 20/20 => Loss 0.052, Train_accy 98.40
Threshold:  0.9783333333333333
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 46/768 type remove
Layer 4 : 50/768 type remove
Layer 5 : 74/768 type remove
Layer 6 : 82/768 type remove
Layer 7 : 86/768 type remove
Layer 8 : 100/768 type remove
Layer 9 : 136/768 type remove
Layer 10 : 159/768 type remove
Layer 11 : 82/768 type remove
Layer 12 : 100/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 01:47:47,995 [trainer.py] => Time:247.5706753730774
3592 3592
3592 3592
2024-08-12 01:47:55,530 [trainer.py] => Time:7.534565687179565
2024-08-12 01:47:55,530 [inflora.py] => Exemplar size: 0
2024-08-12 01:47:55,530 [trainer.py] => CNN: {'total': 69.46, '00-09': 77.5, '10-19': 76.0, '20-29': 67.0, '30-39': 65.5, '40-49': 72.0, '50-59': 75.38, '60-69': 84.0, '70-79': 73.0, '80-89': 66.33, '90-99': 63.32, '100-109': 72.36, '110-119': 60.3, '120-129': 74.5, '130-139': 72.0, '140-149': 66.33, '150-159': 70.85, '160-169': 64.32, '170-179': 49.5, 'old': 70.64, 'new': 49.5}
2024-08-12 01:47:55,530 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68, 74.49, 73.85, 72.44, 71.08, 69.46]
2024-08-12 01:47:55,530 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11, 97.89, 97.76, 97.56, 97.7, 97.83]
2024-08-12 01:47:55,530 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734, 0.7456171735241502, 0.7391449565798264, 0.7256498590667084, 0.7122641509433962, 0.6959910913140311]
2024-08-12 01:47:56,083 [trainer.py] => All params: 112239531
2024-08-12 01:47:56,087 [trainer.py] => Trainable params: 81418
2024-08-12 01:47:56,087 [inflora.py] => Learning on 180-190
Parameters to be updated: {'classifier_pool.18.weight', 'image_encoder.blocks.5.attn.lora_B_v.18.weight', 'image_encoder.blocks.8.attn.lora_B_k.18.weight', 'image_encoder.blocks.8.attn.lora_B_v.18.weight', 'image_encoder.blocks.2.attn.lora_B_k.18.weight', 'image_encoder.blocks.3.attn.lora_B_v.18.weight', 'image_encoder.blocks.11.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_v.18.weight', 'image_encoder.blocks.2.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_k.18.weight', 'image_encoder.blocks.1.attn.lora_B_v.18.weight', 'classifier_pool.18.bias', 'image_encoder.blocks.4.attn.lora_B_v.18.weight', 'image_encoder.blocks.11.attn.lora_B_v.18.weight', 'image_encoder.blocks.10.attn.lora_B_k.18.weight', 'image_encoder.blocks.0.attn.lora_B_k.18.weight', 'image_encoder.blocks.5.attn.lora_B_k.18.weight', 'image_encoder.blocks.1.attn.lora_B_k.18.weight', 'image_encoder.blocks.6.attn.lora_B_k.18.weight', 'image_encoder.blocks.9.attn.lora_B_k.18.weight', 'image_encoder.blocks.4.attn.lora_B_k.18.weight', 'image_encoder.blocks.6.attn.lora_B_v.18.weight', 'image_encoder.blocks.9.attn.lora_B_v.18.weight', 'image_encoder.blocks.7.attn.lora_B_v.18.weight', 'image_encoder.blocks.3.attn.lora_B_k.18.weight', 'image_encoder.blocks.10.attn.lora_B_v.18.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 18, Epoch 1/20 => Loss 0.435, Train_accy 85.39:   0%|          | 0/20 [00:11<?, ?it/s]
Task 18, Epoch 1/20 => Loss 0.435, Train_accy 85.39:   5%|▌         | 1/20 [00:11<03:35, 11.33s/it]
Task 18, Epoch 2/20 => Loss 0.104, Train_accy 96.52:   5%|▌         | 1/20 [00:22<03:35, 11.33s/it]
Task 18, Epoch 2/20 => Loss 0.104, Train_accy 96.52:  10%|█         | 2/20 [00:22<03:23, 11.30s/it]
Task 18, Epoch 3/20 => Loss 0.066, Train_accy 97.63:  10%|█         | 2/20 [00:33<03:23, 11.30s/it]
Task 18, Epoch 3/20 => Loss 0.066, Train_accy 97.63:  15%|█▌        | 3/20 [00:33<03:12, 11.32s/it]
Task 18, Epoch 4/20 => Loss 0.068, Train_accy 97.53:  15%|█▌        | 3/20 [00:45<03:12, 11.32s/it]
Task 18, Epoch 4/20 => Loss 0.068, Train_accy 97.53:  20%|██        | 4/20 [00:45<03:02, 11.40s/it]
Task 18, Epoch 5/20 => Loss 0.064, Train_accy 97.80:  20%|██        | 4/20 [00:56<03:02, 11.40s/it]
Task 18, Epoch 5/20 => Loss 0.064, Train_accy 97.80:  25%|██▌       | 5/20 [00:56<02:50, 11.37s/it]
Task 18, Epoch 6/20 => Loss 0.068, Train_accy 97.40:  25%|██▌       | 5/20 [01:08<02:50, 11.37s/it]
Task 18, Epoch 6/20 => Loss 0.068, Train_accy 97.40:  30%|███       | 6/20 [01:08<02:39, 11.42s/it]
Task 18, Epoch 7/20 => Loss 0.058, Train_accy 98.14:  30%|███       | 6/20 [01:19<02:39, 11.42s/it]
Task 18, Epoch 7/20 => Loss 0.058, Train_accy 98.14:  35%|███▌      | 7/20 [01:19<02:28, 11.42s/it]
Task 18, Epoch 8/20 => Loss 0.062, Train_accy 97.94:  35%|███▌      | 7/20 [01:31<02:28, 11.42s/it]
Task 18, Epoch 8/20 => Loss 0.062, Train_accy 97.94:  40%|████      | 8/20 [01:31<02:17, 11.45s/it]
Task 18, Epoch 9/20 => Loss 0.052, Train_accy 98.07:  40%|████      | 8/20 [01:42<02:17, 11.45s/it]
Task 18, Epoch 9/20 => Loss 0.052, Train_accy 98.07:  45%|████▌     | 9/20 [01:42<02:05, 11.41s/it]
Task 18, Epoch 10/20 => Loss 0.052, Train_accy 98.14:  45%|████▌     | 9/20 [01:54<02:05, 11.41s/it]
Task 18, Epoch 10/20 => Loss 0.052, Train_accy 98.14:  50%|█████     | 10/20 [01:54<01:54, 11.43s/it]
Task 18, Epoch 11/20 => Loss 0.045, Train_accy 98.44:  50%|█████     | 10/20 [02:05<01:54, 11.43s/it]
Task 18, Epoch 11/20 => Loss 0.045, Train_accy 98.44:  55%|█████▌    | 11/20 [02:05<01:42, 11.41s/it]
Task 18, Epoch 12/20 => Loss 0.046, Train_accy 98.41:  55%|█████▌    | 11/20 [02:16<01:42, 11.41s/it]
Task 18, Epoch 12/20 => Loss 0.046, Train_accy 98.41:  60%|██████    | 12/20 [02:16<01:31, 11.43s/it]
Task 18, Epoch 13/20 => Loss 0.056, Train_accy 97.97:  60%|██████    | 12/20 [02:28<01:31, 11.43s/it]
Task 18, Epoch 13/20 => Loss 0.056, Train_accy 97.97:  65%|██████▌   | 13/20 [02:28<01:19, 11.39s/it]
Task 18, Epoch 14/20 => Loss 0.044, Train_accy 98.48:  65%|██████▌   | 13/20 [02:39<01:19, 11.39s/it]
Task 18, Epoch 14/20 => Loss 0.044, Train_accy 98.48:  70%|███████   | 14/20 [02:39<01:08, 11.40s/it]
Task 18, Epoch 15/20 => Loss 0.042, Train_accy 98.41:  70%|███████   | 14/20 [02:50<01:08, 11.40s/it]
Task 18, Epoch 15/20 => Loss 0.042, Train_accy 98.41:  75%|███████▌  | 15/20 [02:50<00:56, 11.38s/it]
Task 18, Epoch 16/20 => Loss 0.040, Train_accy 98.51:  75%|███████▌  | 15/20 [03:02<00:56, 11.38s/it]
Task 18, Epoch 16/20 => Loss 0.040, Train_accy 98.51:  80%|████████  | 16/20 [03:02<00:45, 11.37s/it]
Task 18, Epoch 17/20 => Loss 0.039, Train_accy 98.58:  80%|████████  | 16/20 [03:13<00:45, 11.37s/it]
Task 18, Epoch 17/20 => Loss 0.039, Train_accy 98.58:  85%|████████▌ | 17/20 [03:13<00:34, 11.39s/it]
Task 18, Epoch 18/20 => Loss 0.039, Train_accy 98.88:  85%|████████▌ | 17/20 [03:25<00:34, 11.39s/it]
Task 18, Epoch 18/20 => Loss 0.039, Train_accy 98.88:  90%|█████████ | 18/20 [03:25<00:22, 11.41s/it]
Task 18, Epoch 19/20 => Loss 0.050, Train_accy 98.34:  90%|█████████ | 18/20 [03:36<00:22, 11.41s/it]
Task 18, Epoch 19/20 => Loss 0.050, Train_accy 98.34:  95%|█████████▌| 19/20 [03:36<00:11, 11.44s/it]
Task 18, Epoch 20/20 => Loss 0.048, Train_accy 98.44:  95%|█████████▌| 19/20 [03:48<00:11, 11.44s/it]
Task 18, Epoch 20/20 => Loss 0.048, Train_accy 98.44: 100%|██████████| 20/20 [03:48<00:00, 11.40s/it]
Task 18, Epoch 20/20 => Loss 0.048, Train_accy 98.44: 100%|██████████| 20/20 [03:48<00:00, 11.40s/it]
2024-08-12 01:51:55,358 [inflora.py] => Task 18, Epoch 20/20 => Loss 0.048, Train_accy 98.44
Threshold:  0.98
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 17/768 type remove
Layer 3 : 52/768 type remove
Layer 4 : 57/768 type remove
Layer 5 : 83/768 type remove
Layer 6 : 93/768 type remove
Layer 7 : 97/768 type remove
Layer 8 : 115/768 type remove
Layer 9 : 154/768 type remove
Layer 10 : 174/768 type remove
Layer 11 : 88/768 type remove
Layer 12 : 107/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 01:52:15,201 [trainer.py] => Time:259.1144196987152
3790 3790
3790 3790
2024-08-12 01:52:23,090 [trainer.py] => Time:7.888228893280029
2024-08-12 01:52:23,090 [inflora.py] => Exemplar size: 0
2024-08-12 01:52:23,090 [trainer.py] => CNN: {'total': 68.42, '00-09': 76.0, '10-19': 76.5, '20-29': 65.0, '30-39': 66.5, '40-49': 71.0, '50-59': 74.37, '60-69': 80.5, '70-79': 72.5, '80-89': 63.82, '90-99': 63.32, '100-109': 70.35, '110-119': 61.31, '120-129': 74.0, '130-139': 73.0, '140-149': 67.34, '150-159': 58.79, '160-169': 60.8, '170-179': 47.5, '180-189': 77.27, 'old': 67.93, 'new': 77.27}
2024-08-12 01:52:23,090 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68, 74.49, 73.85, 72.44, 71.08, 69.46, 68.42]
2024-08-12 01:52:23,090 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11, 97.89, 97.76, 97.56, 97.7, 97.83, 97.78]
2024-08-12 01:52:23,090 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734, 0.7456171735241502, 0.7391449565798264, 0.7256498590667084, 0.7122641509433962, 0.6959910913140311, 0.6857519788918206]
2024-08-12 01:52:23,696 [trainer.py] => All params: 112239531
2024-08-12 01:52:23,705 [trainer.py] => Trainable params: 81418
2024-08-12 01:52:23,705 [inflora.py] => Learning on 190-200
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_v.19.weight', 'image_encoder.blocks.1.attn.lora_B_v.19.weight', 'image_encoder.blocks.8.attn.lora_B_v.19.weight', 'image_encoder.blocks.0.attn.lora_B_v.19.weight', 'image_encoder.blocks.0.attn.lora_B_k.19.weight', 'image_encoder.blocks.7.attn.lora_B_v.19.weight', 'image_encoder.blocks.4.attn.lora_B_k.19.weight', 'image_encoder.blocks.10.attn.lora_B_k.19.weight', 'image_encoder.blocks.9.attn.lora_B_k.19.weight', 'image_encoder.blocks.5.attn.lora_B_v.19.weight', 'image_encoder.blocks.6.attn.lora_B_v.19.weight', 'image_encoder.blocks.2.attn.lora_B_v.19.weight', 'image_encoder.blocks.6.attn.lora_B_k.19.weight', 'image_encoder.blocks.11.attn.lora_B_k.19.weight', 'image_encoder.blocks.2.attn.lora_B_k.19.weight', 'image_encoder.blocks.5.attn.lora_B_k.19.weight', 'classifier_pool.19.weight', 'classifier_pool.19.bias', 'image_encoder.blocks.1.attn.lora_B_k.19.weight', 'image_encoder.blocks.8.attn.lora_B_k.19.weight', 'image_encoder.blocks.9.attn.lora_B_v.19.weight', 'image_encoder.blocks.7.attn.lora_B_k.19.weight', 'image_encoder.blocks.10.attn.lora_B_v.19.weight', 'image_encoder.blocks.3.attn.lora_B_k.19.weight', 'image_encoder.blocks.3.attn.lora_B_v.19.weight', 'image_encoder.blocks.11.attn.lora_B_v.19.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 19, Epoch 1/20 => Loss 0.485, Train_accy 85.74:   0%|          | 0/20 [00:11<?, ?it/s]
Task 19, Epoch 1/20 => Loss 0.485, Train_accy 85.74:   5%|▌         | 1/20 [00:11<03:30, 11.10s/it]
Task 19, Epoch 2/20 => Loss 0.098, Train_accy 96.79:   5%|▌         | 1/20 [00:22<03:30, 11.10s/it]
Task 19, Epoch 2/20 => Loss 0.098, Train_accy 96.79:  10%|█         | 2/20 [00:22<03:21, 11.19s/it]
Task 19, Epoch 3/20 => Loss 0.082, Train_accy 97.25:  10%|█         | 2/20 [00:33<03:21, 11.19s/it]
Task 19, Epoch 3/20 => Loss 0.082, Train_accy 97.25:  15%|█▌        | 3/20 [00:33<03:09, 11.15s/it]
Task 19, Epoch 4/20 => Loss 0.056, Train_accy 98.43:  15%|█▌        | 3/20 [00:44<03:09, 11.15s/it]
Task 19, Epoch 4/20 => Loss 0.056, Train_accy 98.43:  20%|██        | 4/20 [00:44<02:57, 11.11s/it]
Task 19, Epoch 5/20 => Loss 0.056, Train_accy 98.26:  20%|██        | 4/20 [00:55<02:57, 11.11s/it]
Task 19, Epoch 5/20 => Loss 0.056, Train_accy 98.26:  25%|██▌       | 5/20 [00:55<02:45, 11.07s/it]
Task 19, Epoch 6/20 => Loss 0.050, Train_accy 98.29:  25%|██▌       | 5/20 [01:06<02:45, 11.07s/it]
Task 19, Epoch 6/20 => Loss 0.050, Train_accy 98.29:  30%|███       | 6/20 [01:06<02:35, 11.07s/it]
Task 19, Epoch 7/20 => Loss 0.058, Train_accy 98.12:  30%|███       | 6/20 [01:17<02:35, 11.07s/it]
Task 19, Epoch 7/20 => Loss 0.058, Train_accy 98.12:  35%|███▌      | 7/20 [01:17<02:24, 11.09s/it]
Task 19, Epoch 8/20 => Loss 0.042, Train_accy 98.43:  35%|███▌      | 7/20 [01:28<02:24, 11.09s/it]
Task 19, Epoch 8/20 => Loss 0.042, Train_accy 98.43:  40%|████      | 8/20 [01:28<02:12, 11.07s/it]
Task 19, Epoch 9/20 => Loss 0.042, Train_accy 98.78:  40%|████      | 8/20 [01:39<02:12, 11.07s/it]
Task 19, Epoch 9/20 => Loss 0.042, Train_accy 98.78:  45%|████▌     | 9/20 [01:39<02:01, 11.07s/it]
Task 19, Epoch 10/20 => Loss 0.040, Train_accy 98.71:  45%|████▌     | 9/20 [01:50<02:01, 11.07s/it]
Task 19, Epoch 10/20 => Loss 0.040, Train_accy 98.71:  50%|█████     | 10/20 [01:50<01:51, 11.10s/it]
Task 19, Epoch 11/20 => Loss 0.050, Train_accy 98.40:  50%|█████     | 10/20 [02:02<01:51, 11.10s/it]
Task 19, Epoch 11/20 => Loss 0.050, Train_accy 98.40:  55%|█████▌    | 11/20 [02:02<01:40, 11.12s/it]
Task 19, Epoch 12/20 => Loss 0.042, Train_accy 98.50:  55%|█████▌    | 11/20 [02:13<01:40, 11.12s/it]
Task 19, Epoch 12/20 => Loss 0.042, Train_accy 98.50:  60%|██████    | 12/20 [02:13<01:28, 11.10s/it]
Task 19, Epoch 13/20 => Loss 0.046, Train_accy 98.36:  60%|██████    | 12/20 [02:24<01:28, 11.10s/it]
Task 19, Epoch 13/20 => Loss 0.046, Train_accy 98.36:  65%|██████▌   | 13/20 [02:24<01:17, 11.07s/it]
Task 19, Epoch 14/20 => Loss 0.048, Train_accy 98.64:  65%|██████▌   | 13/20 [02:35<01:17, 11.07s/it]
Task 19, Epoch 14/20 => Loss 0.048, Train_accy 98.64:  70%|███████   | 14/20 [02:35<01:06, 11.06s/it]
Task 19, Epoch 15/20 => Loss 0.032, Train_accy 98.99:  70%|███████   | 14/20 [02:46<01:06, 11.06s/it]
Task 19, Epoch 15/20 => Loss 0.032, Train_accy 98.99:  75%|███████▌  | 15/20 [02:46<00:55, 11.10s/it]
Task 19, Epoch 16/20 => Loss 0.042, Train_accy 98.57:  75%|███████▌  | 15/20 [02:57<00:55, 11.10s/it]
Task 19, Epoch 16/20 => Loss 0.042, Train_accy 98.57:  80%|████████  | 16/20 [02:57<00:44, 11.15s/it]
Task 19, Epoch 17/20 => Loss 0.045, Train_accy 98.40:  80%|████████  | 16/20 [03:08<00:44, 11.15s/it]
Task 19, Epoch 17/20 => Loss 0.045, Train_accy 98.40:  85%|████████▌ | 17/20 [03:08<00:33, 11.16s/it]
Task 19, Epoch 18/20 => Loss 0.030, Train_accy 99.09:  85%|████████▌ | 17/20 [03:20<00:33, 11.16s/it]
Task 19, Epoch 18/20 => Loss 0.030, Train_accy 99.09:  90%|█████████ | 18/20 [03:20<00:22, 11.17s/it]
Task 19, Epoch 19/20 => Loss 0.039, Train_accy 98.47:  90%|█████████ | 18/20 [03:31<00:22, 11.17s/it]
Task 19, Epoch 19/20 => Loss 0.039, Train_accy 98.47:  95%|█████████▌| 19/20 [03:31<00:11, 11.13s/it]
Task 19, Epoch 20/20 => Loss 0.028, Train_accy 99.27:  95%|█████████▌| 19/20 [03:42<00:11, 11.13s/it]
Task 19, Epoch 20/20 => Loss 0.028, Train_accy 99.27: 100%|██████████| 20/20 [03:42<00:00, 11.18s/it]
Task 19, Epoch 20/20 => Loss 0.028, Train_accy 99.27: 100%|██████████| 20/20 [03:42<00:00, 11.12s/it]
2024-08-12 01:56:17,128 [inflora.py] => Task 19, Epoch 20/20 => Loss 0.028, Train_accy 99.27
Threshold:  0.9816666666666667
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 17/768 type remove
Layer 3 : 56/768 type remove
Layer 4 : 63/768 type remove
Layer 5 : 90/768 type remove
Layer 6 : 103/768 type remove
Layer 7 : 107/768 type remove
Layer 8 : 124/768 type remove
Layer 9 : 168/768 type remove
Layer 10 : 196/768 type remove
Layer 11 : 103/768 type remove
Layer 12 : 120/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 01:56:36,720 [trainer.py] => Time:253.01506328582764
3990 3990
3990 3990
2024-08-12 01:56:45,187 [trainer.py] => Time:8.466278791427612
2024-08-12 01:56:45,187 [inflora.py] => Exemplar size: 0
2024-08-12 01:56:45,187 [trainer.py] => CNN: {'total': 68.55, '00-09': 74.0, '10-19': 77.5, '20-29': 64.5, '30-39': 64.5, '40-49': 71.5, '50-59': 74.37, '60-69': 77.5, '70-79': 74.5, '80-89': 63.32, '90-99': 62.31, '100-109': 72.86, '110-119': 59.3, '120-129': 75.0, '130-139': 72.5, '140-149': 68.34, '150-159': 57.79, '160-169': 59.3, '170-179': 46.5, '180-189': 77.78, '190-199': 77.5, 'old': 68.07, 'new': 77.5}
2024-08-12 01:56:45,187 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68, 74.49, 73.85, 72.44, 71.08, 69.46, 68.42, 68.55]
2024-08-12 01:56:45,187 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11, 97.89, 97.76, 97.56, 97.7, 97.83, 97.78, 97.92]
2024-08-12 01:56:45,188 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734, 0.7456171735241502, 0.7391449565798264, 0.7256498590667084, 0.7122641509433962, 0.6959910913140311, 0.6857519788918206, 0.6864661654135338]
2024-08-12 01:56:45,719 [trainer.py] => All params: 112239531
2024-08-12 01:56:45,724 [trainer.py] => Trainable params: 81418
2024-08-12 01:56:45,724 [inflora.py] => Learning on 200-210
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.20.weight', 'image_encoder.blocks.9.attn.lora_B_v.20.weight', 'image_encoder.blocks.2.attn.lora_B_v.20.weight', 'image_encoder.blocks.6.attn.lora_B_v.20.weight', 'image_encoder.blocks.5.attn.lora_B_v.20.weight', 'image_encoder.blocks.10.attn.lora_B_v.20.weight', 'image_encoder.blocks.0.attn.lora_B_k.20.weight', 'image_encoder.blocks.9.attn.lora_B_k.20.weight', 'classifier_pool.20.weight', 'image_encoder.blocks.0.attn.lora_B_v.20.weight', 'image_encoder.blocks.1.attn.lora_B_k.20.weight', 'image_encoder.blocks.1.attn.lora_B_v.20.weight', 'image_encoder.blocks.3.attn.lora_B_v.20.weight', 'image_encoder.blocks.7.attn.lora_B_v.20.weight', 'image_encoder.blocks.11.attn.lora_B_v.20.weight', 'classifier_pool.20.bias', 'image_encoder.blocks.5.attn.lora_B_k.20.weight', 'image_encoder.blocks.4.attn.lora_B_k.20.weight', 'image_encoder.blocks.8.attn.lora_B_v.20.weight', 'image_encoder.blocks.7.attn.lora_B_k.20.weight', 'image_encoder.blocks.4.attn.lora_B_v.20.weight', 'image_encoder.blocks.11.attn.lora_B_k.20.weight', 'image_encoder.blocks.3.attn.lora_B_k.20.weight', 'image_encoder.blocks.8.attn.lora_B_k.20.weight', 'image_encoder.blocks.2.attn.lora_B_k.20.weight', 'image_encoder.blocks.10.attn.lora_B_k.20.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 20, Epoch 1/20 => Loss 0.605, Train_accy 79.01:   0%|          | 0/20 [00:11<?, ?it/s]
Task 20, Epoch 1/20 => Loss 0.605, Train_accy 79.01:   5%|▌         | 1/20 [00:11<03:39, 11.57s/it]
Task 20, Epoch 2/20 => Loss 0.232, Train_accy 89.74:   5%|▌         | 1/20 [00:23<03:39, 11.57s/it]
Task 20, Epoch 2/20 => Loss 0.232, Train_accy 89.74:  10%|█         | 2/20 [00:23<03:28, 11.60s/it]
Task 20, Epoch 3/20 => Loss 0.197, Train_accy 91.33:  10%|█         | 2/20 [00:34<03:28, 11.60s/it]
Task 20, Epoch 3/20 => Loss 0.197, Train_accy 91.33:  15%|█▌        | 3/20 [00:34<03:16, 11.59s/it]
Task 20, Epoch 4/20 => Loss 0.170, Train_accy 92.96:  15%|█▌        | 3/20 [00:46<03:16, 11.59s/it]
Task 20, Epoch 4/20 => Loss 0.170, Train_accy 92.96:  20%|██        | 4/20 [00:46<03:06, 11.63s/it]
Task 20, Epoch 5/20 => Loss 0.170, Train_accy 92.73:  20%|██        | 4/20 [00:58<03:06, 11.63s/it]
Task 20, Epoch 5/20 => Loss 0.170, Train_accy 92.73:  25%|██▌       | 5/20 [00:58<02:55, 11.67s/it]
Task 20, Epoch 6/20 => Loss 0.152, Train_accy 93.92:  25%|██▌       | 5/20 [01:09<02:55, 11.67s/it]
Task 20, Epoch 6/20 => Loss 0.152, Train_accy 93.92:  30%|███       | 6/20 [01:09<02:43, 11.67s/it]
Task 20, Epoch 7/20 => Loss 0.142, Train_accy 93.56:  30%|███       | 6/20 [01:21<02:43, 11.67s/it]
Task 20, Epoch 7/20 => Loss 0.142, Train_accy 93.56:  35%|███▌      | 7/20 [01:21<02:31, 11.67s/it]
Task 20, Epoch 8/20 => Loss 0.149, Train_accy 93.86:  35%|███▌      | 7/20 [01:33<02:31, 11.67s/it]
Task 20, Epoch 8/20 => Loss 0.149, Train_accy 93.86:  40%|████      | 8/20 [01:33<02:19, 11.61s/it]
Task 20, Epoch 9/20 => Loss 0.129, Train_accy 94.69:  40%|████      | 8/20 [01:44<02:19, 11.61s/it]
Task 20, Epoch 9/20 => Loss 0.129, Train_accy 94.69:  45%|████▌     | 9/20 [01:44<02:07, 11.60s/it]
Task 20, Epoch 10/20 => Loss 0.135, Train_accy 94.39:  45%|████▌     | 9/20 [01:56<02:07, 11.60s/it]
Task 20, Epoch 10/20 => Loss 0.135, Train_accy 94.39:  50%|█████     | 10/20 [01:56<01:56, 11.61s/it]
Task 20, Epoch 11/20 => Loss 0.124, Train_accy 94.55:  50%|█████     | 10/20 [02:07<01:56, 11.61s/it]
Task 20, Epoch 11/20 => Loss 0.124, Train_accy 94.55:  55%|█████▌    | 11/20 [02:07<01:44, 11.59s/it]
Task 20, Epoch 12/20 => Loss 0.116, Train_accy 95.48:  55%|█████▌    | 11/20 [02:19<01:44, 11.59s/it]
Task 20, Epoch 12/20 => Loss 0.116, Train_accy 95.48:  60%|██████    | 12/20 [02:19<01:33, 11.63s/it]
Task 20, Epoch 13/20 => Loss 0.117, Train_accy 95.22:  60%|██████    | 12/20 [02:31<01:33, 11.63s/it]
Task 20, Epoch 13/20 => Loss 0.117, Train_accy 95.22:  65%|██████▌   | 13/20 [02:31<01:21, 11.60s/it]
Task 20, Epoch 14/20 => Loss 0.113, Train_accy 95.62:  65%|██████▌   | 13/20 [02:42<01:21, 11.60s/it]
Task 20, Epoch 14/20 => Loss 0.113, Train_accy 95.62:  70%|███████   | 14/20 [02:42<01:09, 11.63s/it]
Task 20, Epoch 15/20 => Loss 0.110, Train_accy 95.45:  70%|███████   | 14/20 [02:54<01:09, 11.63s/it]
Task 20, Epoch 15/20 => Loss 0.110, Train_accy 95.45:  75%|███████▌  | 15/20 [02:54<00:58, 11.66s/it]
Task 20, Epoch 16/20 => Loss 0.109, Train_accy 95.52:  75%|███████▌  | 15/20 [03:05<00:58, 11.66s/it]
Task 20, Epoch 16/20 => Loss 0.109, Train_accy 95.52:  80%|████████  | 16/20 [03:05<00:46, 11.62s/it]
Task 20, Epoch 17/20 => Loss 0.107, Train_accy 95.55:  80%|████████  | 16/20 [03:17<00:46, 11.62s/it]
Task 20, Epoch 17/20 => Loss 0.107, Train_accy 95.55:  85%|████████▌ | 17/20 [03:17<00:35, 11.68s/it]
Task 20, Epoch 18/20 => Loss 0.095, Train_accy 96.65:  85%|████████▌ | 17/20 [03:29<00:35, 11.68s/it]
Task 20, Epoch 18/20 => Loss 0.095, Train_accy 96.65:  90%|█████████ | 18/20 [03:29<00:23, 11.64s/it]
Task 20, Epoch 19/20 => Loss 0.096, Train_accy 96.25:  90%|█████████ | 18/20 [03:40<00:23, 11.64s/it]
Task 20, Epoch 19/20 => Loss 0.096, Train_accy 96.25:  95%|█████████▌| 19/20 [03:40<00:11, 11.62s/it]
Task 20, Epoch 20/20 => Loss 0.106, Train_accy 96.15:  95%|█████████▌| 19/20 [03:52<00:11, 11.62s/it]
Task 20, Epoch 20/20 => Loss 0.106, Train_accy 96.15: 100%|██████████| 20/20 [03:52<00:00, 11.60s/it]
Task 20, Epoch 20/20 => Loss 0.106, Train_accy 96.15: 100%|██████████| 20/20 [03:52<00:00, 11.62s/it]
2024-08-12 02:00:49,729 [inflora.py] => Task 20, Epoch 20/20 => Loss 0.106, Train_accy 96.15
Threshold:  0.9833333333333333
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 18/768 type remove
Layer 3 : 61/768 type remove
Layer 4 : 68/768 type remove
Layer 5 : 95/768 type remove
Layer 6 : 107/768 type remove
Layer 7 : 112/768 type remove
Layer 8 : 129/768 type remove
Layer 9 : 177/768 type remove
Layer 10 : 211/768 type remove
Layer 11 : 116/768 type remove
Layer 12 : 127/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:01:10,199 [trainer.py] => Time:264.47565746307373
4189 4189
4189 4189
2024-08-12 02:01:19,108 [trainer.py] => Time:8.908451795578003
2024-08-12 02:01:19,108 [inflora.py] => Exemplar size: 0
2024-08-12 02:01:19,109 [trainer.py] => CNN: {'total': 66.96, '00-09': 73.0, '10-19': 77.5, '20-29': 63.0, '30-39': 66.0, '40-49': 68.5, '50-59': 73.87, '60-69': 78.0, '70-79': 73.5, '80-89': 63.32, '90-99': 61.31, '100-109': 70.85, '110-119': 56.28, '120-129': 73.5, '130-139': 72.0, '140-149': 66.33, '150-159': 56.78, '160-169': 57.29, '170-179': 44.0, '180-189': 77.78, '190-199': 72.0, '200-209': 61.31, 'old': 67.24, 'new': 61.31}
2024-08-12 02:01:19,109 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68, 74.49, 73.85, 72.44, 71.08, 69.46, 68.42, 68.55, 66.96]
2024-08-12 02:01:19,109 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11, 97.89, 97.76, 97.56, 97.7, 97.83, 97.78, 97.92, 97.61]
2024-08-12 02:01:19,109 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734, 0.7456171735241502, 0.7391449565798264, 0.7256498590667084, 0.7122641509433962, 0.6959910913140311, 0.6857519788918206, 0.6864661654135338, 0.6708044879446169]
2024-08-12 02:01:19,640 [trainer.py] => All params: 112239531
2024-08-12 02:01:19,645 [trainer.py] => Trainable params: 81418
2024-08-12 02:01:19,645 [inflora.py] => Learning on 210-220
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_v.21.weight', 'image_encoder.blocks.9.attn.lora_B_k.21.weight', 'classifier_pool.21.weight', 'classifier_pool.21.bias', 'image_encoder.blocks.3.attn.lora_B_v.21.weight', 'image_encoder.blocks.8.attn.lora_B_k.21.weight', 'image_encoder.blocks.4.attn.lora_B_v.21.weight', 'image_encoder.blocks.2.attn.lora_B_v.21.weight', 'image_encoder.blocks.10.attn.lora_B_k.21.weight', 'image_encoder.blocks.4.attn.lora_B_k.21.weight', 'image_encoder.blocks.3.attn.lora_B_k.21.weight', 'image_encoder.blocks.1.attn.lora_B_k.21.weight', 'image_encoder.blocks.11.attn.lora_B_v.21.weight', 'image_encoder.blocks.7.attn.lora_B_k.21.weight', 'image_encoder.blocks.9.attn.lora_B_v.21.weight', 'image_encoder.blocks.2.attn.lora_B_k.21.weight', 'image_encoder.blocks.7.attn.lora_B_v.21.weight', 'image_encoder.blocks.0.attn.lora_B_k.21.weight', 'image_encoder.blocks.0.attn.lora_B_v.21.weight', 'image_encoder.blocks.8.attn.lora_B_v.21.weight', 'image_encoder.blocks.11.attn.lora_B_k.21.weight', 'image_encoder.blocks.6.attn.lora_B_k.21.weight', 'image_encoder.blocks.6.attn.lora_B_v.21.weight', 'image_encoder.blocks.5.attn.lora_B_k.21.weight', 'image_encoder.blocks.10.attn.lora_B_v.21.weight', 'image_encoder.blocks.5.attn.lora_B_v.21.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 21, Epoch 1/20 => Loss 0.978, Train_accy 63.57:   0%|          | 0/20 [00:11<?, ?it/s]
Task 21, Epoch 1/20 => Loss 0.978, Train_accy 63.57:   5%|▌         | 1/20 [00:11<03:47, 11.96s/it]
Task 21, Epoch 2/20 => Loss 0.647, Train_accy 74.60:   5%|▌         | 1/20 [00:23<03:47, 11.96s/it]
Task 21, Epoch 2/20 => Loss 0.647, Train_accy 74.60:  10%|█         | 2/20 [00:23<03:34, 11.93s/it]
Task 21, Epoch 3/20 => Loss 0.552, Train_accy 78.63:  10%|█         | 2/20 [00:35<03:34, 11.93s/it]
Task 21, Epoch 3/20 => Loss 0.552, Train_accy 78.63:  15%|█▌        | 3/20 [00:35<03:22, 11.91s/it]
Task 21, Epoch 4/20 => Loss 0.512, Train_accy 80.30:  15%|█▌        | 3/20 [00:47<03:22, 11.91s/it]
Task 21, Epoch 4/20 => Loss 0.512, Train_accy 80.30:  20%|██        | 4/20 [00:47<03:10, 11.92s/it]
Task 21, Epoch 5/20 => Loss 0.432, Train_accy 83.59:  20%|██        | 4/20 [00:59<03:10, 11.92s/it]
Task 21, Epoch 5/20 => Loss 0.432, Train_accy 83.59:  25%|██▌       | 5/20 [00:59<02:59, 11.98s/it]
Task 21, Epoch 6/20 => Loss 0.424, Train_accy 84.01:  25%|██▌       | 5/20 [01:11<02:59, 11.98s/it]
Task 21, Epoch 6/20 => Loss 0.424, Train_accy 84.01:  30%|███       | 6/20 [01:11<02:47, 11.96s/it]
Task 21, Epoch 7/20 => Loss 0.404, Train_accy 84.56:  30%|███       | 6/20 [01:23<02:47, 11.96s/it]
Task 21, Epoch 7/20 => Loss 0.404, Train_accy 84.56:  35%|███▌      | 7/20 [01:23<02:35, 11.95s/it]
Task 21, Epoch 8/20 => Loss 0.388, Train_accy 85.59:  35%|███▌      | 7/20 [01:35<02:35, 11.95s/it]
Task 21, Epoch 8/20 => Loss 0.388, Train_accy 85.59:  40%|████      | 8/20 [01:35<02:23, 11.98s/it]
Task 21, Epoch 9/20 => Loss 0.382, Train_accy 85.85:  40%|████      | 8/20 [01:47<02:23, 11.98s/it]
Task 21, Epoch 9/20 => Loss 0.382, Train_accy 85.85:  45%|████▌     | 9/20 [01:47<02:11, 11.97s/it]
Task 21, Epoch 10/20 => Loss 0.345, Train_accy 87.36:  45%|████▌     | 9/20 [01:59<02:11, 11.97s/it]
Task 21, Epoch 10/20 => Loss 0.345, Train_accy 87.36:  50%|█████     | 10/20 [01:59<01:59, 11.94s/it]
Task 21, Epoch 11/20 => Loss 0.333, Train_accy 87.49:  50%|█████     | 10/20 [02:11<01:59, 11.94s/it]
Task 21, Epoch 11/20 => Loss 0.333, Train_accy 87.49:  55%|█████▌    | 11/20 [02:11<01:47, 11.94s/it]
Task 21, Epoch 12/20 => Loss 0.325, Train_accy 88.30:  55%|█████▌    | 11/20 [02:23<01:47, 11.94s/it]
Task 21, Epoch 12/20 => Loss 0.325, Train_accy 88.30:  60%|██████    | 12/20 [02:23<01:35, 11.99s/it]
Task 21, Epoch 13/20 => Loss 0.301, Train_accy 89.17:  60%|██████    | 12/20 [02:35<01:35, 11.99s/it]
Task 21, Epoch 13/20 => Loss 0.301, Train_accy 89.17:  65%|██████▌   | 13/20 [02:35<01:23, 11.95s/it]
Task 21, Epoch 14/20 => Loss 0.300, Train_accy 89.10:  65%|██████▌   | 13/20 [02:47<01:23, 11.95s/it]
Task 21, Epoch 14/20 => Loss 0.300, Train_accy 89.10:  70%|███████   | 14/20 [02:47<01:11, 11.93s/it]
Task 21, Epoch 15/20 => Loss 0.311, Train_accy 89.26:  70%|███████   | 14/20 [02:59<01:11, 11.93s/it]
Task 21, Epoch 15/20 => Loss 0.311, Train_accy 89.26:  75%|███████▌  | 15/20 [02:59<00:59, 11.96s/it]
Task 21, Epoch 16/20 => Loss 0.299, Train_accy 88.91:  75%|███████▌  | 15/20 [03:11<00:59, 11.96s/it]
Task 21, Epoch 16/20 => Loss 0.299, Train_accy 88.91:  80%|████████  | 16/20 [03:11<00:47, 11.94s/it]
Task 21, Epoch 17/20 => Loss 0.296, Train_accy 89.14:  80%|████████  | 16/20 [03:23<00:47, 11.94s/it]
Task 21, Epoch 17/20 => Loss 0.296, Train_accy 89.14:  85%|████████▌ | 17/20 [03:23<00:35, 11.99s/it]
Task 21, Epoch 18/20 => Loss 0.272, Train_accy 90.20:  85%|████████▌ | 17/20 [03:35<00:35, 11.99s/it]
Task 21, Epoch 18/20 => Loss 0.272, Train_accy 90.20:  90%|█████████ | 18/20 [03:35<00:24, 12.01s/it]
Task 21, Epoch 19/20 => Loss 0.283, Train_accy 89.49:  90%|█████████ | 18/20 [03:47<00:24, 12.01s/it]
Task 21, Epoch 19/20 => Loss 0.283, Train_accy 89.49:  95%|█████████▌| 19/20 [03:47<00:11, 12.00s/it]
Task 21, Epoch 20/20 => Loss 0.268, Train_accy 90.36:  95%|█████████▌| 19/20 [03:59<00:11, 12.00s/it]
Task 21, Epoch 20/20 => Loss 0.268, Train_accy 90.36: 100%|██████████| 20/20 [03:59<00:00, 12.00s/it]
Task 21, Epoch 20/20 => Loss 0.268, Train_accy 90.36: 100%|██████████| 20/20 [03:59<00:00, 11.97s/it]
2024-08-12 02:05:30,814 [inflora.py] => Task 21, Epoch 20/20 => Loss 0.268, Train_accy 90.36
Threshold:  0.985
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 19/768 type remove
Layer 3 : 63/768 type remove
Layer 4 : 72/768 type remove
Layer 5 : 99/768 type remove
Layer 6 : 113/768 type remove
Layer 7 : 118/768 type remove
Layer 8 : 134/768 type remove
Layer 9 : 182/768 type remove
Layer 10 : 215/768 type remove
Layer 11 : 120/768 type remove
Layer 12 : 130/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:05:51,926 [trainer.py] => Time:272.2810306549072
4389 4389
4389 4389
2024-08-12 02:06:00,915 [trainer.py] => Time:8.988309383392334
2024-08-12 02:06:00,915 [inflora.py] => Exemplar size: 0
2024-08-12 02:06:00,915 [trainer.py] => CNN: {'total': 65.41, '00-09': 72.5, '10-19': 76.0, '20-29': 64.5, '30-39': 66.5, '40-49': 65.0, '50-59': 74.87, '60-69': 76.5, '70-79': 72.5, '80-89': 63.82, '90-99': 61.31, '100-109': 70.85, '110-119': 57.79, '120-129': 76.0, '130-139': 72.0, '140-149': 66.83, '150-159': 56.78, '160-169': 56.28, '170-179': 47.0, '180-189': 78.79, '190-199': 71.0, '200-209': 61.31, '210-219': 31.0, 'old': 67.06, 'new': 31.0}
2024-08-12 02:06:00,915 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68, 74.49, 73.85, 72.44, 71.08, 69.46, 68.42, 68.55, 66.96, 65.41]
2024-08-12 02:06:00,915 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11, 97.89, 97.76, 97.56, 97.7, 97.83, 97.78, 97.92, 97.61, 97.13]
2024-08-12 02:06:00,916 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734, 0.7456171735241502, 0.7391449565798264, 0.7256498590667084, 0.7122641509433962, 0.6959910913140311, 0.6857519788918206, 0.6864661654135338, 0.6708044879446169, 0.6548188653451812]
2024-08-12 02:06:01,464 [trainer.py] => All params: 112239531
2024-08-12 02:06:01,468 [trainer.py] => Trainable params: 81418
2024-08-12 02:06:01,468 [inflora.py] => Learning on 220-230
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.22.weight', 'image_encoder.blocks.9.attn.lora_B_v.22.weight', 'image_encoder.blocks.4.attn.lora_B_v.22.weight', 'image_encoder.blocks.5.attn.lora_B_v.22.weight', 'image_encoder.blocks.1.attn.lora_B_k.22.weight', 'image_encoder.blocks.9.attn.lora_B_k.22.weight', 'image_encoder.blocks.4.attn.lora_B_k.22.weight', 'image_encoder.blocks.2.attn.lora_B_v.22.weight', 'image_encoder.blocks.8.attn.lora_B_v.22.weight', 'image_encoder.blocks.8.attn.lora_B_k.22.weight', 'image_encoder.blocks.11.attn.lora_B_v.22.weight', 'image_encoder.blocks.1.attn.lora_B_v.22.weight', 'image_encoder.blocks.6.attn.lora_B_k.22.weight', 'image_encoder.blocks.11.attn.lora_B_k.22.weight', 'classifier_pool.22.weight', 'image_encoder.blocks.6.attn.lora_B_v.22.weight', 'image_encoder.blocks.7.attn.lora_B_v.22.weight', 'image_encoder.blocks.10.attn.lora_B_k.22.weight', 'image_encoder.blocks.10.attn.lora_B_v.22.weight', 'image_encoder.blocks.2.attn.lora_B_k.22.weight', 'image_encoder.blocks.7.attn.lora_B_k.22.weight', 'classifier_pool.22.bias', 'image_encoder.blocks.3.attn.lora_B_k.22.weight', 'image_encoder.blocks.0.attn.lora_B_v.22.weight', 'image_encoder.blocks.0.attn.lora_B_k.22.weight', 'image_encoder.blocks.5.attn.lora_B_k.22.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 22, Epoch 1/20 => Loss 0.354, Train_accy 89.04:   0%|          | 0/20 [00:11<?, ?it/s]
Task 22, Epoch 1/20 => Loss 0.354, Train_accy 89.04:   5%|▌         | 1/20 [00:11<03:47, 11.98s/it]
Task 22, Epoch 2/20 => Loss 0.102, Train_accy 96.67:   5%|▌         | 1/20 [00:23<03:47, 11.98s/it]
Task 22, Epoch 2/20 => Loss 0.102, Train_accy 96.67:  10%|█         | 2/20 [00:23<03:34, 11.89s/it]
Task 22, Epoch 3/20 => Loss 0.087, Train_accy 97.29:  10%|█         | 2/20 [00:36<03:34, 11.89s/it]
Task 22, Epoch 3/20 => Loss 0.087, Train_accy 97.29:  15%|█▌        | 3/20 [00:36<03:26, 12.12s/it]
Task 22, Epoch 4/20 => Loss 0.075, Train_accy 97.58:  15%|█▌        | 3/20 [00:48<03:26, 12.12s/it]
Task 22, Epoch 4/20 => Loss 0.075, Train_accy 97.58:  20%|██        | 4/20 [00:48<03:13, 12.11s/it]
Task 22, Epoch 5/20 => Loss 0.069, Train_accy 98.09:  20%|██        | 4/20 [01:00<03:13, 12.11s/it]
Task 22, Epoch 5/20 => Loss 0.069, Train_accy 98.09:  25%|██▌       | 5/20 [01:00<03:00, 12.05s/it]
Task 22, Epoch 6/20 => Loss 0.084, Train_accy 97.45:  25%|██▌       | 5/20 [01:12<03:00, 12.05s/it]
Task 22, Epoch 6/20 => Loss 0.084, Train_accy 97.45:  30%|███       | 6/20 [01:12<02:48, 12.03s/it]
Task 22, Epoch 7/20 => Loss 0.072, Train_accy 97.41:  30%|███       | 6/20 [01:24<02:48, 12.03s/it]
Task 22, Epoch 7/20 => Loss 0.072, Train_accy 97.41:  35%|███▌      | 7/20 [01:24<02:35, 12.00s/it]
Task 22, Epoch 8/20 => Loss 0.062, Train_accy 97.96:  35%|███▌      | 7/20 [01:36<02:35, 12.00s/it]
Task 22, Epoch 8/20 => Loss 0.062, Train_accy 97.96:  40%|████      | 8/20 [01:36<02:23, 11.96s/it]
Task 22, Epoch 9/20 => Loss 0.060, Train_accy 98.03:  40%|████      | 8/20 [01:47<02:23, 11.96s/it]
Task 22, Epoch 9/20 => Loss 0.060, Train_accy 98.03:  45%|████▌     | 9/20 [01:47<02:11, 11.93s/it]
Task 22, Epoch 10/20 => Loss 0.064, Train_accy 97.90:  45%|████▌     | 9/20 [01:59<02:11, 11.93s/it]
Task 22, Epoch 10/20 => Loss 0.064, Train_accy 97.90:  50%|█████     | 10/20 [01:59<01:59, 11.91s/it]
Task 22, Epoch 11/20 => Loss 0.062, Train_accy 97.83:  50%|█████     | 10/20 [02:11<01:59, 11.91s/it]
Task 22, Epoch 11/20 => Loss 0.062, Train_accy 97.83:  55%|█████▌    | 11/20 [02:11<01:47, 11.94s/it]
Task 22, Epoch 12/20 => Loss 0.053, Train_accy 98.35:  55%|█████▌    | 11/20 [02:23<01:47, 11.94s/it]
Task 22, Epoch 12/20 => Loss 0.053, Train_accy 98.35:  60%|██████    | 12/20 [02:23<01:35, 11.94s/it]
Task 22, Epoch 13/20 => Loss 0.055, Train_accy 98.06:  60%|██████    | 12/20 [02:35<01:35, 11.94s/it]
Task 22, Epoch 13/20 => Loss 0.055, Train_accy 98.06:  65%|██████▌   | 13/20 [02:35<01:23, 11.95s/it]
Task 22, Epoch 14/20 => Loss 0.048, Train_accy 98.25:  65%|██████▌   | 13/20 [02:47<01:23, 11.95s/it]
Task 22, Epoch 14/20 => Loss 0.048, Train_accy 98.25:  70%|███████   | 14/20 [02:47<01:11, 11.97s/it]
Task 22, Epoch 15/20 => Loss 0.049, Train_accy 98.32:  70%|███████   | 14/20 [02:59<01:11, 11.97s/it]
Task 22, Epoch 15/20 => Loss 0.049, Train_accy 98.32:  75%|███████▌  | 15/20 [02:59<01:00, 12.00s/it]
Task 22, Epoch 16/20 => Loss 0.058, Train_accy 97.93:  75%|███████▌  | 15/20 [03:11<01:00, 12.00s/it]
Task 22, Epoch 16/20 => Loss 0.058, Train_accy 97.93:  80%|████████  | 16/20 [03:11<00:47, 11.99s/it]
Task 22, Epoch 17/20 => Loss 0.051, Train_accy 98.29:  80%|████████  | 16/20 [03:23<00:47, 11.99s/it]
Task 22, Epoch 17/20 => Loss 0.051, Train_accy 98.29:  85%|████████▌ | 17/20 [03:23<00:36, 12.02s/it]
Task 22, Epoch 18/20 => Loss 0.046, Train_accy 98.25:  85%|████████▌ | 17/20 [03:35<00:36, 12.02s/it]
Task 22, Epoch 18/20 => Loss 0.046, Train_accy 98.25:  90%|█████████ | 18/20 [03:35<00:24, 12.03s/it]
Task 22, Epoch 19/20 => Loss 0.058, Train_accy 97.90:  90%|█████████ | 18/20 [03:47<00:24, 12.03s/it]
Task 22, Epoch 19/20 => Loss 0.058, Train_accy 97.90:  95%|█████████▌| 19/20 [03:47<00:11, 11.99s/it]
Task 22, Epoch 20/20 => Loss 0.042, Train_accy 98.45:  95%|█████████▌| 19/20 [03:59<00:11, 11.99s/it]
Task 22, Epoch 20/20 => Loss 0.042, Train_accy 98.45: 100%|██████████| 20/20 [03:59<00:00, 11.96s/it]
Task 22, Epoch 20/20 => Loss 0.042, Train_accy 98.45: 100%|██████████| 20/20 [03:59<00:00, 11.98s/it]
2024-08-12 02:10:13,089 [inflora.py] => Task 22, Epoch 20/20 => Loss 0.042, Train_accy 98.45
Threshold:  0.9866666666666667
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 24/768 type remove
Layer 3 : 78/768 type remove
Layer 4 : 90/768 type remove
Layer 5 : 120/768 type remove
Layer 6 : 135/768 type remove
Layer 7 : 141/768 type remove
Layer 8 : 162/768 type remove
Layer 9 : 216/768 type remove
Layer 10 : 259/768 type remove
Layer 11 : 145/768 type remove
Layer 12 : 141/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:10:35,175 [trainer.py] => Time:273.7057189941406
4588 4588
4588 4588
2024-08-12 02:10:44,545 [trainer.py] => Time:9.369835376739502
2024-08-12 02:10:44,546 [inflora.py] => Exemplar size: 0
2024-08-12 02:10:44,546 [trainer.py] => CNN: {'total': 64.63, '00-09': 72.5, '10-19': 75.0, '20-29': 65.0, '30-39': 64.5, '40-49': 63.5, '50-59': 73.87, '60-69': 72.5, '70-79': 70.5, '80-89': 63.32, '90-99': 61.81, '100-109': 63.82, '110-119': 58.29, '120-129': 72.5, '130-139': 73.0, '140-149': 66.83, '150-159': 58.79, '160-169': 56.28, '170-179': 46.0, '180-189': 74.75, '190-199': 69.5, '200-209': 61.31, '210-219': 30.5, '220-229': 72.36, 'old': 64.27, 'new': 72.36}
2024-08-12 02:10:44,546 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68, 74.49, 73.85, 72.44, 71.08, 69.46, 68.42, 68.55, 66.96, 65.41, 64.63]
2024-08-12 02:10:44,546 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11, 97.89, 97.76, 97.56, 97.7, 97.83, 97.78, 97.92, 97.61, 97.13, 97.19]
2024-08-12 02:10:44,546 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734, 0.7456171735241502, 0.7391449565798264, 0.7256498590667084, 0.7122641509433962, 0.6959910913140311, 0.6857519788918206, 0.6864661654135338, 0.6708044879446169, 0.6548188653451812, 0.6466870095902354]
2024-08-12 02:10:45,106 [trainer.py] => All params: 112239531
2024-08-12 02:10:45,110 [trainer.py] => Trainable params: 81418
2024-08-12 02:10:45,111 [inflora.py] => Learning on 230-240
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.23.weight', 'image_encoder.blocks.0.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_v.23.weight', 'image_encoder.blocks.3.attn.lora_B_v.23.weight', 'image_encoder.blocks.10.attn.lora_B_v.23.weight', 'image_encoder.blocks.9.attn.lora_B_v.23.weight', 'image_encoder.blocks.4.attn.lora_B_v.23.weight', 'image_encoder.blocks.7.attn.lora_B_v.23.weight', 'image_encoder.blocks.5.attn.lora_B_v.23.weight', 'image_encoder.blocks.5.attn.lora_B_k.23.weight', 'image_encoder.blocks.2.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_k.23.weight', 'image_encoder.blocks.2.attn.lora_B_k.23.weight', 'classifier_pool.23.bias', 'image_encoder.blocks.3.attn.lora_B_k.23.weight', 'image_encoder.blocks.10.attn.lora_B_k.23.weight', 'image_encoder.blocks.1.attn.lora_B_k.23.weight', 'image_encoder.blocks.4.attn.lora_B_k.23.weight', 'image_encoder.blocks.0.attn.lora_B_v.23.weight', 'image_encoder.blocks.11.attn.lora_B_v.23.weight', 'image_encoder.blocks.7.attn.lora_B_k.23.weight', 'image_encoder.blocks.9.attn.lora_B_k.23.weight', 'image_encoder.blocks.8.attn.lora_B_v.23.weight', 'image_encoder.blocks.8.attn.lora_B_k.23.weight', 'classifier_pool.23.weight', 'image_encoder.blocks.6.attn.lora_B_v.23.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 23, Epoch 1/20 => Loss 0.388, Train_accy 87.59:   0%|          | 0/20 [00:12<?, ?it/s]
Task 23, Epoch 1/20 => Loss 0.388, Train_accy 87.59:   5%|▌         | 1/20 [00:12<03:49, 12.06s/it]
Task 23, Epoch 2/20 => Loss 0.092, Train_accy 97.24:   5%|▌         | 1/20 [00:24<03:49, 12.06s/it]
Task 23, Epoch 2/20 => Loss 0.092, Train_accy 97.24:  10%|█         | 2/20 [00:24<03:36, 12.03s/it]
Task 23, Epoch 3/20 => Loss 0.067, Train_accy 97.81:  10%|█         | 2/20 [00:36<03:36, 12.03s/it]
Task 23, Epoch 3/20 => Loss 0.067, Train_accy 97.81:  15%|█▌        | 3/20 [00:36<03:25, 12.10s/it]
Task 23, Epoch 4/20 => Loss 0.058, Train_accy 98.14:  15%|█▌        | 3/20 [00:48<03:25, 12.10s/it]
Task 23, Epoch 4/20 => Loss 0.058, Train_accy 98.14:  20%|██        | 4/20 [00:48<03:13, 12.10s/it]
Task 23, Epoch 5/20 => Loss 0.054, Train_accy 98.10:  20%|██        | 4/20 [01:00<03:13, 12.10s/it]
Task 23, Epoch 5/20 => Loss 0.054, Train_accy 98.10:  25%|██▌       | 5/20 [01:00<03:01, 12.13s/it]
Task 23, Epoch 6/20 => Loss 0.051, Train_accy 98.20:  25%|██▌       | 5/20 [01:12<03:01, 12.13s/it]
Task 23, Epoch 6/20 => Loss 0.051, Train_accy 98.20:  30%|███       | 6/20 [01:12<02:48, 12.05s/it]
Task 23, Epoch 7/20 => Loss 0.039, Train_accy 98.49:  30%|███       | 6/20 [01:24<02:48, 12.05s/it]
Task 23, Epoch 7/20 => Loss 0.039, Train_accy 98.49:  35%|███▌      | 7/20 [01:24<02:36, 12.07s/it]
Task 23, Epoch 8/20 => Loss 0.046, Train_accy 98.33:  35%|███▌      | 7/20 [01:36<02:36, 12.07s/it]
Task 23, Epoch 8/20 => Loss 0.046, Train_accy 98.33:  40%|████      | 8/20 [01:36<02:25, 12.09s/it]
Task 23, Epoch 9/20 => Loss 0.042, Train_accy 98.52:  40%|████      | 8/20 [01:48<02:25, 12.09s/it]
Task 23, Epoch 9/20 => Loss 0.042, Train_accy 98.52:  45%|████▌     | 9/20 [01:48<02:12, 12.06s/it]
Task 23, Epoch 10/20 => Loss 0.043, Train_accy 98.39:  45%|████▌     | 9/20 [02:00<02:12, 12.06s/it]
Task 23, Epoch 10/20 => Loss 0.043, Train_accy 98.39:  50%|█████     | 10/20 [02:00<02:00, 12.03s/it]
Task 23, Epoch 11/20 => Loss 0.033, Train_accy 98.94:  50%|█████     | 10/20 [02:12<02:00, 12.03s/it]
Task 23, Epoch 11/20 => Loss 0.033, Train_accy 98.94:  55%|█████▌    | 11/20 [02:12<01:48, 12.04s/it]
Task 23, Epoch 12/20 => Loss 0.027, Train_accy 98.91:  55%|█████▌    | 11/20 [02:24<01:48, 12.04s/it]
Task 23, Epoch 12/20 => Loss 0.027, Train_accy 98.91:  60%|██████    | 12/20 [02:24<01:36, 12.06s/it]
Task 23, Epoch 13/20 => Loss 0.030, Train_accy 99.07:  60%|██████    | 12/20 [02:36<01:36, 12.06s/it]
Task 23, Epoch 13/20 => Loss 0.030, Train_accy 99.07:  65%|██████▌   | 13/20 [02:36<01:24, 12.06s/it]
Task 23, Epoch 14/20 => Loss 0.025, Train_accy 99.07:  65%|██████▌   | 13/20 [02:49<01:24, 12.06s/it]
Task 23, Epoch 14/20 => Loss 0.025, Train_accy 99.07:  70%|███████   | 14/20 [02:49<01:12, 12.09s/it]
Task 23, Epoch 15/20 => Loss 0.032, Train_accy 99.00:  70%|███████   | 14/20 [03:00<01:12, 12.09s/it]
Task 23, Epoch 15/20 => Loss 0.032, Train_accy 99.00:  75%|███████▌  | 15/20 [03:00<01:00, 12.04s/it]
Task 23, Epoch 16/20 => Loss 0.042, Train_accy 98.39:  75%|███████▌  | 15/20 [03:12<01:00, 12.04s/it]
Task 23, Epoch 16/20 => Loss 0.042, Train_accy 98.39:  80%|████████  | 16/20 [03:12<00:48, 12.02s/it]
Task 23, Epoch 17/20 => Loss 0.036, Train_accy 98.68:  80%|████████  | 16/20 [03:25<00:48, 12.02s/it]
Task 23, Epoch 17/20 => Loss 0.036, Train_accy 98.68:  85%|████████▌ | 17/20 [03:25<00:36, 12.05s/it]
Task 23, Epoch 18/20 => Loss 0.039, Train_accy 98.81:  85%|████████▌ | 17/20 [03:37<00:36, 12.05s/it]
Task 23, Epoch 18/20 => Loss 0.039, Train_accy 98.81:  90%|█████████ | 18/20 [03:37<00:24, 12.03s/it]
Task 23, Epoch 19/20 => Loss 0.038, Train_accy 98.97:  90%|█████████ | 18/20 [03:49<00:24, 12.03s/it]
Task 23, Epoch 19/20 => Loss 0.038, Train_accy 98.97:  95%|█████████▌| 19/20 [03:49<00:12, 12.07s/it]
Task 23, Epoch 20/20 => Loss 0.028, Train_accy 99.10:  95%|█████████▌| 19/20 [04:01<00:12, 12.07s/it]
Task 23, Epoch 20/20 => Loss 0.028, Train_accy 99.10: 100%|██████████| 20/20 [04:01<00:00, 12.06s/it]
Task 23, Epoch 20/20 => Loss 0.028, Train_accy 99.10: 100%|██████████| 20/20 [04:01<00:00, 12.06s/it]
2024-08-12 02:14:58,212 [inflora.py] => Task 23, Epoch 20/20 => Loss 0.028, Train_accy 99.10
Threshold:  0.9883333333333333
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 25/768 type remove
Layer 3 : 80/768 type remove
Layer 4 : 93/768 type remove
Layer 5 : 125/768 type remove
Layer 6 : 144/768 type remove
Layer 7 : 154/768 type remove
Layer 8 : 180/768 type remove
Layer 9 : 241/768 type remove
Layer 10 : 286/768 type remove
Layer 11 : 169/768 type remove
Layer 12 : 162/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:15:20,146 [trainer.py] => Time:275.0357127189636
4788 4788
4788 4788
2024-08-12 02:15:30,014 [trainer.py] => Time:9.8670654296875
2024-08-12 02:15:30,014 [inflora.py] => Exemplar size: 0
2024-08-12 02:15:30,014 [trainer.py] => CNN: {'total': 64.31, '00-09': 73.0, '10-19': 73.5, '20-29': 64.5, '30-39': 63.0, '40-49': 62.5, '50-59': 71.86, '60-69': 74.5, '70-79': 70.5, '80-89': 63.32, '90-99': 60.8, '100-109': 62.81, '110-119': 54.77, '120-129': 72.5, '130-139': 73.5, '140-149': 62.31, '150-159': 56.28, '160-169': 55.28, '170-179': 43.5, '180-189': 73.74, '190-199': 71.5, '200-209': 58.79, '210-219': 30.0, '220-229': 71.36, '230-239': 79.5, 'old': 63.64, 'new': 79.5}
2024-08-12 02:15:30,014 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68, 74.49, 73.85, 72.44, 71.08, 69.46, 68.42, 68.55, 66.96, 65.41, 64.63, 64.31]
2024-08-12 02:15:30,014 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11, 97.89, 97.76, 97.56, 97.7, 97.83, 97.78, 97.92, 97.61, 97.13, 97.19, 97.1]
2024-08-12 02:15:30,014 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734, 0.7456171735241502, 0.7391449565798264, 0.7256498590667084, 0.7122641509433962, 0.6959910913140311, 0.6857519788918206, 0.6864661654135338, 0.6708044879446169, 0.6548188653451812, 0.6466870095902354, 0.6436925647451963]
2024-08-12 02:15:30,548 [trainer.py] => All params: 112239531
2024-08-12 02:15:30,552 [trainer.py] => Trainable params: 81418
2024-08-12 02:15:30,553 [inflora.py] => Learning on 240-250
Parameters to be updated: {'image_encoder.blocks.3.attn.lora_B_v.24.weight', 'image_encoder.blocks.3.attn.lora_B_k.24.weight', 'image_encoder.blocks.1.attn.lora_B_k.24.weight', 'image_encoder.blocks.2.attn.lora_B_v.24.weight', 'image_encoder.blocks.11.attn.lora_B_k.24.weight', 'image_encoder.blocks.1.attn.lora_B_v.24.weight', 'image_encoder.blocks.0.attn.lora_B_k.24.weight', 'image_encoder.blocks.11.attn.lora_B_v.24.weight', 'image_encoder.blocks.6.attn.lora_B_k.24.weight', 'image_encoder.blocks.6.attn.lora_B_v.24.weight', 'image_encoder.blocks.10.attn.lora_B_v.24.weight', 'classifier_pool.24.weight', 'image_encoder.blocks.2.attn.lora_B_k.24.weight', 'image_encoder.blocks.9.attn.lora_B_k.24.weight', 'image_encoder.blocks.8.attn.lora_B_v.24.weight', 'image_encoder.blocks.4.attn.lora_B_v.24.weight', 'image_encoder.blocks.5.attn.lora_B_v.24.weight', 'image_encoder.blocks.7.attn.lora_B_k.24.weight', 'image_encoder.blocks.7.attn.lora_B_v.24.weight', 'image_encoder.blocks.8.attn.lora_B_k.24.weight', 'image_encoder.blocks.10.attn.lora_B_k.24.weight', 'image_encoder.blocks.5.attn.lora_B_k.24.weight', 'image_encoder.blocks.0.attn.lora_B_v.24.weight', 'classifier_pool.24.bias', 'image_encoder.blocks.9.attn.lora_B_v.24.weight', 'image_encoder.blocks.4.attn.lora_B_k.24.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 24, Epoch 1/20 => Loss 0.541, Train_accy 82.09:   0%|          | 0/20 [00:12<?, ?it/s]
Task 24, Epoch 1/20 => Loss 0.541, Train_accy 82.09:   5%|▌         | 1/20 [00:12<03:51, 12.21s/it]
Task 24, Epoch 2/20 => Loss 0.198, Train_accy 92.82:   5%|▌         | 1/20 [00:24<03:51, 12.21s/it]
Task 24, Epoch 2/20 => Loss 0.198, Train_accy 92.82:  10%|█         | 2/20 [00:24<03:38, 12.12s/it]
Task 24, Epoch 3/20 => Loss 0.180, Train_accy 93.26:  10%|█         | 2/20 [00:36<03:38, 12.12s/it]
Task 24, Epoch 3/20 => Loss 0.180, Train_accy 93.26:  15%|█▌        | 3/20 [00:36<03:26, 12.14s/it]
Task 24, Epoch 4/20 => Loss 0.143, Train_accy 94.75:  15%|█▌        | 3/20 [00:48<03:26, 12.14s/it]
Task 24, Epoch 4/20 => Loss 0.143, Train_accy 94.75:  20%|██        | 4/20 [00:48<03:14, 12.18s/it]
Task 24, Epoch 5/20 => Loss 0.128, Train_accy 95.03:  20%|██        | 4/20 [01:00<03:14, 12.18s/it]
Task 24, Epoch 5/20 => Loss 0.128, Train_accy 95.03:  25%|██▌       | 5/20 [01:00<03:02, 12.16s/it]
Task 24, Epoch 6/20 => Loss 0.126, Train_accy 95.00:  25%|██▌       | 5/20 [01:12<03:02, 12.16s/it]
Task 24, Epoch 6/20 => Loss 0.126, Train_accy 95.00:  30%|███       | 6/20 [01:12<02:50, 12.15s/it]
Task 24, Epoch 7/20 => Loss 0.108, Train_accy 96.33:  30%|███       | 6/20 [01:25<02:50, 12.15s/it]
Task 24, Epoch 7/20 => Loss 0.108, Train_accy 96.33:  35%|███▌      | 7/20 [01:25<02:38, 12.16s/it]
Task 24, Epoch 8/20 => Loss 0.116, Train_accy 95.60:  35%|███▌      | 7/20 [01:37<02:38, 12.16s/it]
Task 24, Epoch 8/20 => Loss 0.116, Train_accy 95.60:  40%|████      | 8/20 [01:37<02:25, 12.14s/it]
Task 24, Epoch 9/20 => Loss 0.102, Train_accy 96.11:  40%|████      | 8/20 [01:49<02:25, 12.14s/it]
Task 24, Epoch 9/20 => Loss 0.102, Train_accy 96.11:  45%|████▌     | 9/20 [01:49<02:13, 12.15s/it]
Task 24, Epoch 10/20 => Loss 0.101, Train_accy 96.27:  45%|████▌     | 9/20 [02:01<02:13, 12.15s/it]
Task 24, Epoch 10/20 => Loss 0.101, Train_accy 96.27:  50%|█████     | 10/20 [02:01<02:01, 12.14s/it]
Task 24, Epoch 11/20 => Loss 0.104, Train_accy 96.01:  50%|█████     | 10/20 [02:13<02:01, 12.14s/it]
Task 24, Epoch 11/20 => Loss 0.104, Train_accy 96.01:  55%|█████▌    | 11/20 [02:13<01:49, 12.13s/it]
Task 24, Epoch 12/20 => Loss 0.088, Train_accy 96.87:  55%|█████▌    | 11/20 [02:25<01:49, 12.13s/it]
Task 24, Epoch 12/20 => Loss 0.088, Train_accy 96.87:  60%|██████    | 12/20 [02:25<01:37, 12.19s/it]
Task 24, Epoch 13/20 => Loss 0.097, Train_accy 96.68:  60%|██████    | 12/20 [02:38<01:37, 12.19s/it]
Task 24, Epoch 13/20 => Loss 0.097, Train_accy 96.68:  65%|██████▌   | 13/20 [02:38<01:25, 12.16s/it]
Task 24, Epoch 14/20 => Loss 0.097, Train_accy 96.46:  65%|██████▌   | 13/20 [02:50<01:25, 12.16s/it]
Task 24, Epoch 14/20 => Loss 0.097, Train_accy 96.46:  70%|███████   | 14/20 [02:50<01:13, 12.20s/it]
Task 24, Epoch 15/20 => Loss 0.085, Train_accy 96.90:  70%|███████   | 14/20 [03:02<01:13, 12.20s/it]
Task 24, Epoch 15/20 => Loss 0.085, Train_accy 96.90:  75%|███████▌  | 15/20 [03:02<01:00, 12.17s/it]
Task 24, Epoch 16/20 => Loss 0.091, Train_accy 96.80:  75%|███████▌  | 15/20 [03:14<01:00, 12.17s/it]
Task 24, Epoch 16/20 => Loss 0.091, Train_accy 96.80:  80%|████████  | 16/20 [03:14<00:48, 12.16s/it]
Task 24, Epoch 17/20 => Loss 0.082, Train_accy 97.03:  80%|████████  | 16/20 [03:26<00:48, 12.16s/it]
Task 24, Epoch 17/20 => Loss 0.082, Train_accy 97.03:  85%|████████▌ | 17/20 [03:26<00:36, 12.21s/it]
Task 24, Epoch 18/20 => Loss 0.077, Train_accy 97.22:  85%|████████▌ | 17/20 [03:38<00:36, 12.21s/it]
Task 24, Epoch 18/20 => Loss 0.077, Train_accy 97.22:  90%|█████████ | 18/20 [03:38<00:24, 12.18s/it]
Task 24, Epoch 19/20 => Loss 0.081, Train_accy 97.18:  90%|█████████ | 18/20 [03:51<00:24, 12.18s/it]
Task 24, Epoch 19/20 => Loss 0.081, Train_accy 97.18:  95%|█████████▌| 19/20 [03:51<00:12, 12.21s/it]
Task 24, Epoch 20/20 => Loss 0.076, Train_accy 96.96:  95%|█████████▌| 19/20 [04:03<00:12, 12.21s/it]
Task 24, Epoch 20/20 => Loss 0.076, Train_accy 96.96: 100%|██████████| 20/20 [04:03<00:00, 12.20s/it]
Task 24, Epoch 20/20 => Loss 0.076, Train_accy 96.96: 100%|██████████| 20/20 [04:03<00:00, 12.17s/it]
2024-08-12 02:19:45,930 [inflora.py] => Task 24, Epoch 20/20 => Loss 0.076, Train_accy 96.96
Threshold:  0.99
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 26/768 type remove
Layer 3 : 84/768 type remove
Layer 4 : 99/768 type remove
Layer 5 : 133/768 type remove
Layer 6 : 154/768 type remove
Layer 7 : 168/768 type remove
Layer 8 : 195/768 type remove
Layer 9 : 257/768 type remove
Layer 10 : 298/768 type remove
Layer 11 : 182/768 type remove
Layer 12 : 173/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:20:07,715 [trainer.py] => Time:277.1624734401703
4988 4988
4988 4988
2024-08-12 02:20:17,734 [trainer.py] => Time:10.018845558166504
2024-08-12 02:20:17,734 [inflora.py] => Exemplar size: 0
2024-08-12 02:20:17,735 [trainer.py] => CNN: {'total': 63.55, '00-09': 72.0, '10-19': 72.5, '20-29': 63.0, '30-39': 63.5, '40-49': 64.0, '50-59': 73.37, '60-69': 75.5, '70-79': 70.5, '80-89': 63.82, '90-99': 60.8, '100-109': 64.32, '110-119': 54.27, '120-129': 64.5, '130-139': 73.0, '140-149': 62.81, '150-159': 56.28, '160-169': 55.28, '170-179': 43.5, '180-189': 72.73, '190-199': 68.5, '200-209': 57.29, '210-219': 28.5, '220-229': 71.86, '230-239': 76.0, '240-249': 61.0, 'old': 63.66, 'new': 61.0}
2024-08-12 02:20:17,735 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68, 74.49, 73.85, 72.44, 71.08, 69.46, 68.42, 68.55, 66.96, 65.41, 64.63, 64.31, 63.55]
2024-08-12 02:20:17,735 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11, 97.89, 97.76, 97.56, 97.7, 97.83, 97.78, 97.92, 97.61, 97.13, 97.19, 97.1, 97.29]
2024-08-12 02:20:17,735 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734, 0.7456171735241502, 0.7391449565798264, 0.7256498590667084, 0.7122641509433962, 0.6959910913140311, 0.6857519788918206, 0.6864661654135338, 0.6708044879446169, 0.6548188653451812, 0.6466870095902354, 0.6436925647451963, 0.6361267040898155]
2024-08-12 02:20:18,276 [trainer.py] => All params: 112239531
2024-08-12 02:20:18,280 [trainer.py] => Trainable params: 81418
2024-08-12 02:20:18,280 [inflora.py] => Learning on 250-260
Parameters to be updated: {'image_encoder.blocks.11.attn.lora_B_k.25.weight', 'image_encoder.blocks.3.attn.lora_B_k.25.weight', 'image_encoder.blocks.8.attn.lora_B_v.25.weight', 'image_encoder.blocks.8.attn.lora_B_k.25.weight', 'image_encoder.blocks.3.attn.lora_B_v.25.weight', 'image_encoder.blocks.6.attn.lora_B_v.25.weight', 'image_encoder.blocks.6.attn.lora_B_k.25.weight', 'image_encoder.blocks.4.attn.lora_B_k.25.weight', 'image_encoder.blocks.1.attn.lora_B_v.25.weight', 'image_encoder.blocks.5.attn.lora_B_k.25.weight', 'image_encoder.blocks.2.attn.lora_B_k.25.weight', 'classifier_pool.25.bias', 'image_encoder.blocks.2.attn.lora_B_v.25.weight', 'image_encoder.blocks.7.attn.lora_B_k.25.weight', 'image_encoder.blocks.5.attn.lora_B_v.25.weight', 'image_encoder.blocks.9.attn.lora_B_v.25.weight', 'image_encoder.blocks.4.attn.lora_B_v.25.weight', 'image_encoder.blocks.0.attn.lora_B_v.25.weight', 'image_encoder.blocks.11.attn.lora_B_v.25.weight', 'image_encoder.blocks.10.attn.lora_B_k.25.weight', 'image_encoder.blocks.0.attn.lora_B_k.25.weight', 'image_encoder.blocks.9.attn.lora_B_k.25.weight', 'image_encoder.blocks.10.attn.lora_B_v.25.weight', 'image_encoder.blocks.7.attn.lora_B_v.25.weight', 'image_encoder.blocks.1.attn.lora_B_k.25.weight', 'classifier_pool.25.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 25, Epoch 1/20 => Loss 0.442, Train_accy 86.41:   0%|          | 0/20 [00:11<?, ?it/s]
Task 25, Epoch 1/20 => Loss 0.442, Train_accy 86.41:   5%|▌         | 1/20 [00:11<03:39, 11.57s/it]
Task 25, Epoch 2/20 => Loss 0.138, Train_accy 95.97:   5%|▌         | 1/20 [00:22<03:39, 11.57s/it]
Task 25, Epoch 2/20 => Loss 0.138, Train_accy 95.97:  10%|█         | 2/20 [00:22<03:26, 11.47s/it]
Task 25, Epoch 3/20 => Loss 0.111, Train_accy 96.24:  10%|█         | 2/20 [00:34<03:26, 11.47s/it]
Task 25, Epoch 3/20 => Loss 0.111, Train_accy 96.24:  15%|█▌        | 3/20 [00:34<03:14, 11.46s/it]
Task 25, Epoch 4/20 => Loss 0.099, Train_accy 96.81:  15%|█▌        | 3/20 [00:45<03:14, 11.46s/it]
Task 25, Epoch 4/20 => Loss 0.099, Train_accy 96.81:  20%|██        | 4/20 [00:45<03:03, 11.47s/it]
Task 25, Epoch 5/20 => Loss 0.102, Train_accy 96.88:  20%|██        | 4/20 [00:57<03:03, 11.47s/it]
Task 25, Epoch 5/20 => Loss 0.102, Train_accy 96.88:  25%|██▌       | 5/20 [00:57<02:52, 11.47s/it]
Task 25, Epoch 6/20 => Loss 0.089, Train_accy 96.81:  25%|██▌       | 5/20 [01:08<02:52, 11.47s/it]
Task 25, Epoch 6/20 => Loss 0.089, Train_accy 96.81:  30%|███       | 6/20 [01:08<02:40, 11.50s/it]
Task 25, Epoch 7/20 => Loss 0.082, Train_accy 97.12:  30%|███       | 6/20 [01:20<02:40, 11.50s/it]
Task 25, Epoch 7/20 => Loss 0.082, Train_accy 97.12:  35%|███▌      | 7/20 [01:20<02:29, 11.50s/it]
Task 25, Epoch 8/20 => Loss 0.068, Train_accy 97.63:  35%|███▌      | 7/20 [01:31<02:29, 11.50s/it]
Task 25, Epoch 8/20 => Loss 0.068, Train_accy 97.63:  40%|████      | 8/20 [01:31<02:18, 11.51s/it]
Task 25, Epoch 9/20 => Loss 0.073, Train_accy 97.70:  40%|████      | 8/20 [01:43<02:18, 11.51s/it]
Task 25, Epoch 9/20 => Loss 0.073, Train_accy 97.70:  45%|████▌     | 9/20 [01:43<02:06, 11.51s/it]
Task 25, Epoch 10/20 => Loss 0.064, Train_accy 97.83:  45%|████▌     | 9/20 [01:55<02:06, 11.51s/it]
Task 25, Epoch 10/20 => Loss 0.064, Train_accy 97.83:  50%|█████     | 10/20 [01:55<01:55, 11.53s/it]
Task 25, Epoch 11/20 => Loss 0.058, Train_accy 97.83:  50%|█████     | 10/20 [02:06<01:55, 11.53s/it]
Task 25, Epoch 11/20 => Loss 0.058, Train_accy 97.83:  55%|█████▌    | 11/20 [02:06<01:43, 11.52s/it]
Task 25, Epoch 12/20 => Loss 0.053, Train_accy 98.00:  55%|█████▌    | 11/20 [02:18<01:43, 11.52s/it]
Task 25, Epoch 12/20 => Loss 0.053, Train_accy 98.00:  60%|██████    | 12/20 [02:18<01:32, 11.56s/it]
Task 25, Epoch 13/20 => Loss 0.058, Train_accy 97.83:  60%|██████    | 12/20 [02:29<01:32, 11.56s/it]
Task 25, Epoch 13/20 => Loss 0.058, Train_accy 97.83:  65%|██████▌   | 13/20 [02:29<01:21, 11.59s/it]
Task 25, Epoch 14/20 => Loss 0.056, Train_accy 98.07:  65%|██████▌   | 13/20 [02:41<01:21, 11.59s/it]
Task 25, Epoch 14/20 => Loss 0.056, Train_accy 98.07:  70%|███████   | 14/20 [02:41<01:09, 11.54s/it]
Task 25, Epoch 15/20 => Loss 0.056, Train_accy 97.93:  70%|███████   | 14/20 [02:53<01:09, 11.54s/it]
Task 25, Epoch 15/20 => Loss 0.056, Train_accy 97.93:  75%|███████▌  | 15/20 [02:53<00:58, 11.61s/it]
Task 25, Epoch 16/20 => Loss 0.056, Train_accy 98.07:  75%|███████▌  | 15/20 [03:04<00:58, 11.61s/it]
Task 25, Epoch 16/20 => Loss 0.056, Train_accy 98.07:  80%|████████  | 16/20 [03:04<00:46, 11.64s/it]
Task 25, Epoch 17/20 => Loss 0.053, Train_accy 98.10:  80%|████████  | 16/20 [03:16<00:46, 11.64s/it]
Task 25, Epoch 17/20 => Loss 0.053, Train_accy 98.10:  85%|████████▌ | 17/20 [03:16<00:34, 11.60s/it]
Task 25, Epoch 18/20 => Loss 0.064, Train_accy 97.63:  85%|████████▌ | 17/20 [03:27<00:34, 11.60s/it]
Task 25, Epoch 18/20 => Loss 0.064, Train_accy 97.63:  90%|█████████ | 18/20 [03:27<00:23, 11.62s/it]
Task 25, Epoch 19/20 => Loss 0.064, Train_accy 97.63:  90%|█████████ | 18/20 [03:39<00:23, 11.62s/it]
Task 25, Epoch 19/20 => Loss 0.064, Train_accy 97.63:  95%|█████████▌| 19/20 [03:39<00:11, 11.67s/it]
Task 25, Epoch 20/20 => Loss 0.050, Train_accy 98.17:  95%|█████████▌| 19/20 [03:51<00:11, 11.67s/it]
Task 25, Epoch 20/20 => Loss 0.050, Train_accy 98.17: 100%|██████████| 20/20 [03:51<00:00, 11.64s/it]
Task 25, Epoch 20/20 => Loss 0.050, Train_accy 98.17: 100%|██████████| 20/20 [03:51<00:00, 11.57s/it]
2024-08-12 02:24:21,234 [inflora.py] => Task 25, Epoch 20/20 => Loss 0.050, Train_accy 98.17
Threshold:  0.9916666666666667
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 31/768 type remove
Layer 3 : 100/768 type remove
Layer 4 : 119/768 type remove
Layer 5 : 157/768 type remove
Layer 6 : 180/768 type remove
Layer 7 : 196/768 type remove
Layer 8 : 230/768 type remove
Layer 9 : 301/768 type remove
Layer 10 : 347/768 type remove
Layer 11 : 225/768 type remove
Layer 12 : 189/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:24:41,823 [trainer.py] => Time:263.54255962371826
5187 5187
5187 5187
2024-08-12 02:24:52,399 [trainer.py] => Time:10.575517177581787
2024-08-12 02:24:52,399 [inflora.py] => Exemplar size: 0
2024-08-12 02:24:52,399 [trainer.py] => CNN: {'total': 63.16, '00-09': 73.5, '10-19': 72.5, '20-29': 63.0, '30-39': 65.0, '40-49': 62.0, '50-59': 74.37, '60-69': 72.5, '70-79': 69.0, '80-89': 63.32, '90-99': 60.8, '100-109': 63.32, '110-119': 51.26, '120-129': 63.0, '130-139': 72.5, '140-149': 63.82, '150-159': 51.26, '160-169': 56.28, '170-179': 43.0, '180-189': 72.22, '190-199': 69.0, '200-209': 57.79, '210-219': 29.0, '220-229': 64.32, '230-239': 77.5, '240-249': 59.5, '250-259': 72.36, 'old': 62.79, 'new': 72.36}
2024-08-12 02:24:52,399 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68, 74.49, 73.85, 72.44, 71.08, 69.46, 68.42, 68.55, 66.96, 65.41, 64.63, 64.31, 63.55, 63.16]
2024-08-12 02:24:52,400 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11, 97.89, 97.76, 97.56, 97.7, 97.83, 97.78, 97.92, 97.61, 97.13, 97.19, 97.1, 97.29, 97.36]
2024-08-12 02:24:52,400 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734, 0.7456171735241502, 0.7391449565798264, 0.7256498590667084, 0.7122641509433962, 0.6959910913140311, 0.6857519788918206, 0.6864661654135338, 0.6708044879446169, 0.6548188653451812, 0.6466870095902354, 0.6436925647451963, 0.6361267040898155, 0.6321573163678427]
2024-08-12 02:24:53,053 [trainer.py] => All params: 112239531
2024-08-12 02:24:53,057 [trainer.py] => Trainable params: 81418
2024-08-12 02:24:53,057 [inflora.py] => Learning on 260-270
Parameters to be updated: {'classifier_pool.26.weight', 'image_encoder.blocks.10.attn.lora_B_v.26.weight', 'image_encoder.blocks.2.attn.lora_B_k.26.weight', 'image_encoder.blocks.4.attn.lora_B_k.26.weight', 'image_encoder.blocks.9.attn.lora_B_v.26.weight', 'classifier_pool.26.bias', 'image_encoder.blocks.3.attn.lora_B_k.26.weight', 'image_encoder.blocks.8.attn.lora_B_v.26.weight', 'image_encoder.blocks.4.attn.lora_B_v.26.weight', 'image_encoder.blocks.11.attn.lora_B_v.26.weight', 'image_encoder.blocks.9.attn.lora_B_k.26.weight', 'image_encoder.blocks.5.attn.lora_B_k.26.weight', 'image_encoder.blocks.8.attn.lora_B_k.26.weight', 'image_encoder.blocks.2.attn.lora_B_v.26.weight', 'image_encoder.blocks.1.attn.lora_B_k.26.weight', 'image_encoder.blocks.1.attn.lora_B_v.26.weight', 'image_encoder.blocks.5.attn.lora_B_v.26.weight', 'image_encoder.blocks.7.attn.lora_B_v.26.weight', 'image_encoder.blocks.0.attn.lora_B_k.26.weight', 'image_encoder.blocks.6.attn.lora_B_k.26.weight', 'image_encoder.blocks.0.attn.lora_B_v.26.weight', 'image_encoder.blocks.3.attn.lora_B_v.26.weight', 'image_encoder.blocks.11.attn.lora_B_k.26.weight', 'image_encoder.blocks.6.attn.lora_B_v.26.weight', 'image_encoder.blocks.10.attn.lora_B_k.26.weight', 'image_encoder.blocks.7.attn.lora_B_k.26.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 26, Epoch 1/20 => Loss 0.390, Train_accy 89.06:   0%|          | 0/20 [00:11<?, ?it/s]
Task 26, Epoch 1/20 => Loss 0.390, Train_accy 89.06:   5%|▌         | 1/20 [00:11<03:45, 11.87s/it]
Task 26, Epoch 2/20 => Loss 0.039, Train_accy 98.95:   5%|▌         | 1/20 [00:23<03:45, 11.87s/it]
Task 26, Epoch 2/20 => Loss 0.039, Train_accy 98.95:  10%|█         | 2/20 [00:23<03:34, 11.91s/it]
Task 26, Epoch 3/20 => Loss 0.031, Train_accy 99.28:  10%|█         | 2/20 [00:35<03:34, 11.91s/it]
Task 26, Epoch 3/20 => Loss 0.031, Train_accy 99.28:  15%|█▌        | 3/20 [00:35<03:21, 11.87s/it]
Task 26, Epoch 4/20 => Loss 0.036, Train_accy 99.01:  15%|█▌        | 3/20 [00:47<03:21, 11.87s/it]
Task 26, Epoch 4/20 => Loss 0.036, Train_accy 99.01:  20%|██        | 4/20 [00:47<03:10, 11.89s/it]
Task 26, Epoch 5/20 => Loss 0.019, Train_accy 99.51:  20%|██        | 4/20 [00:59<03:10, 11.89s/it]
Task 26, Epoch 5/20 => Loss 0.019, Train_accy 99.51:  25%|██▌       | 5/20 [00:59<02:57, 11.85s/it]
Task 26, Epoch 6/20 => Loss 0.021, Train_accy 99.38:  25%|██▌       | 5/20 [01:11<02:57, 11.85s/it]
Task 26, Epoch 6/20 => Loss 0.021, Train_accy 99.38:  30%|███       | 6/20 [01:11<02:45, 11.84s/it]
Task 26, Epoch 7/20 => Loss 0.029, Train_accy 99.24:  30%|███       | 6/20 [01:22<02:45, 11.84s/it]
Task 26, Epoch 7/20 => Loss 0.029, Train_accy 99.24:  35%|███▌      | 7/20 [01:22<02:33, 11.83s/it]
Task 26, Epoch 8/20 => Loss 0.015, Train_accy 99.54:  35%|███▌      | 7/20 [01:34<02:33, 11.83s/it]
Task 26, Epoch 8/20 => Loss 0.015, Train_accy 99.54:  40%|████      | 8/20 [01:34<02:21, 11.81s/it]
Task 26, Epoch 9/20 => Loss 0.017, Train_accy 99.61:  40%|████      | 8/20 [01:46<02:21, 11.81s/it]
Task 26, Epoch 9/20 => Loss 0.017, Train_accy 99.61:  45%|████▌     | 9/20 [01:46<02:10, 11.83s/it]
Task 26, Epoch 10/20 => Loss 0.021, Train_accy 99.34:  45%|████▌     | 9/20 [01:58<02:10, 11.83s/it]
Task 26, Epoch 10/20 => Loss 0.021, Train_accy 99.34:  50%|█████     | 10/20 [01:58<01:58, 11.84s/it]
Task 26, Epoch 11/20 => Loss 0.022, Train_accy 99.24:  50%|█████     | 10/20 [02:10<01:58, 11.84s/it]
Task 26, Epoch 11/20 => Loss 0.022, Train_accy 99.24:  55%|█████▌    | 11/20 [02:10<01:46, 11.84s/it]
Task 26, Epoch 12/20 => Loss 0.012, Train_accy 99.74:  55%|█████▌    | 11/20 [02:22<01:46, 11.84s/it]
Task 26, Epoch 12/20 => Loss 0.012, Train_accy 99.74:  60%|██████    | 12/20 [02:22<01:34, 11.82s/it]
Task 26, Epoch 13/20 => Loss 0.021, Train_accy 99.41:  60%|██████    | 12/20 [02:33<01:34, 11.82s/it]
Task 26, Epoch 13/20 => Loss 0.021, Train_accy 99.41:  65%|██████▌   | 13/20 [02:33<01:22, 11.82s/it]
Task 26, Epoch 14/20 => Loss 0.014, Train_accy 99.51:  65%|██████▌   | 13/20 [02:45<01:22, 11.82s/it]
Task 26, Epoch 14/20 => Loss 0.014, Train_accy 99.51:  70%|███████   | 14/20 [02:45<01:10, 11.83s/it]
Task 26, Epoch 15/20 => Loss 0.017, Train_accy 99.57:  70%|███████   | 14/20 [02:57<01:10, 11.83s/it]
Task 26, Epoch 15/20 => Loss 0.017, Train_accy 99.57:  75%|███████▌  | 15/20 [02:57<00:59, 11.82s/it]
Task 26, Epoch 16/20 => Loss 0.012, Train_accy 99.64:  75%|███████▌  | 15/20 [03:09<00:59, 11.82s/it]
Task 26, Epoch 16/20 => Loss 0.012, Train_accy 99.64:  80%|████████  | 16/20 [03:09<00:47, 11.84s/it]
Task 26, Epoch 17/20 => Loss 0.017, Train_accy 99.57:  80%|████████  | 16/20 [03:21<00:47, 11.84s/it]
Task 26, Epoch 17/20 => Loss 0.017, Train_accy 99.57:  85%|████████▌ | 17/20 [03:21<00:35, 11.83s/it]
Task 26, Epoch 18/20 => Loss 0.013, Train_accy 99.64:  85%|████████▌ | 17/20 [03:33<00:35, 11.83s/it]
Task 26, Epoch 18/20 => Loss 0.013, Train_accy 99.64:  90%|█████████ | 18/20 [03:33<00:23, 11.82s/it]
Task 26, Epoch 19/20 => Loss 0.014, Train_accy 99.54:  90%|█████████ | 18/20 [03:44<00:23, 11.82s/it]
Task 26, Epoch 19/20 => Loss 0.014, Train_accy 99.54:  95%|█████████▌| 19/20 [03:44<00:11, 11.83s/it]
Task 26, Epoch 20/20 => Loss 0.021, Train_accy 99.41:  95%|█████████▌| 19/20 [03:56<00:11, 11.83s/it]
Task 26, Epoch 20/20 => Loss 0.021, Train_accy 99.41: 100%|██████████| 20/20 [03:56<00:00, 11.82s/it]
Task 26, Epoch 20/20 => Loss 0.021, Train_accy 99.41: 100%|██████████| 20/20 [03:56<00:00, 11.83s/it]
2024-08-12 02:29:01,412 [inflora.py] => Task 26, Epoch 20/20 => Loss 0.021, Train_accy 99.41
Threshold:  0.9933333333333333
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 35/768 type remove
Layer 3 : 109/768 type remove
Layer 4 : 134/768 type remove
Layer 5 : 176/768 type remove
Layer 6 : 204/768 type remove
Layer 7 : 227/768 type remove
Layer 8 : 273/768 type remove
Layer 9 : 361/768 type remove
Layer 10 : 351/768 type retain
Layer 11 : 290/768 type remove
Layer 12 : 245/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:29:22,553 [trainer.py] => Time:269.49625873565674
5387 5387
5387 5387
2024-08-12 02:29:33,497 [trainer.py] => Time:10.942940950393677
2024-08-12 02:29:33,497 [inflora.py] => Exemplar size: 0
2024-08-12 02:29:33,497 [trainer.py] => CNN: {'total': 62.41, '00-09': 73.0, '10-19': 72.0, '20-29': 62.5, '30-39': 64.0, '40-49': 62.5, '50-59': 73.37, '60-69': 70.5, '70-79': 68.0, '80-89': 62.81, '90-99': 60.8, '100-109': 58.79, '110-119': 50.75, '120-129': 63.5, '130-139': 72.5, '140-149': 59.3, '150-159': 46.73, '160-169': 54.77, '170-179': 40.5, '180-189': 73.23, '190-199': 67.0, '200-209': 55.78, '210-219': 28.5, '220-229': 62.81, '230-239': 78.5, '240-249': 56.5, '250-259': 72.36, '260-269': 74.0, 'old': 61.96, 'new': 74.0}
2024-08-12 02:29:33,497 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68, 74.49, 73.85, 72.44, 71.08, 69.46, 68.42, 68.55, 66.96, 65.41, 64.63, 64.31, 63.55, 63.16, 62.41]
2024-08-12 02:29:33,497 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11, 97.89, 97.76, 97.56, 97.7, 97.83, 97.78, 97.92, 97.61, 97.13, 97.19, 97.1, 97.29, 97.36, 97.31]
2024-08-12 02:29:33,497 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734, 0.7456171735241502, 0.7391449565798264, 0.7256498590667084, 0.7122641509433962, 0.6959910913140311, 0.6857519788918206, 0.6864661654135338, 0.6708044879446169, 0.6548188653451812, 0.6466870095902354, 0.6436925647451963, 0.6361267040898155, 0.6321573163678427, 0.6242806757007611]
2024-08-12 02:29:34,028 [trainer.py] => All params: 112239531
2024-08-12 02:29:34,033 [trainer.py] => Trainable params: 81418
2024-08-12 02:29:34,033 [inflora.py] => Learning on 270-280
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.27.weight', 'image_encoder.blocks.9.attn.lora_B_k.27.weight', 'image_encoder.blocks.6.attn.lora_B_k.27.weight', 'image_encoder.blocks.11.attn.lora_B_v.27.weight', 'image_encoder.blocks.8.attn.lora_B_k.27.weight', 'image_encoder.blocks.10.attn.lora_B_k.27.weight', 'image_encoder.blocks.0.attn.lora_B_v.27.weight', 'image_encoder.blocks.2.attn.lora_B_v.27.weight', 'image_encoder.blocks.2.attn.lora_B_k.27.weight', 'image_encoder.blocks.4.attn.lora_B_v.27.weight', 'image_encoder.blocks.11.attn.lora_B_k.27.weight', 'image_encoder.blocks.1.attn.lora_B_v.27.weight', 'image_encoder.blocks.5.attn.lora_B_v.27.weight', 'image_encoder.blocks.1.attn.lora_B_k.27.weight', 'image_encoder.blocks.4.attn.lora_B_k.27.weight', 'image_encoder.blocks.7.attn.lora_B_v.27.weight', 'image_encoder.blocks.6.attn.lora_B_v.27.weight', 'image_encoder.blocks.9.attn.lora_B_v.27.weight', 'image_encoder.blocks.7.attn.lora_B_k.27.weight', 'image_encoder.blocks.3.attn.lora_B_v.27.weight', 'image_encoder.blocks.10.attn.lora_B_v.27.weight', 'image_encoder.blocks.3.attn.lora_B_k.27.weight', 'classifier_pool.27.bias', 'image_encoder.blocks.0.attn.lora_B_k.27.weight', 'classifier_pool.27.weight', 'image_encoder.blocks.5.attn.lora_B_k.27.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 27, Epoch 1/20 => Loss 0.497, Train_accy 85.64:   0%|          | 0/20 [00:10<?, ?it/s]
Task 27, Epoch 1/20 => Loss 0.497, Train_accy 85.64:   5%|▌         | 1/20 [00:10<03:27, 10.91s/it]
Task 27, Epoch 2/20 => Loss 0.082, Train_accy 97.26:   5%|▌         | 1/20 [00:21<03:27, 10.91s/it]
Task 27, Epoch 2/20 => Loss 0.082, Train_accy 97.26:  10%|█         | 2/20 [00:21<03:15, 10.89s/it]
Task 27, Epoch 3/20 => Loss 0.065, Train_accy 97.84:  10%|█         | 2/20 [00:32<03:15, 10.89s/it]
Task 27, Epoch 3/20 => Loss 0.065, Train_accy 97.84:  15%|█▌        | 3/20 [00:32<03:04, 10.88s/it]
Task 27, Epoch 4/20 => Loss 0.047, Train_accy 98.49:  15%|█▌        | 3/20 [00:43<03:04, 10.88s/it]
Task 27, Epoch 4/20 => Loss 0.047, Train_accy 98.49:  20%|██        | 4/20 [00:43<02:55, 10.95s/it]
Task 27, Epoch 5/20 => Loss 0.049, Train_accy 98.56:  20%|██        | 4/20 [00:54<02:55, 10.95s/it]
Task 27, Epoch 5/20 => Loss 0.049, Train_accy 98.56:  25%|██▌       | 5/20 [00:54<02:44, 10.97s/it]
Task 27, Epoch 6/20 => Loss 0.034, Train_accy 98.96:  25%|██▌       | 5/20 [01:05<02:44, 10.97s/it]
Task 27, Epoch 6/20 => Loss 0.034, Train_accy 98.96:  30%|███       | 6/20 [01:05<02:33, 10.96s/it]
Task 27, Epoch 7/20 => Loss 0.041, Train_accy 98.63:  30%|███       | 6/20 [01:16<02:33, 10.96s/it]
Task 27, Epoch 7/20 => Loss 0.041, Train_accy 98.63:  35%|███▌      | 7/20 [01:16<02:22, 10.96s/it]
Task 27, Epoch 8/20 => Loss 0.028, Train_accy 99.17:  35%|███▌      | 7/20 [01:27<02:22, 10.96s/it]
Task 27, Epoch 8/20 => Loss 0.028, Train_accy 99.17:  40%|████      | 8/20 [01:27<02:11, 10.96s/it]
Task 27, Epoch 9/20 => Loss 0.037, Train_accy 98.81:  40%|████      | 8/20 [01:38<02:11, 10.96s/it]
Task 27, Epoch 9/20 => Loss 0.037, Train_accy 98.81:  45%|████▌     | 9/20 [01:38<02:01, 11.00s/it]
Task 27, Epoch 10/20 => Loss 0.035, Train_accy 98.92:  45%|████▌     | 9/20 [01:49<02:01, 11.00s/it]
Task 27, Epoch 10/20 => Loss 0.035, Train_accy 98.92:  50%|█████     | 10/20 [01:49<01:49, 10.99s/it]
Task 27, Epoch 11/20 => Loss 0.035, Train_accy 98.96:  50%|█████     | 10/20 [02:00<01:49, 10.99s/it]
Task 27, Epoch 11/20 => Loss 0.035, Train_accy 98.96:  55%|█████▌    | 11/20 [02:00<01:38, 10.96s/it]
Task 27, Epoch 12/20 => Loss 0.038, Train_accy 98.60:  55%|█████▌    | 11/20 [02:11<01:38, 10.96s/it]
Task 27, Epoch 12/20 => Loss 0.038, Train_accy 98.60:  60%|██████    | 12/20 [02:11<01:28, 11.01s/it]
Task 27, Epoch 13/20 => Loss 0.028, Train_accy 99.21:  60%|██████    | 12/20 [02:22<01:28, 11.01s/it]
Task 27, Epoch 13/20 => Loss 0.028, Train_accy 99.21:  65%|██████▌   | 13/20 [02:22<01:16, 11.00s/it]
Task 27, Epoch 14/20 => Loss 0.034, Train_accy 98.88:  65%|██████▌   | 13/20 [02:33<01:16, 11.00s/it]
Task 27, Epoch 14/20 => Loss 0.034, Train_accy 98.88:  70%|███████   | 14/20 [02:33<01:05, 10.98s/it]
Task 27, Epoch 15/20 => Loss 0.029, Train_accy 99.03:  70%|███████   | 14/20 [02:44<01:05, 10.98s/it]
Task 27, Epoch 15/20 => Loss 0.029, Train_accy 99.03:  75%|███████▌  | 15/20 [02:44<00:55, 11.01s/it]
Task 27, Epoch 16/20 => Loss 0.038, Train_accy 98.85:  75%|███████▌  | 15/20 [02:55<00:55, 11.01s/it]
Task 27, Epoch 16/20 => Loss 0.038, Train_accy 98.85:  80%|████████  | 16/20 [02:55<00:43, 10.98s/it]
Task 27, Epoch 17/20 => Loss 0.030, Train_accy 98.99:  80%|████████  | 16/20 [03:06<00:43, 10.98s/it]
Task 27, Epoch 17/20 => Loss 0.030, Train_accy 98.99:  85%|████████▌ | 17/20 [03:06<00:32, 10.96s/it]
Task 27, Epoch 18/20 => Loss 0.023, Train_accy 99.28:  85%|████████▌ | 17/20 [03:17<00:32, 10.96s/it]
Task 27, Epoch 18/20 => Loss 0.023, Train_accy 99.28:  90%|█████████ | 18/20 [03:17<00:21, 10.97s/it]
Task 27, Epoch 19/20 => Loss 0.024, Train_accy 99.10:  90%|█████████ | 18/20 [03:28<00:21, 10.97s/it]
Task 27, Epoch 19/20 => Loss 0.024, Train_accy 99.10:  95%|█████████▌| 19/20 [03:28<00:10, 10.95s/it]
Task 27, Epoch 20/20 => Loss 0.026, Train_accy 99.21:  95%|█████████▌| 19/20 [03:39<00:10, 10.95s/it]
Task 27, Epoch 20/20 => Loss 0.026, Train_accy 99.21: 100%|██████████| 20/20 [03:39<00:00, 10.94s/it]
Task 27, Epoch 20/20 => Loss 0.026, Train_accy 99.21: 100%|██████████| 20/20 [03:39<00:00, 10.96s/it]
2024-08-12 02:33:24,233 [inflora.py] => Task 27, Epoch 20/20 => Loss 0.026, Train_accy 99.21
Threshold:  0.995
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 41/768 type remove
Layer 3 : 120/768 type remove
Layer 4 : 152/768 type remove
Layer 5 : 197/768 type remove
Layer 6 : 228/768 type remove
Layer 7 : 259/768 type remove
Layer 8 : 309/768 type remove
Layer 9 : 369/768 type retain
Layer 10 : 311/768 type retain
Layer 11 : 340/768 type remove
Layer 12 : 290/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:33:43,965 [trainer.py] => Time:249.9320194721222
5586 5586
5586 5586
2024-08-12 02:33:55,503 [trainer.py] => Time:11.538235187530518
2024-08-12 02:33:55,503 [inflora.py] => Exemplar size: 0
2024-08-12 02:33:55,503 [trainer.py] => CNN: {'total': 62.41, '00-09': 72.5, '10-19': 71.5, '20-29': 62.0, '30-39': 62.5, '40-49': 54.5, '50-59': 71.36, '60-69': 71.0, '70-79': 67.0, '80-89': 62.81, '90-99': 61.31, '100-109': 60.8, '110-119': 51.76, '120-129': 63.5, '130-139': 72.0, '140-149': 58.29, '150-159': 46.23, '160-169': 55.28, '170-179': 41.5, '180-189': 72.22, '190-199': 67.0, '200-209': 55.28, '210-219': 28.5, '220-229': 62.31, '230-239': 78.0, '240-249': 55.5, '250-259': 72.36, '260-269': 75.0, '270-279': 75.38, 'old': 61.93, 'new': 75.38}
2024-08-12 02:33:55,504 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68, 74.49, 73.85, 72.44, 71.08, 69.46, 68.42, 68.55, 66.96, 65.41, 64.63, 64.31, 63.55, 63.16, 62.41, 62.41]
2024-08-12 02:33:55,504 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11, 97.89, 97.76, 97.56, 97.7, 97.83, 97.78, 97.92, 97.61, 97.13, 97.19, 97.1, 97.29, 97.36, 97.31, 97.4]
2024-08-12 02:33:55,504 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734, 0.7456171735241502, 0.7391449565798264, 0.7256498590667084, 0.7122641509433962, 0.6959910913140311, 0.6857519788918206, 0.6864661654135338, 0.6708044879446169, 0.6548188653451812, 0.6466870095902354, 0.6436925647451963, 0.6361267040898155, 0.6321573163678427, 0.6242806757007611, 0.6244181883279628]
2024-08-12 02:33:56,223 [trainer.py] => All params: 112239531
2024-08-12 02:33:56,228 [trainer.py] => Trainable params: 81418
2024-08-12 02:33:56,228 [inflora.py] => Learning on 280-290
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_v.28.weight', 'image_encoder.blocks.2.attn.lora_B_v.28.weight', 'image_encoder.blocks.0.attn.lora_B_v.28.weight', 'image_encoder.blocks.1.attn.lora_B_k.28.weight', 'image_encoder.blocks.11.attn.lora_B_k.28.weight', 'image_encoder.blocks.4.attn.lora_B_v.28.weight', 'image_encoder.blocks.4.attn.lora_B_k.28.weight', 'image_encoder.blocks.6.attn.lora_B_v.28.weight', 'image_encoder.blocks.7.attn.lora_B_v.28.weight', 'classifier_pool.28.bias', 'image_encoder.blocks.10.attn.lora_B_v.28.weight', 'image_encoder.blocks.11.attn.lora_B_v.28.weight', 'image_encoder.blocks.8.attn.lora_B_k.28.weight', 'image_encoder.blocks.9.attn.lora_B_v.28.weight', 'image_encoder.blocks.8.attn.lora_B_v.28.weight', 'image_encoder.blocks.7.attn.lora_B_k.28.weight', 'image_encoder.blocks.10.attn.lora_B_k.28.weight', 'image_encoder.blocks.5.attn.lora_B_v.28.weight', 'image_encoder.blocks.3.attn.lora_B_k.28.weight', 'image_encoder.blocks.9.attn.lora_B_k.28.weight', 'image_encoder.blocks.6.attn.lora_B_k.28.weight', 'image_encoder.blocks.3.attn.lora_B_v.28.weight', 'image_encoder.blocks.0.attn.lora_B_k.28.weight', 'image_encoder.blocks.5.attn.lora_B_k.28.weight', 'image_encoder.blocks.2.attn.lora_B_k.28.weight', 'classifier_pool.28.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 28, Epoch 1/20 => Loss 0.482, Train_accy 83.07:   0%|          | 0/20 [00:11<?, ?it/s]
Task 28, Epoch 1/20 => Loss 0.482, Train_accy 83.07:   5%|▌         | 1/20 [00:11<03:36, 11.41s/it]
Task 28, Epoch 2/20 => Loss 0.163, Train_accy 93.55:   5%|▌         | 1/20 [00:22<03:36, 11.41s/it]
Task 28, Epoch 2/20 => Loss 0.163, Train_accy 93.55:  10%|█         | 2/20 [00:22<03:25, 11.43s/it]
Task 28, Epoch 3/20 => Loss 0.133, Train_accy 94.87:  10%|█         | 2/20 [00:34<03:25, 11.43s/it]
Task 28, Epoch 3/20 => Loss 0.133, Train_accy 94.87:  15%|█▌        | 3/20 [00:34<03:14, 11.43s/it]
Task 28, Epoch 4/20 => Loss 0.118, Train_accy 95.46:  15%|█▌        | 3/20 [00:45<03:14, 11.43s/it]
Task 28, Epoch 4/20 => Loss 0.118, Train_accy 95.46:  20%|██        | 4/20 [00:45<03:02, 11.40s/it]
Task 28, Epoch 5/20 => Loss 0.109, Train_accy 95.66:  20%|██        | 4/20 [00:57<03:02, 11.40s/it]
Task 28, Epoch 5/20 => Loss 0.109, Train_accy 95.66:  25%|██▌       | 5/20 [00:57<02:51, 11.43s/it]
Task 28, Epoch 6/20 => Loss 0.103, Train_accy 96.08:  25%|██▌       | 5/20 [01:08<02:51, 11.43s/it]
Task 28, Epoch 6/20 => Loss 0.103, Train_accy 96.08:  30%|███       | 6/20 [01:08<02:39, 11.40s/it]
Task 28, Epoch 7/20 => Loss 0.087, Train_accy 96.67:  30%|███       | 6/20 [01:19<02:39, 11.40s/it]
Task 28, Epoch 7/20 => Loss 0.087, Train_accy 96.67:  35%|███▌      | 7/20 [01:19<02:28, 11.43s/it]
Task 28, Epoch 8/20 => Loss 0.095, Train_accy 96.53:  35%|███▌      | 7/20 [01:31<02:28, 11.43s/it]
Task 28, Epoch 8/20 => Loss 0.095, Train_accy 96.53:  40%|████      | 8/20 [01:31<02:16, 11.39s/it]
Task 28, Epoch 9/20 => Loss 0.075, Train_accy 96.84:  40%|████      | 8/20 [01:42<02:16, 11.39s/it]
Task 28, Epoch 9/20 => Loss 0.075, Train_accy 96.84:  45%|████▌     | 9/20 [01:42<02:05, 11.40s/it]
Task 28, Epoch 10/20 => Loss 0.080, Train_accy 96.95:  45%|████▌     | 9/20 [01:54<02:05, 11.40s/it]
Task 28, Epoch 10/20 => Loss 0.080, Train_accy 96.95:  50%|█████     | 10/20 [01:54<01:54, 11.44s/it]
Task 28, Epoch 11/20 => Loss 0.072, Train_accy 97.19:  50%|█████     | 10/20 [02:05<01:54, 11.44s/it]
Task 28, Epoch 11/20 => Loss 0.072, Train_accy 97.19:  55%|█████▌    | 11/20 [02:05<01:42, 11.40s/it]
Task 28, Epoch 12/20 => Loss 0.078, Train_accy 97.29:  55%|█████▌    | 11/20 [02:17<01:42, 11.40s/it]
Task 28, Epoch 12/20 => Loss 0.078, Train_accy 97.29:  60%|██████    | 12/20 [02:17<01:31, 11.45s/it]
Task 28, Epoch 13/20 => Loss 0.060, Train_accy 97.71:  60%|██████    | 12/20 [02:28<01:31, 11.45s/it]
Task 28, Epoch 13/20 => Loss 0.060, Train_accy 97.71:  65%|██████▌   | 13/20 [02:28<01:19, 11.42s/it]
Task 28, Epoch 14/20 => Loss 0.065, Train_accy 97.50:  65%|██████▌   | 13/20 [02:39<01:19, 11.42s/it]
Task 28, Epoch 14/20 => Loss 0.065, Train_accy 97.50:  70%|███████   | 14/20 [02:39<01:08, 11.45s/it]
Task 28, Epoch 15/20 => Loss 0.060, Train_accy 97.88:  70%|███████   | 14/20 [02:51<01:08, 11.45s/it]
Task 28, Epoch 15/20 => Loss 0.060, Train_accy 97.88:  75%|███████▌  | 15/20 [02:51<00:57, 11.40s/it]
Task 28, Epoch 16/20 => Loss 0.061, Train_accy 97.75:  75%|███████▌  | 15/20 [03:02<00:57, 11.40s/it]
Task 28, Epoch 16/20 => Loss 0.061, Train_accy 97.75:  80%|████████  | 16/20 [03:02<00:45, 11.46s/it]
Task 28, Epoch 17/20 => Loss 0.063, Train_accy 97.57:  80%|████████  | 16/20 [03:14<00:45, 11.46s/it]
Task 28, Epoch 17/20 => Loss 0.063, Train_accy 97.57:  85%|████████▌ | 17/20 [03:14<00:34, 11.42s/it]
Task 28, Epoch 18/20 => Loss 0.064, Train_accy 97.43:  85%|████████▌ | 17/20 [03:25<00:34, 11.42s/it]
Task 28, Epoch 18/20 => Loss 0.064, Train_accy 97.43:  90%|█████████ | 18/20 [03:25<00:22, 11.45s/it]
Task 28, Epoch 19/20 => Loss 0.064, Train_accy 98.09:  90%|█████████ | 18/20 [03:37<00:22, 11.45s/it]
Task 28, Epoch 19/20 => Loss 0.064, Train_accy 98.09:  95%|█████████▌| 19/20 [03:37<00:11, 11.44s/it]
Task 28, Epoch 20/20 => Loss 0.051, Train_accy 98.16:  95%|█████████▌| 19/20 [03:48<00:11, 11.44s/it]
Task 28, Epoch 20/20 => Loss 0.051, Train_accy 98.16: 100%|██████████| 20/20 [03:48<00:00, 11.51s/it]
Task 28, Epoch 20/20 => Loss 0.051, Train_accy 98.16: 100%|██████████| 20/20 [03:48<00:00, 11.44s/it]
2024-08-12 02:37:56,440 [inflora.py] => Task 28, Epoch 20/20 => Loss 0.051, Train_accy 98.16
Threshold:  0.9966666666666667
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 13/768 type remove
Layer 2 : 50/768 type remove
Layer 3 : 149/768 type remove
Layer 4 : 193/768 type remove
Layer 5 : 243/768 type remove
Layer 6 : 277/768 type remove
Layer 7 : 312/768 type remove
Layer 8 : 368/768 type remove
Layer 9 : 297/768 type retain
Layer 10 : 231/768 type retain
Layer 11 : 343/768 type retain
Layer 12 : 339/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:38:17,093 [trainer.py] => Time:260.8645668029785
5786 5786
5786 5786
2024-08-12 02:38:29,264 [trainer.py] => Time:12.17129921913147
2024-08-12 02:38:29,264 [inflora.py] => Exemplar size: 0
2024-08-12 02:38:29,264 [trainer.py] => CNN: {'total': 62.39, '00-09': 72.0, '10-19': 73.0, '20-29': 61.5, '30-39': 63.5, '40-49': 57.0, '50-59': 73.37, '60-69': 72.0, '70-79': 68.0, '80-89': 61.81, '90-99': 60.8, '100-109': 57.79, '110-119': 51.76, '120-129': 64.5, '130-139': 72.5, '140-149': 58.29, '150-159': 47.74, '160-169': 55.78, '170-179': 43.0, '180-189': 71.21, '190-199': 67.5, '200-209': 56.28, '210-219': 27.0, '220-229': 68.34, '230-239': 74.5, '240-249': 56.5, '250-259': 72.86, '260-269': 74.5, '270-279': 75.88, '280-289': 50.5, 'old': 62.82, 'new': 50.5}
2024-08-12 02:38:29,265 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68, 74.49, 73.85, 72.44, 71.08, 69.46, 68.42, 68.55, 66.96, 65.41, 64.63, 64.31, 63.55, 63.16, 62.41, 62.41, 62.39]
2024-08-12 02:38:29,265 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11, 97.89, 97.76, 97.56, 97.7, 97.83, 97.78, 97.92, 97.61, 97.13, 97.19, 97.1, 97.29, 97.36, 97.31, 97.4, 97.42]
2024-08-12 02:38:29,265 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734, 0.7456171735241502, 0.7391449565798264, 0.7256498590667084, 0.7122641509433962, 0.6959910913140311, 0.6857519788918206, 0.6864661654135338, 0.6708044879446169, 0.6548188653451812, 0.6466870095902354, 0.6436925647451963, 0.6361267040898155, 0.6321573163678427, 0.6242806757007611, 0.6244181883279628, 0.6242654683719322]
2024-08-12 02:38:29,798 [trainer.py] => All params: 112239531
2024-08-12 02:38:29,803 [trainer.py] => Trainable params: 81418
2024-08-12 02:38:29,803 [inflora.py] => Learning on 290-300
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_k.29.weight', 'image_encoder.blocks.0.attn.lora_B_v.29.weight', 'image_encoder.blocks.7.attn.lora_B_k.29.weight', 'image_encoder.blocks.1.attn.lora_B_k.29.weight', 'image_encoder.blocks.6.attn.lora_B_k.29.weight', 'image_encoder.blocks.9.attn.lora_B_v.29.weight', 'image_encoder.blocks.4.attn.lora_B_k.29.weight', 'image_encoder.blocks.0.attn.lora_B_k.29.weight', 'image_encoder.blocks.2.attn.lora_B_v.29.weight', 'image_encoder.blocks.5.attn.lora_B_v.29.weight', 'image_encoder.blocks.3.attn.lora_B_k.29.weight', 'classifier_pool.29.weight', 'image_encoder.blocks.1.attn.lora_B_v.29.weight', 'classifier_pool.29.bias', 'image_encoder.blocks.4.attn.lora_B_v.29.weight', 'image_encoder.blocks.11.attn.lora_B_k.29.weight', 'image_encoder.blocks.9.attn.lora_B_k.29.weight', 'image_encoder.blocks.10.attn.lora_B_v.29.weight', 'image_encoder.blocks.3.attn.lora_B_v.29.weight', 'image_encoder.blocks.8.attn.lora_B_v.29.weight', 'image_encoder.blocks.10.attn.lora_B_k.29.weight', 'image_encoder.blocks.7.attn.lora_B_v.29.weight', 'image_encoder.blocks.5.attn.lora_B_k.29.weight', 'image_encoder.blocks.11.attn.lora_B_v.29.weight', 'image_encoder.blocks.2.attn.lora_B_k.29.weight', 'image_encoder.blocks.6.attn.lora_B_v.29.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 29, Epoch 1/20 => Loss 0.512, Train_accy 84.10:   0%|          | 0/20 [00:11<?, ?it/s]
Task 29, Epoch 1/20 => Loss 0.512, Train_accy 84.10:   5%|▌         | 1/20 [00:11<03:41, 11.65s/it]
Task 29, Epoch 2/20 => Loss 0.154, Train_accy 94.42:   5%|▌         | 1/20 [00:23<03:41, 11.65s/it]
Task 29, Epoch 2/20 => Loss 0.154, Train_accy 94.42:  10%|█         | 2/20 [00:23<03:30, 11.67s/it]
Task 29, Epoch 3/20 => Loss 0.134, Train_accy 94.89:  10%|█         | 2/20 [00:34<03:30, 11.67s/it]
Task 29, Epoch 3/20 => Loss 0.134, Train_accy 94.89:  15%|█▌        | 3/20 [00:34<03:17, 11.62s/it]
Task 29, Epoch 4/20 => Loss 0.125, Train_accy 95.30:  15%|█▌        | 3/20 [00:46<03:17, 11.62s/it]
Task 29, Epoch 4/20 => Loss 0.125, Train_accy 95.30:  20%|██        | 4/20 [00:46<03:05, 11.62s/it]
Task 29, Epoch 5/20 => Loss 0.117, Train_accy 95.84:  20%|██        | 4/20 [00:58<03:05, 11.62s/it]
Task 29, Epoch 5/20 => Loss 0.117, Train_accy 95.84:  25%|██▌       | 5/20 [00:58<02:54, 11.65s/it]
Task 29, Epoch 6/20 => Loss 0.097, Train_accy 96.79:  25%|██▌       | 5/20 [01:10<02:54, 11.65s/it]
Task 29, Epoch 6/20 => Loss 0.097, Train_accy 96.79:  30%|███       | 6/20 [01:10<02:43, 11.71s/it]
Task 29, Epoch 7/20 => Loss 0.089, Train_accy 96.79:  30%|███       | 6/20 [01:21<02:43, 11.71s/it]
Task 29, Epoch 7/20 => Loss 0.089, Train_accy 96.79:  35%|███▌      | 7/20 [01:21<02:32, 11.72s/it]
Task 29, Epoch 8/20 => Loss 0.091, Train_accy 97.19:  35%|███▌      | 7/20 [01:33<02:32, 11.72s/it]
Task 29, Epoch 8/20 => Loss 0.091, Train_accy 97.19:  40%|████      | 8/20 [01:33<02:20, 11.72s/it]
Task 29, Epoch 9/20 => Loss 0.082, Train_accy 97.12:  40%|████      | 8/20 [01:45<02:20, 11.72s/it]
Task 29, Epoch 9/20 => Loss 0.082, Train_accy 97.12:  45%|████▌     | 9/20 [01:45<02:09, 11.74s/it]
Task 29, Epoch 10/20 => Loss 0.075, Train_accy 97.19:  45%|████▌     | 9/20 [01:56<02:09, 11.74s/it]
Task 29, Epoch 10/20 => Loss 0.075, Train_accy 97.19:  50%|█████     | 10/20 [01:56<01:56, 11.69s/it]
Task 29, Epoch 11/20 => Loss 0.090, Train_accy 96.85:  50%|█████     | 10/20 [02:08<01:56, 11.69s/it]
Task 29, Epoch 11/20 => Loss 0.090, Train_accy 96.85:  55%|█████▌    | 11/20 [02:08<01:45, 11.76s/it]
Task 29, Epoch 12/20 => Loss 0.085, Train_accy 97.06:  55%|█████▌    | 11/20 [02:20<01:45, 11.76s/it]
Task 29, Epoch 12/20 => Loss 0.085, Train_accy 97.06:  60%|██████    | 12/20 [02:20<01:33, 11.72s/it]
Task 29, Epoch 13/20 => Loss 0.070, Train_accy 97.50:  60%|██████    | 12/20 [02:32<01:33, 11.72s/it]
Task 29, Epoch 13/20 => Loss 0.070, Train_accy 97.50:  65%|██████▌   | 13/20 [02:32<01:22, 11.78s/it]
Task 29, Epoch 14/20 => Loss 0.078, Train_accy 97.16:  65%|██████▌   | 13/20 [02:44<01:22, 11.78s/it]
Task 29, Epoch 14/20 => Loss 0.078, Train_accy 97.16:  70%|███████   | 14/20 [02:44<01:10, 11.80s/it]
Task 29, Epoch 15/20 => Loss 0.086, Train_accy 96.82:  70%|███████   | 14/20 [02:55<01:10, 11.80s/it]
Task 29, Epoch 15/20 => Loss 0.086, Train_accy 96.82:  75%|███████▌  | 15/20 [02:55<00:58, 11.76s/it]
Task 29, Epoch 16/20 => Loss 0.080, Train_accy 97.23:  75%|███████▌  | 15/20 [03:07<00:58, 11.76s/it]
Task 29, Epoch 16/20 => Loss 0.080, Train_accy 97.23:  80%|████████  | 16/20 [03:07<00:46, 11.75s/it]
Task 29, Epoch 17/20 => Loss 0.068, Train_accy 97.90:  80%|████████  | 16/20 [03:19<00:46, 11.75s/it]
Task 29, Epoch 17/20 => Loss 0.068, Train_accy 97.90:  85%|████████▌ | 17/20 [03:19<00:35, 11.72s/it]
Task 29, Epoch 18/20 => Loss 0.066, Train_accy 97.36:  85%|████████▌ | 17/20 [03:30<00:35, 11.72s/it]
Task 29, Epoch 18/20 => Loss 0.066, Train_accy 97.36:  90%|█████████ | 18/20 [03:30<00:23, 11.68s/it]
Task 29, Epoch 19/20 => Loss 0.067, Train_accy 98.07:  90%|█████████ | 18/20 [03:42<00:23, 11.68s/it]
Task 29, Epoch 19/20 => Loss 0.067, Train_accy 98.07:  95%|█████████▌| 19/20 [03:42<00:11, 11.70s/it]
Task 29, Epoch 20/20 => Loss 0.061, Train_accy 97.90:  95%|█████████▌| 19/20 [03:54<00:11, 11.70s/it]
Task 29, Epoch 20/20 => Loss 0.061, Train_accy 97.90: 100%|██████████| 20/20 [03:54<00:00, 11.67s/it]
Task 29, Epoch 20/20 => Loss 0.061, Train_accy 97.90: 100%|██████████| 20/20 [03:54<00:00, 11.71s/it]
2024-08-12 02:42:35,600 [inflora.py] => Task 29, Epoch 20/20 => Loss 0.061, Train_accy 97.90
Threshold:  0.9983333333333333
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 14/768 type remove
Layer 2 : 67/768 type remove
Layer 3 : 178/768 type remove
Layer 4 : 240/768 type remove
Layer 5 : 299/768 type remove
Layer 6 : 335/768 type remove
Layer 7 : 376/768 type remove
Layer 8 : 342/768 type retain
Layer 9 : 253/768 type retain
Layer 10 : 193/768 type retain
Layer 11 : 283/768 type retain
Layer 12 : 358/768 type retain
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:42:58,032 [trainer.py] => Time:268.22967624664307
5985 5985
5985 5985
2024-08-12 02:43:10,132 [trainer.py] => Time:12.098803758621216
2024-08-12 02:43:10,132 [inflora.py] => Exemplar size: 0
2024-08-12 02:43:10,132 [trainer.py] => CNN: {'total': 61.95, '00-09': 71.0, '10-19': 72.5, '20-29': 61.5, '30-39': 62.5, '40-49': 56.5, '50-59': 72.36, '60-69': 70.5, '70-79': 67.5, '80-89': 61.81, '90-99': 59.3, '100-109': 60.8, '110-119': 52.26, '120-129': 65.0, '130-139': 72.0, '140-149': 57.29, '150-159': 45.23, '160-169': 55.28, '170-179': 43.0, '180-189': 71.72, '190-199': 68.0, '200-209': 56.28, '210-219': 27.5, '220-229': 67.34, '230-239': 74.5, '240-249': 55.0, '250-259': 70.35, '260-269': 73.5, '270-279': 74.87, '280-289': 50.5, '290-299': 62.81, 'old': 61.93, 'new': 62.81}
2024-08-12 02:43:10,132 [trainer.py] => CNN top1 curve: [99.0, 91.75, 86.83, 86.62, 81.5, 81.32, 80.2, 80.74, 78.7, 76.87, 77.14, 75.82, 75.68, 74.49, 73.85, 72.44, 71.08, 69.46, 68.42, 68.55, 66.96, 65.41, 64.63, 64.31, 63.55, 63.16, 62.41, 62.41, 62.39, 61.95]
2024-08-12 02:43:10,132 [trainer.py] => CNN top1 with task curve: [99.0, 98.5, 97.67, 97.75, 97.8, 98.17, 98.21, 98.25, 98.33, 98.15, 98.13, 98.16, 98.11, 97.89, 97.76, 97.56, 97.7, 97.83, 97.78, 97.92, 97.61, 97.13, 97.19, 97.1, 97.29, 97.36, 97.31, 97.4, 97.42, 97.44]
2024-08-12 02:43:10,132 [trainer.py] => CNN top1 task curve: [1.0, 0.9275, 0.8733333333333333, 0.87, 0.819, 0.8156797331109258, 0.8041458184417442, 0.8086303939962477, 0.7886540600667408, 0.7701552328492739, 0.7723132969034608, 0.7590814196242172, 0.7583815028901734, 0.7456171735241502, 0.7391449565798264, 0.7256498590667084, 0.7122641509433962, 0.6959910913140311, 0.6857519788918206, 0.6864661654135338, 0.6708044879446169, 0.6548188653451812, 0.6466870095902354, 0.6436925647451963, 0.6361267040898155, 0.6321573163678427, 0.6242806757007611, 0.6244181883279628, 0.6242654683719322, 0.6198830409356725]
Traceback (most recent call last):
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/main.py", line 33, in <module>
    main()
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/main.py", line 11, in main
    train(args)
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/trainer.py", line 28, in train
    _set_random(args["seed"])
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/trainer.py", line 101, in _set_random
    torch.manual_seed(args['seed'])
TypeError: 'int' object is not subscriptable
logs/vtab/5_5_sip/InfLoRA/adam/4/0.95_1.0-0.0005/1993
2024-08-12 02:43:13,935 [trainer.py] => config: ./configs/vtab_inflora.json
2024-08-12 02:43:13,940 [trainer.py] => device: [device(type='cuda', index=0)]
2024-08-12 02:43:13,940 [trainer.py] => prefix: reproduce
2024-08-12 02:43:13,940 [trainer.py] => dataset: vtab
2024-08-12 02:43:13,940 [trainer.py] => data_path: /mnt/mydisk/ruoheng.li/lrh/Dataset
2024-08-12 02:43:13,940 [trainer.py] => memory_size: 0
2024-08-12 02:43:13,940 [trainer.py] => memory_per_class: 0
2024-08-12 02:43:13,940 [trainer.py] => fixed_memory: True
2024-08-12 02:43:13,940 [trainer.py] => shuffle: True
2024-08-12 02:43:13,940 [trainer.py] => init_cls: 5
2024-08-12 02:43:13,940 [trainer.py] => increment: 5
2024-08-12 02:43:13,940 [trainer.py] => model_name: InfLoRA
2024-08-12 02:43:13,940 [trainer.py] => net_type: sip
2024-08-12 02:43:13,940 [trainer.py] => embd_dim: 768
2024-08-12 02:43:13,940 [trainer.py] => num_heads: 12
2024-08-12 02:43:13,940 [trainer.py] => total_sessions: 10
2024-08-12 02:43:13,940 [trainer.py] => seed: 1993
2024-08-12 02:43:13,940 [trainer.py] => EPSILON: 1e-08
2024-08-12 02:43:13,940 [trainer.py] => init_epoch: 20
2024-08-12 02:43:13,940 [trainer.py] => optim: adam
2024-08-12 02:43:13,940 [trainer.py] => init_lr: 0.0005
2024-08-12 02:43:13,940 [trainer.py] => init_lr_decay: 0.1
2024-08-12 02:43:13,940 [trainer.py] => init_weight_decay: 0.0
2024-08-12 02:43:13,940 [trainer.py] => epochs: 20
2024-08-12 02:43:13,940 [trainer.py] => lrate: 0.0005
2024-08-12 02:43:13,940 [trainer.py] => lrate_decay: 0.1
2024-08-12 02:43:13,941 [trainer.py] => batch_size: 48
2024-08-12 02:43:13,941 [trainer.py] => weight_decay: 0.0
2024-08-12 02:43:13,941 [trainer.py] => rank: 4
2024-08-12 02:43:13,941 [trainer.py] => lamb: 0.95
2024-08-12 02:43:13,941 [trainer.py] => lame: 1.0
2024-08-12 02:43:13,941 [trainer.py] => num_workers: 16
{'10': 0, '11': 1, '12': 2, '13': 3, '14': 4, '15': 5, '16': 6, '17': 7, '18': 8, '19': 9, '20': 10, '21': 11, '22': 12, '23': 13, '24': 14, '25': 15, '26': 16, '27': 17, '28': 18, '29': 19, '30': 20, '31': 21, '32': 22, '33': 23, '34': 24, '35': 25, '36': 26, '37': 27, '38': 28, '39': 29, '40': 30, '41': 31, '42': 32, '43': 33, '44': 34, '45': 35, '46': 36, '47': 37, '48': 38, '49': 39, '50': 40, '51': 41, '52': 42, '53': 43, '54': 44, '55': 45, '56': 46, '57': 47, '58': 48, '59': 49}
{'10': 0, '11': 1, '12': 2, '13': 3, '14': 4, '15': 5, '16': 6, '17': 7, '18': 8, '19': 9, '20': 10, '21': 11, '22': 12, '23': 13, '24': 14, '25': 15, '26': 16, '27': 17, '28': 18, '29': 19, '30': 20, '31': 21, '32': 22, '33': 23, '34': 24, '35': 25, '36': 26, '37': 27, '38': 28, '39': 29, '40': 30, '41': 31, '42': 32, '43': 33, '44': 34, '45': 35, '46': 36, '47': 37, '48': 38, '49': 39, '50': 40, '51': 41, '52': 42, '53': 43, '54': 44, '55': 45, '56': 46, '57': 47, '58': 48, '59': 49}
2024-08-12 02:43:13,965 [data_manager.py] => [43, 41, 23, 14, 13, 40, 42, 22, 16, 45, 17, 10, 27, 46, 35, 8, 2, 34, 1, 37, 21, 0, 18, 36, 38, 24, 12, 6, 15, 20, 25, 48, 30, 19, 44, 26, 7, 28, 11, 5, 32, 4, 9, 47, 39, 31, 3, 29, 49, 33]
2024-08-12 02:43:15,944 [trainer.py] => All params: 108905911
2024-08-12 02:43:15,946 [trainer.py] => Trainable params: 108905911
2024-08-12 02:43:15,946 [inflora.py] => Learning on 0-5
Parameters to be updated: {'image_encoder.blocks.2.attn.lora_B_k.0.weight', 'image_encoder.blocks.5.attn.lora_B_v.0.weight', 'image_encoder.blocks.11.attn.lora_B_k.0.weight', 'image_encoder.blocks.9.attn.lora_B_v.0.weight', 'image_encoder.blocks.1.attn.lora_B_v.0.weight', 'image_encoder.blocks.9.attn.lora_B_k.0.weight', 'image_encoder.blocks.3.attn.lora_B_k.0.weight', 'image_encoder.blocks.6.attn.lora_B_k.0.weight', 'image_encoder.blocks.4.attn.lora_B_v.0.weight', 'image_encoder.blocks.10.attn.lora_B_v.0.weight', 'image_encoder.blocks.7.attn.lora_B_v.0.weight', 'image_encoder.blocks.6.attn.lora_B_v.0.weight', 'image_encoder.blocks.8.attn.lora_B_v.0.weight', 'image_encoder.blocks.0.attn.lora_B_k.0.weight', 'image_encoder.blocks.8.attn.lora_B_k.0.weight', 'image_encoder.blocks.5.attn.lora_B_k.0.weight', 'classifier_pool.0.bias', 'image_encoder.blocks.10.attn.lora_B_k.0.weight', 'image_encoder.blocks.7.attn.lora_B_k.0.weight', 'image_encoder.blocks.1.attn.lora_B_k.0.weight', 'classifier_pool.0.weight', 'image_encoder.blocks.11.attn.lora_B_v.0.weight', 'image_encoder.blocks.4.attn.lora_B_k.0.weight', 'image_encoder.blocks.0.attn.lora_B_v.0.weight', 'image_encoder.blocks.2.attn.lora_B_v.0.weight', 'image_encoder.blocks.3.attn.lora_B_v.0.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 0, Epoch 1/20 => Loss 1.451, Train_accy 42.86:   0%|          | 0/20 [00:00<?, ?it/s]
Task 0, Epoch 1/20 => Loss 1.451, Train_accy 42.86:   5%|▌         | 1/20 [00:00<00:17,  1.07it/s]
Task 0, Epoch 2/20 => Loss 0.821, Train_accy 73.63:   5%|▌         | 1/20 [00:01<00:17,  1.07it/s]
Task 0, Epoch 2/20 => Loss 0.821, Train_accy 73.63:  10%|█         | 2/20 [00:01<00:15,  1.14it/s]
Task 0, Epoch 3/20 => Loss 0.555, Train_accy 86.81:  10%|█         | 2/20 [00:02<00:15,  1.14it/s]
Task 0, Epoch 3/20 => Loss 0.555, Train_accy 86.81:  15%|█▌        | 3/20 [00:02<00:14,  1.15it/s]
Task 0, Epoch 4/20 => Loss 0.360, Train_accy 92.31:  15%|█▌        | 3/20 [00:03<00:14,  1.15it/s]
Task 0, Epoch 4/20 => Loss 0.360, Train_accy 92.31:  20%|██        | 4/20 [00:03<00:13,  1.17it/s]
Task 0, Epoch 5/20 => Loss 0.218, Train_accy 95.60:  20%|██        | 4/20 [00:04<00:13,  1.17it/s]
Task 0, Epoch 5/20 => Loss 0.218, Train_accy 95.60:  25%|██▌       | 5/20 [00:04<00:12,  1.18it/s]
Task 0, Epoch 6/20 => Loss 0.151, Train_accy 95.60:  25%|██▌       | 5/20 [00:05<00:12,  1.18it/s]
Task 0, Epoch 6/20 => Loss 0.151, Train_accy 95.60:  30%|███       | 6/20 [00:05<00:11,  1.17it/s]
Task 0, Epoch 7/20 => Loss 0.091, Train_accy 98.90:  30%|███       | 6/20 [00:06<00:11,  1.17it/s]
Task 0, Epoch 7/20 => Loss 0.091, Train_accy 98.90:  35%|███▌      | 7/20 [00:06<00:11,  1.14it/s]
Task 0, Epoch 8/20 => Loss 0.110, Train_accy 97.80:  35%|███▌      | 7/20 [00:06<00:11,  1.14it/s]
Task 0, Epoch 8/20 => Loss 0.110, Train_accy 97.80:  40%|████      | 8/20 [00:06<00:10,  1.14it/s]
Task 0, Epoch 9/20 => Loss 0.033, Train_accy 100.00:  40%|████      | 8/20 [00:07<00:10,  1.14it/s]
Task 0, Epoch 9/20 => Loss 0.033, Train_accy 100.00:  45%|████▌     | 9/20 [00:07<00:09,  1.15it/s]
Task 0, Epoch 10/20 => Loss 0.018, Train_accy 100.00:  45%|████▌     | 9/20 [00:08<00:09,  1.15it/s]
Task 0, Epoch 10/20 => Loss 0.018, Train_accy 100.00:  50%|█████     | 10/20 [00:08<00:08,  1.13it/s]
Task 0, Epoch 11/20 => Loss 0.013, Train_accy 100.00:  50%|█████     | 10/20 [00:09<00:08,  1.13it/s]
Task 0, Epoch 11/20 => Loss 0.013, Train_accy 100.00:  55%|█████▌    | 11/20 [00:09<00:08,  1.12it/s]
Task 0, Epoch 12/20 => Loss 0.082, Train_accy 97.80:  55%|█████▌    | 11/20 [00:10<00:08,  1.12it/s] 
Task 0, Epoch 12/20 => Loss 0.082, Train_accy 97.80:  60%|██████    | 12/20 [00:10<00:07,  1.13it/s]
Task 0, Epoch 13/20 => Loss 0.017, Train_accy 100.00:  60%|██████    | 12/20 [00:11<00:07,  1.13it/s]
Task 0, Epoch 13/20 => Loss 0.017, Train_accy 100.00:  65%|██████▌   | 13/20 [00:11<00:06,  1.14it/s]
Task 0, Epoch 14/20 => Loss 0.007, Train_accy 100.00:  65%|██████▌   | 13/20 [00:12<00:06,  1.14it/s]
Task 0, Epoch 14/20 => Loss 0.007, Train_accy 100.00:  70%|███████   | 14/20 [00:12<00:05,  1.13it/s]
Task 0, Epoch 15/20 => Loss 0.011, Train_accy 100.00:  70%|███████   | 14/20 [00:13<00:05,  1.13it/s]
Task 0, Epoch 15/20 => Loss 0.011, Train_accy 100.00:  75%|███████▌  | 15/20 [00:13<00:04,  1.14it/s]
Task 0, Epoch 16/20 => Loss 0.051, Train_accy 97.80:  75%|███████▌  | 15/20 [00:14<00:04,  1.14it/s] 
Task 0, Epoch 16/20 => Loss 0.051, Train_accy 97.80:  80%|████████  | 16/20 [00:14<00:03,  1.14it/s]
Task 0, Epoch 17/20 => Loss 0.020, Train_accy 98.90:  80%|████████  | 16/20 [00:14<00:03,  1.14it/s]
Task 0, Epoch 17/20 => Loss 0.020, Train_accy 98.90:  85%|████████▌ | 17/20 [00:14<00:02,  1.15it/s]
Task 0, Epoch 18/20 => Loss 0.023, Train_accy 98.90:  85%|████████▌ | 17/20 [00:15<00:02,  1.15it/s]
Task 0, Epoch 18/20 => Loss 0.023, Train_accy 98.90:  90%|█████████ | 18/20 [00:15<00:01,  1.16it/s]
Task 0, Epoch 19/20 => Loss 0.038, Train_accy 98.90:  90%|█████████ | 18/20 [00:16<00:01,  1.16it/s]
Task 0, Epoch 19/20 => Loss 0.038, Train_accy 98.90:  95%|█████████▌| 19/20 [00:16<00:00,  1.15it/s]
Task 0, Epoch 20/20 => Loss 0.017, Train_accy 98.90:  95%|█████████▌| 19/20 [00:17<00:00,  1.15it/s]
Task 0, Epoch 20/20 => Loss 0.017, Train_accy 98.90: 100%|██████████| 20/20 [00:17<00:00,  1.13it/s]
Task 0, Epoch 20/20 => Loss 0.017, Train_accy 98.90: 100%|██████████| 20/20 [00:17<00:00,  1.14it/s]
2024-08-12 02:43:36,781 [inflora.py] => Task 0, Epoch 20/20 => Loss 0.017, Train_accy 98.90
Threshold:  0.95
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 6/768 type remove
Layer 2 : 10/768 type remove
Layer 3 : 12/768 type remove
Layer 4 : 13/768 type remove
Layer 5 : 21/768 type remove
Layer 6 : 21/768 type remove
Layer 7 : 21/768 type remove
Layer 8 : 28/768 type remove
Layer 9 : 39/768 type remove
Layer 10 : 30/768 type remove
Layer 11 : 17/768 type remove
Layer 12 : 8/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:43:41,047 [trainer.py] => Time:25.100494861602783
286 286
286 286
2024-08-12 02:43:42,500 [trainer.py] => Time:1.4530529975891113
2024-08-12 02:43:42,500 [inflora.py] => Exemplar size: 0
2024-08-12 02:43:42,500 [trainer.py] => CNN: {'total': 97.9, '00-04': 97.9, 'old': 0, 'new': 97.9}
2024-08-12 02:43:42,500 [trainer.py] => CNN top1 curve: [97.9]
2024-08-12 02:43:42,500 [trainer.py] => CNN top1 with task curve: [97.9]
2024-08-12 02:43:42,500 [trainer.py] => CNN top1 task curve: [1.0]
2024-08-12 02:43:43,048 [trainer.py] => All params: 108905911
2024-08-12 02:43:43,050 [trainer.py] => Trainable params: 77573
2024-08-12 02:43:43,050 [inflora.py] => Learning on 5-10
Parameters to be updated: {'image_encoder.blocks.4.attn.lora_B_v.1.weight', 'image_encoder.blocks.3.attn.lora_B_v.1.weight', 'image_encoder.blocks.8.attn.lora_B_k.1.weight', 'classifier_pool.1.weight', 'image_encoder.blocks.10.attn.lora_B_k.1.weight', 'image_encoder.blocks.11.attn.lora_B_k.1.weight', 'image_encoder.blocks.11.attn.lora_B_v.1.weight', 'image_encoder.blocks.7.attn.lora_B_k.1.weight', 'image_encoder.blocks.5.attn.lora_B_k.1.weight', 'image_encoder.blocks.8.attn.lora_B_v.1.weight', 'image_encoder.blocks.1.attn.lora_B_v.1.weight', 'image_encoder.blocks.1.attn.lora_B_k.1.weight', 'image_encoder.blocks.4.attn.lora_B_k.1.weight', 'image_encoder.blocks.3.attn.lora_B_k.1.weight', 'image_encoder.blocks.9.attn.lora_B_k.1.weight', 'image_encoder.blocks.2.attn.lora_B_v.1.weight', 'image_encoder.blocks.6.attn.lora_B_k.1.weight', 'image_encoder.blocks.7.attn.lora_B_v.1.weight', 'image_encoder.blocks.6.attn.lora_B_v.1.weight', 'image_encoder.blocks.5.attn.lora_B_v.1.weight', 'image_encoder.blocks.2.attn.lora_B_k.1.weight', 'image_encoder.blocks.9.attn.lora_B_v.1.weight', 'classifier_pool.1.bias', 'image_encoder.blocks.0.attn.lora_B_v.1.weight', 'image_encoder.blocks.0.attn.lora_B_k.1.weight', 'image_encoder.blocks.10.attn.lora_B_v.1.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 1, Epoch 1/20 => Loss 1.902, Train_accy 22.86:   0%|          | 0/20 [00:00<?, ?it/s]
Task 1, Epoch 1/20 => Loss 1.902, Train_accy 22.86:   5%|▌         | 1/20 [00:00<00:17,  1.08it/s]
Task 1, Epoch 2/20 => Loss 1.068, Train_accy 54.29:   5%|▌         | 1/20 [00:01<00:17,  1.08it/s]
Task 1, Epoch 2/20 => Loss 1.068, Train_accy 54.29:  10%|█         | 2/20 [00:01<00:16,  1.09it/s]
Task 1, Epoch 3/20 => Loss 0.628, Train_accy 84.29:  10%|█         | 2/20 [00:02<00:16,  1.09it/s]
Task 1, Epoch 3/20 => Loss 0.628, Train_accy 84.29:  15%|█▌        | 3/20 [00:02<00:15,  1.08it/s]
Task 1, Epoch 4/20 => Loss 0.278, Train_accy 98.57:  15%|█▌        | 3/20 [00:03<00:15,  1.08it/s]
Task 1, Epoch 4/20 => Loss 0.278, Train_accy 98.57:  20%|██        | 4/20 [00:03<00:14,  1.08it/s]
Task 1, Epoch 5/20 => Loss 0.140, Train_accy 100.00:  20%|██        | 4/20 [00:04<00:14,  1.08it/s]
Task 1, Epoch 5/20 => Loss 0.140, Train_accy 100.00:  25%|██▌       | 5/20 [00:04<00:14,  1.06it/s]
Task 1, Epoch 6/20 => Loss 0.065, Train_accy 100.00:  25%|██▌       | 5/20 [00:05<00:14,  1.06it/s]
Task 1, Epoch 6/20 => Loss 0.065, Train_accy 100.00:  30%|███       | 6/20 [00:05<00:13,  1.05it/s]
Task 1, Epoch 7/20 => Loss 0.038, Train_accy 100.00:  30%|███       | 6/20 [00:06<00:13,  1.05it/s]
Task 1, Epoch 7/20 => Loss 0.038, Train_accy 100.00:  35%|███▌      | 7/20 [00:06<00:12,  1.05it/s]
Task 1, Epoch 8/20 => Loss 0.093, Train_accy 95.71:  35%|███▌      | 7/20 [00:07<00:12,  1.05it/s] 
Task 1, Epoch 8/20 => Loss 0.093, Train_accy 95.71:  40%|████      | 8/20 [00:07<00:11,  1.05it/s]
Task 1, Epoch 9/20 => Loss 0.069, Train_accy 97.14:  40%|████      | 8/20 [00:08<00:11,  1.05it/s]
Task 1, Epoch 9/20 => Loss 0.069, Train_accy 97.14:  45%|████▌     | 9/20 [00:08<00:10,  1.04it/s]
Task 1, Epoch 10/20 => Loss 0.026, Train_accy 100.00:  45%|████▌     | 9/20 [00:09<00:10,  1.04it/s]
Task 1, Epoch 10/20 => Loss 0.026, Train_accy 100.00:  50%|█████     | 10/20 [00:09<00:09,  1.04it/s]
Task 1, Epoch 11/20 => Loss 0.057, Train_accy 97.14:  50%|█████     | 10/20 [00:10<00:09,  1.04it/s] 
Task 1, Epoch 11/20 => Loss 0.057, Train_accy 97.14:  55%|█████▌    | 11/20 [00:10<00:08,  1.03it/s]
Task 1, Epoch 12/20 => Loss 0.010, Train_accy 100.00:  55%|█████▌    | 11/20 [00:11<00:08,  1.03it/s]
Task 1, Epoch 12/20 => Loss 0.010, Train_accy 100.00:  60%|██████    | 12/20 [00:11<00:07,  1.03it/s]
Task 1, Epoch 13/20 => Loss 0.017, Train_accy 100.00:  60%|██████    | 12/20 [00:12<00:07,  1.03it/s]
Task 1, Epoch 13/20 => Loss 0.017, Train_accy 100.00:  65%|██████▌   | 13/20 [00:12<00:06,  1.04it/s]
Task 1, Epoch 14/20 => Loss 0.014, Train_accy 100.00:  65%|██████▌   | 13/20 [00:13<00:06,  1.04it/s]
Task 1, Epoch 14/20 => Loss 0.014, Train_accy 100.00:  70%|███████   | 14/20 [00:13<00:05,  1.04it/s]
Task 1, Epoch 15/20 => Loss 0.029, Train_accy 98.57:  70%|███████   | 14/20 [00:14<00:05,  1.04it/s] 
Task 1, Epoch 15/20 => Loss 0.029, Train_accy 98.57:  75%|███████▌  | 15/20 [00:14<00:04,  1.04it/s]
Task 1, Epoch 16/20 => Loss 0.016, Train_accy 100.00:  75%|███████▌  | 15/20 [00:15<00:04,  1.04it/s]
Task 1, Epoch 16/20 => Loss 0.016, Train_accy 100.00:  80%|████████  | 16/20 [00:15<00:03,  1.04it/s]
Task 1, Epoch 17/20 => Loss 0.008, Train_accy 100.00:  80%|████████  | 16/20 [00:16<00:03,  1.04it/s]
Task 1, Epoch 17/20 => Loss 0.008, Train_accy 100.00:  85%|████████▌ | 17/20 [00:16<00:02,  1.03it/s]
Task 1, Epoch 18/20 => Loss 0.035, Train_accy 98.57:  85%|████████▌ | 17/20 [00:17<00:02,  1.03it/s] 
Task 1, Epoch 18/20 => Loss 0.035, Train_accy 98.57:  90%|█████████ | 18/20 [00:17<00:01,  1.03it/s]
Task 1, Epoch 19/20 => Loss 0.009, Train_accy 100.00:  90%|█████████ | 18/20 [00:18<00:01,  1.03it/s]
Task 1, Epoch 19/20 => Loss 0.009, Train_accy 100.00:  95%|█████████▌| 19/20 [00:18<00:00,  1.04it/s]
Task 1, Epoch 20/20 => Loss 0.024, Train_accy 100.00:  95%|█████████▌| 19/20 [00:19<00:00,  1.04it/s]
Task 1, Epoch 20/20 => Loss 0.024, Train_accy 100.00: 100%|██████████| 20/20 [00:19<00:00,  1.04it/s]
Task 1, Epoch 20/20 => Loss 0.024, Train_accy 100.00: 100%|██████████| 20/20 [00:19<00:00,  1.04it/s]
2024-08-12 02:44:06,660 [inflora.py] => Task 1, Epoch 20/20 => Loss 0.024, Train_accy 100.00
Threshold:  0.955
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 12/768 type remove
Layer 3 : 17/768 type remove
Layer 4 : 18/768 type remove
Layer 5 : 29/768 type remove
Layer 6 : 29/768 type remove
Layer 7 : 32/768 type remove
Layer 8 : 46/768 type remove
Layer 9 : 67/768 type remove
Layer 10 : 52/768 type remove
Layer 11 : 28/768 type remove
Layer 12 : 14/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:44:13,805 [trainer.py] => Time:30.754990100860596
826 826
826 826
2024-08-12 02:44:16,106 [trainer.py] => Time:2.301323652267456
2024-08-12 02:44:16,106 [inflora.py] => Exemplar size: 0
2024-08-12 02:44:16,106 [trainer.py] => CNN: {'total': 95.76, '00-04': 96.5, '05-09': 95.37, 'old': 96.5, 'new': 95.37}
2024-08-12 02:44:16,106 [trainer.py] => CNN top1 curve: [97.9, 95.76]
2024-08-12 02:44:16,106 [trainer.py] => CNN top1 with task curve: [97.9, 99.15]
2024-08-12 02:44:16,107 [trainer.py] => CNN top1 task curve: [1.0, 0.9661016949152542]
2024-08-12 02:44:16,684 [trainer.py] => All params: 108905911
2024-08-12 02:44:16,686 [trainer.py] => Trainable params: 77573
2024-08-12 02:44:16,686 [inflora.py] => Learning on 10-15
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_v.2.weight', 'image_encoder.blocks.5.attn.lora_B_v.2.weight', 'classifier_pool.2.weight', 'image_encoder.blocks.10.attn.lora_B_v.2.weight', 'image_encoder.blocks.3.attn.lora_B_v.2.weight', 'image_encoder.blocks.9.attn.lora_B_v.2.weight', 'image_encoder.blocks.0.attn.lora_B_k.2.weight', 'image_encoder.blocks.4.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_k.2.weight', 'image_encoder.blocks.5.attn.lora_B_k.2.weight', 'classifier_pool.2.bias', 'image_encoder.blocks.1.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_k.2.weight', 'image_encoder.blocks.3.attn.lora_B_k.2.weight', 'image_encoder.blocks.9.attn.lora_B_k.2.weight', 'image_encoder.blocks.10.attn.lora_B_k.2.weight', 'image_encoder.blocks.0.attn.lora_B_v.2.weight', 'image_encoder.blocks.11.attn.lora_B_v.2.weight', 'image_encoder.blocks.2.attn.lora_B_v.2.weight', 'image_encoder.blocks.7.attn.lora_B_v.2.weight', 'image_encoder.blocks.1.attn.lora_B_v.2.weight', 'image_encoder.blocks.4.attn.lora_B_k.2.weight', 'image_encoder.blocks.6.attn.lora_B_k.2.weight', 'image_encoder.blocks.8.attn.lora_B_v.2.weight', 'image_encoder.blocks.11.attn.lora_B_k.2.weight', 'image_encoder.blocks.7.attn.lora_B_k.2.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 2, Epoch 1/20 => Loss 1.411, Train_accy 44.93:   0%|          | 0/20 [00:01<?, ?it/s]
Task 2, Epoch 1/20 => Loss 1.411, Train_accy 44.93:   5%|▌         | 1/20 [00:01<00:22,  1.19s/it]
Task 2, Epoch 2/20 => Loss 0.577, Train_accy 82.61:   5%|▌         | 1/20 [00:02<00:22,  1.19s/it]
Task 2, Epoch 2/20 => Loss 0.577, Train_accy 82.61:  10%|█         | 2/20 [00:02<00:21,  1.18s/it]
Task 2, Epoch 3/20 => Loss 0.283, Train_accy 92.03:  10%|█         | 2/20 [00:03<00:21,  1.18s/it]
Task 2, Epoch 3/20 => Loss 0.283, Train_accy 92.03:  15%|█▌        | 3/20 [00:03<00:20,  1.18s/it]
Task 2, Epoch 4/20 => Loss 0.134, Train_accy 95.65:  15%|█▌        | 3/20 [00:04<00:20,  1.18s/it]
Task 2, Epoch 4/20 => Loss 0.134, Train_accy 95.65:  20%|██        | 4/20 [00:04<00:18,  1.18s/it]
Task 2, Epoch 5/20 => Loss 0.086, Train_accy 96.38:  20%|██        | 4/20 [00:05<00:18,  1.18s/it]
Task 2, Epoch 5/20 => Loss 0.086, Train_accy 96.38:  25%|██▌       | 5/20 [00:05<00:17,  1.19s/it]
Task 2, Epoch 6/20 => Loss 0.074, Train_accy 97.10:  25%|██▌       | 5/20 [00:07<00:17,  1.19s/it]
Task 2, Epoch 6/20 => Loss 0.074, Train_accy 97.10:  30%|███       | 6/20 [00:07<00:16,  1.19s/it]
Task 2, Epoch 7/20 => Loss 0.017, Train_accy 100.00:  30%|███       | 6/20 [00:08<00:16,  1.19s/it]
Task 2, Epoch 7/20 => Loss 0.017, Train_accy 100.00:  35%|███▌      | 7/20 [00:08<00:15,  1.19s/it]
Task 2, Epoch 8/20 => Loss 0.027, Train_accy 99.28:  35%|███▌      | 7/20 [00:09<00:15,  1.19s/it] 
Task 2, Epoch 8/20 => Loss 0.027, Train_accy 99.28:  40%|████      | 8/20 [00:09<00:14,  1.19s/it]
Task 2, Epoch 9/20 => Loss 0.054, Train_accy 98.55:  40%|████      | 8/20 [00:10<00:14,  1.19s/it]
Task 2, Epoch 9/20 => Loss 0.054, Train_accy 98.55:  45%|████▌     | 9/20 [00:10<00:13,  1.19s/it]
Task 2, Epoch 10/20 => Loss 0.030, Train_accy 99.28:  45%|████▌     | 9/20 [00:11<00:13,  1.19s/it]
Task 2, Epoch 10/20 => Loss 0.030, Train_accy 99.28:  50%|█████     | 10/20 [00:11<00:11,  1.18s/it]
Task 2, Epoch 11/20 => Loss 0.005, Train_accy 100.00:  50%|█████     | 10/20 [00:13<00:11,  1.18s/it]
Task 2, Epoch 11/20 => Loss 0.005, Train_accy 100.00:  55%|█████▌    | 11/20 [00:13<00:10,  1.21s/it]
Task 2, Epoch 12/20 => Loss 0.004, Train_accy 100.00:  55%|█████▌    | 11/20 [00:14<00:10,  1.21s/it]
Task 2, Epoch 12/20 => Loss 0.004, Train_accy 100.00:  60%|██████    | 12/20 [00:14<00:09,  1.21s/it]
Task 2, Epoch 13/20 => Loss 0.032, Train_accy 99.28:  60%|██████    | 12/20 [00:15<00:09,  1.21s/it] 
Task 2, Epoch 13/20 => Loss 0.032, Train_accy 99.28:  65%|██████▌   | 13/20 [00:15<00:08,  1.20s/it]
Task 2, Epoch 14/20 => Loss 0.061, Train_accy 98.55:  65%|██████▌   | 13/20 [00:16<00:08,  1.20s/it]
Task 2, Epoch 14/20 => Loss 0.061, Train_accy 98.55:  70%|███████   | 14/20 [00:16<00:07,  1.21s/it]
Task 2, Epoch 15/20 => Loss 0.030, Train_accy 99.28:  70%|███████   | 14/20 [00:17<00:07,  1.21s/it]
Task 2, Epoch 15/20 => Loss 0.030, Train_accy 99.28:  75%|███████▌  | 15/20 [00:17<00:06,  1.21s/it]
Task 2, Epoch 16/20 => Loss 0.013, Train_accy 100.00:  75%|███████▌  | 15/20 [00:19<00:06,  1.21s/it]
Task 2, Epoch 16/20 => Loss 0.013, Train_accy 100.00:  80%|████████  | 16/20 [00:19<00:04,  1.20s/it]
Task 2, Epoch 17/20 => Loss 0.014, Train_accy 100.00:  80%|████████  | 16/20 [00:20<00:04,  1.20s/it]
Task 2, Epoch 17/20 => Loss 0.014, Train_accy 100.00:  85%|████████▌ | 17/20 [00:20<00:03,  1.20s/it]
Task 2, Epoch 18/20 => Loss 0.022, Train_accy 99.28:  85%|████████▌ | 17/20 [00:21<00:03,  1.20s/it] 
Task 2, Epoch 18/20 => Loss 0.022, Train_accy 99.28:  90%|█████████ | 18/20 [00:21<00:02,  1.19s/it]
Task 2, Epoch 19/20 => Loss 0.010, Train_accy 99.28:  90%|█████████ | 18/20 [00:22<00:02,  1.19s/it]
Task 2, Epoch 19/20 => Loss 0.010, Train_accy 99.28:  95%|█████████▌| 19/20 [00:22<00:01,  1.20s/it]
Task 2, Epoch 20/20 => Loss 0.006, Train_accy 100.00:  95%|█████████▌| 19/20 [00:23<00:01,  1.20s/it]
Task 2, Epoch 20/20 => Loss 0.006, Train_accy 100.00: 100%|██████████| 20/20 [00:23<00:00,  1.21s/it]
Task 2, Epoch 20/20 => Loss 0.006, Train_accy 100.00: 100%|██████████| 20/20 [00:23<00:00,  1.20s/it]
2024-08-12 02:44:44,231 [inflora.py] => Task 2, Epoch 20/20 => Loss 0.006, Train_accy 100.00
Threshold:  0.96
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 13/768 type remove
Layer 3 : 18/768 type remove
Layer 4 : 22/768 type remove
Layer 5 : 33/768 type remove
Layer 6 : 34/768 type remove
Layer 7 : 41/768 type remove
Layer 8 : 57/768 type remove
Layer 9 : 77/768 type remove
Layer 10 : 59/768 type remove
Layer 11 : 33/768 type remove
Layer 12 : 30/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:44:51,725 [trainer.py] => Time:35.03906297683716
1438 1438
1438 1438
2024-08-12 02:44:55,032 [trainer.py] => Time:3.306628465652466
2024-08-12 02:44:55,032 [inflora.py] => Exemplar size: 0
2024-08-12 02:44:55,032 [trainer.py] => CNN: {'total': 95.48, '00-04': 93.71, '05-09': 93.15, '10-14': 98.37, 'old': 93.34, 'new': 98.37}
2024-08-12 02:44:55,032 [trainer.py] => CNN top1 curve: [97.9, 95.76, 95.48]
2024-08-12 02:44:55,032 [trainer.py] => CNN top1 with task curve: [97.9, 99.15, 99.37]
2024-08-12 02:44:55,032 [trainer.py] => CNN top1 task curve: [1.0, 0.9661016949152542, 0.9582753824756607]
2024-08-12 02:44:55,501 [trainer.py] => All params: 108905911
2024-08-12 02:44:55,503 [trainer.py] => Trainable params: 77573
2024-08-12 02:44:55,503 [inflora.py] => Learning on 15-20
Parameters to be updated: {'image_encoder.blocks.6.attn.lora_B_k.3.weight', 'classifier_pool.3.bias', 'image_encoder.blocks.4.attn.lora_B_v.3.weight', 'image_encoder.blocks.6.attn.lora_B_v.3.weight', 'image_encoder.blocks.0.attn.lora_B_v.3.weight', 'image_encoder.blocks.3.attn.lora_B_k.3.weight', 'image_encoder.blocks.5.attn.lora_B_k.3.weight', 'image_encoder.blocks.5.attn.lora_B_v.3.weight', 'image_encoder.blocks.8.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_v.3.weight', 'image_encoder.blocks.11.attn.lora_B_k.3.weight', 'image_encoder.blocks.0.attn.lora_B_k.3.weight', 'classifier_pool.3.weight', 'image_encoder.blocks.3.attn.lora_B_v.3.weight', 'image_encoder.blocks.11.attn.lora_B_v.3.weight', 'image_encoder.blocks.10.attn.lora_B_k.3.weight', 'image_encoder.blocks.1.attn.lora_B_k.3.weight', 'image_encoder.blocks.2.attn.lora_B_k.3.weight', 'image_encoder.blocks.7.attn.lora_B_k.3.weight', 'image_encoder.blocks.8.attn.lora_B_k.3.weight', 'image_encoder.blocks.1.attn.lora_B_v.3.weight', 'image_encoder.blocks.2.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_v.3.weight', 'image_encoder.blocks.7.attn.lora_B_v.3.weight', 'image_encoder.blocks.9.attn.lora_B_k.3.weight', 'image_encoder.blocks.4.attn.lora_B_k.3.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 3, Epoch 1/20 => Loss 1.688, Train_accy 39.49:   0%|          | 0/20 [00:01<?, ?it/s]
Task 3, Epoch 1/20 => Loss 1.688, Train_accy 39.49:   5%|▌         | 1/20 [00:01<00:31,  1.64s/it]
Task 3, Epoch 2/20 => Loss 0.560, Train_accy 81.88:   5%|▌         | 1/20 [00:03<00:31,  1.64s/it]
Task 3, Epoch 2/20 => Loss 0.560, Train_accy 81.88:  10%|█         | 2/20 [00:03<00:29,  1.63s/it]
Task 3, Epoch 3/20 => Loss 0.336, Train_accy 87.32:  10%|█         | 2/20 [00:04<00:29,  1.63s/it]
Task 3, Epoch 3/20 => Loss 0.336, Train_accy 87.32:  15%|█▌        | 3/20 [00:04<00:27,  1.63s/it]
Task 3, Epoch 4/20 => Loss 0.223, Train_accy 92.75:  15%|█▌        | 3/20 [00:06<00:27,  1.63s/it]
Task 3, Epoch 4/20 => Loss 0.223, Train_accy 92.75:  20%|██        | 4/20 [00:06<00:25,  1.62s/it]
Task 3, Epoch 5/20 => Loss 0.194, Train_accy 94.20:  20%|██        | 4/20 [00:08<00:25,  1.62s/it]
Task 3, Epoch 5/20 => Loss 0.194, Train_accy 94.20:  25%|██▌       | 5/20 [00:08<00:24,  1.62s/it]
Task 3, Epoch 6/20 => Loss 0.171, Train_accy 94.20:  25%|██▌       | 5/20 [00:09<00:24,  1.62s/it]
Task 3, Epoch 6/20 => Loss 0.171, Train_accy 94.20:  30%|███       | 6/20 [00:09<00:22,  1.62s/it]
Task 3, Epoch 7/20 => Loss 0.129, Train_accy 94.57:  30%|███       | 6/20 [00:11<00:22,  1.62s/it]
Task 3, Epoch 7/20 => Loss 0.129, Train_accy 94.57:  35%|███▌      | 7/20 [00:11<00:21,  1.63s/it]
Task 3, Epoch 8/20 => Loss 0.114, Train_accy 96.74:  35%|███▌      | 7/20 [00:13<00:21,  1.63s/it]
Task 3, Epoch 8/20 => Loss 0.114, Train_accy 96.74:  40%|████      | 8/20 [00:13<00:19,  1.64s/it]
Task 3, Epoch 9/20 => Loss 0.121, Train_accy 96.38:  40%|████      | 8/20 [00:14<00:19,  1.64s/it]
Task 3, Epoch 9/20 => Loss 0.121, Train_accy 96.38:  45%|████▌     | 9/20 [00:14<00:18,  1.65s/it]
Task 3, Epoch 10/20 => Loss 0.121, Train_accy 96.01:  45%|████▌     | 9/20 [00:16<00:18,  1.65s/it]
Task 3, Epoch 10/20 => Loss 0.121, Train_accy 96.01:  50%|█████     | 10/20 [00:16<00:16,  1.66s/it]
Task 3, Epoch 11/20 => Loss 0.110, Train_accy 96.01:  50%|█████     | 10/20 [00:18<00:16,  1.66s/it]
Task 3, Epoch 11/20 => Loss 0.110, Train_accy 96.01:  55%|█████▌    | 11/20 [00:18<00:14,  1.65s/it]
Task 3, Epoch 12/20 => Loss 0.071, Train_accy 98.55:  55%|█████▌    | 11/20 [00:19<00:14,  1.65s/it]
Task 3, Epoch 12/20 => Loss 0.071, Train_accy 98.55:  60%|██████    | 12/20 [00:19<00:13,  1.66s/it]
Task 3, Epoch 13/20 => Loss 0.056, Train_accy 98.19:  60%|██████    | 12/20 [00:21<00:13,  1.66s/it]
Task 3, Epoch 13/20 => Loss 0.056, Train_accy 98.19:  65%|██████▌   | 13/20 [00:21<00:11,  1.65s/it]
Task 3, Epoch 14/20 => Loss 0.093, Train_accy 97.10:  65%|██████▌   | 13/20 [00:22<00:11,  1.65s/it]
Task 3, Epoch 14/20 => Loss 0.093, Train_accy 97.10:  70%|███████   | 14/20 [00:22<00:09,  1.64s/it]
Task 3, Epoch 15/20 => Loss 0.070, Train_accy 98.91:  70%|███████   | 14/20 [00:24<00:09,  1.64s/it]
Task 3, Epoch 15/20 => Loss 0.070, Train_accy 98.91:  75%|███████▌  | 15/20 [00:24<00:08,  1.64s/it]
Task 3, Epoch 16/20 => Loss 0.066, Train_accy 98.55:  75%|███████▌  | 15/20 [00:26<00:08,  1.64s/it]
Task 3, Epoch 16/20 => Loss 0.066, Train_accy 98.55:  80%|████████  | 16/20 [00:26<00:06,  1.64s/it]
Task 3, Epoch 17/20 => Loss 0.084, Train_accy 97.46:  80%|████████  | 16/20 [00:27<00:06,  1.64s/it]
Task 3, Epoch 17/20 => Loss 0.084, Train_accy 97.46:  85%|████████▌ | 17/20 [00:27<00:04,  1.64s/it]
Task 3, Epoch 18/20 => Loss 0.078, Train_accy 97.83:  85%|████████▌ | 17/20 [00:29<00:04,  1.64s/it]
Task 3, Epoch 18/20 => Loss 0.078, Train_accy 97.83:  90%|█████████ | 18/20 [00:29<00:03,  1.64s/it]
Task 3, Epoch 19/20 => Loss 0.074, Train_accy 97.10:  90%|█████████ | 18/20 [00:31<00:03,  1.64s/it]
Task 3, Epoch 19/20 => Loss 0.074, Train_accy 97.10:  95%|█████████▌| 19/20 [00:31<00:01,  1.64s/it]
Task 3, Epoch 20/20 => Loss 0.104, Train_accy 97.10:  95%|█████████▌| 19/20 [00:32<00:01,  1.64s/it]
Task 3, Epoch 20/20 => Loss 0.104, Train_accy 97.10: 100%|██████████| 20/20 [00:32<00:00,  1.64s/it]
Task 3, Epoch 20/20 => Loss 0.104, Train_accy 97.10: 100%|██████████| 20/20 [00:32<00:00,  1.64s/it]
2024-08-12 02:45:32,159 [inflora.py] => Task 3, Epoch 20/20 => Loss 0.104, Train_accy 97.10
Threshold:  0.965
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 20/768 type remove
Layer 4 : 27/768 type remove
Layer 5 : 38/768 type remove
Layer 6 : 43/768 type remove
Layer 7 : 49/768 type remove
Layer 8 : 74/768 type remove
Layer 9 : 86/768 type remove
Layer 10 : 65/768 type remove
Layer 11 : 37/768 type remove
Layer 12 : 46/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:45:39,882 [trainer.py] => Time:44.37933111190796
2847 2847
2847 2847
2024-08-12 02:45:45,611 [trainer.py] => Time:5.7285072803497314
2024-08-12 02:45:45,611 [inflora.py] => Exemplar size: 0
2024-08-12 02:45:45,612 [trainer.py] => CNN: {'total': 88.58, '00-04': 93.01, '05-09': 92.96, '10-14': 98.04, '15-19': 81.9, 'old': 95.13, 'new': 81.9}
2024-08-12 02:45:45,612 [trainer.py] => CNN top1 curve: [97.9, 95.76, 95.48, 88.58]
2024-08-12 02:45:45,612 [trainer.py] => CNN top1 with task curve: [97.9, 99.15, 99.37, 98.24]
2024-08-12 02:45:45,612 [trainer.py] => CNN top1 task curve: [1.0, 0.9661016949152542, 0.9582753824756607, 0.8935721812434141]
2024-08-12 02:45:46,058 [trainer.py] => All params: 108905911
2024-08-12 02:45:46,060 [trainer.py] => Trainable params: 77573
2024-08-12 02:45:46,060 [inflora.py] => Learning on 20-25
Parameters to be updated: {'image_encoder.blocks.0.attn.lora_B_k.4.weight', 'image_encoder.blocks.2.attn.lora_B_k.4.weight', 'image_encoder.blocks.11.attn.lora_B_v.4.weight', 'image_encoder.blocks.6.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_v.4.weight', 'image_encoder.blocks.3.attn.lora_B_k.4.weight', 'image_encoder.blocks.6.attn.lora_B_k.4.weight', 'image_encoder.blocks.9.attn.lora_B_v.4.weight', 'classifier_pool.4.bias', 'image_encoder.blocks.2.attn.lora_B_v.4.weight', 'image_encoder.blocks.4.attn.lora_B_k.4.weight', 'image_encoder.blocks.10.attn.lora_B_v.4.weight', 'image_encoder.blocks.7.attn.lora_B_k.4.weight', 'image_encoder.blocks.5.attn.lora_B_v.4.weight', 'image_encoder.blocks.8.attn.lora_B_v.4.weight', 'classifier_pool.4.weight', 'image_encoder.blocks.9.attn.lora_B_k.4.weight', 'image_encoder.blocks.5.attn.lora_B_k.4.weight', 'image_encoder.blocks.4.attn.lora_B_v.4.weight', 'image_encoder.blocks.1.attn.lora_B_k.4.weight', 'image_encoder.blocks.1.attn.lora_B_v.4.weight', 'image_encoder.blocks.0.attn.lora_B_v.4.weight', 'image_encoder.blocks.10.attn.lora_B_k.4.weight', 'image_encoder.blocks.11.attn.lora_B_k.4.weight', 'image_encoder.blocks.7.attn.lora_B_v.4.weight', 'image_encoder.blocks.8.attn.lora_B_k.4.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 4, Epoch 1/20 => Loss 1.469, Train_accy 42.00:   0%|          | 0/20 [00:01<?, ?it/s]
Task 4, Epoch 1/20 => Loss 1.469, Train_accy 42.00:   5%|▌         | 1/20 [00:01<00:30,  1.60s/it]
Task 4, Epoch 2/20 => Loss 0.522, Train_accy 78.80:   5%|▌         | 1/20 [00:03<00:30,  1.60s/it]
Task 4, Epoch 2/20 => Loss 0.522, Train_accy 78.80:  10%|█         | 2/20 [00:03<00:28,  1.58s/it]
Task 4, Epoch 3/20 => Loss 0.266, Train_accy 89.60:  10%|█         | 2/20 [00:04<00:28,  1.58s/it]
Task 4, Epoch 3/20 => Loss 0.266, Train_accy 89.60:  15%|█▌        | 3/20 [00:04<00:27,  1.59s/it]
Task 4, Epoch 4/20 => Loss 0.161, Train_accy 94.00:  15%|█▌        | 3/20 [00:06<00:27,  1.59s/it]
Task 4, Epoch 4/20 => Loss 0.161, Train_accy 94.00:  20%|██        | 4/20 [00:06<00:25,  1.61s/it]
Task 4, Epoch 5/20 => Loss 0.099, Train_accy 97.20:  20%|██        | 4/20 [00:07<00:25,  1.61s/it]
Task 4, Epoch 5/20 => Loss 0.099, Train_accy 97.20:  25%|██▌       | 5/20 [00:07<00:24,  1.60s/it]
Task 4, Epoch 6/20 => Loss 0.155, Train_accy 97.20:  25%|██▌       | 5/20 [00:09<00:24,  1.60s/it]
Task 4, Epoch 6/20 => Loss 0.155, Train_accy 97.20:  30%|███       | 6/20 [00:09<00:22,  1.61s/it]
Task 4, Epoch 7/20 => Loss 0.089, Train_accy 97.60:  30%|███       | 6/20 [00:11<00:22,  1.61s/it]
Task 4, Epoch 7/20 => Loss 0.089, Train_accy 97.60:  35%|███▌      | 7/20 [00:11<00:21,  1.62s/it]
Task 4, Epoch 8/20 => Loss 0.058, Train_accy 97.60:  35%|███▌      | 7/20 [00:12<00:21,  1.62s/it]
Task 4, Epoch 8/20 => Loss 0.058, Train_accy 97.60:  40%|████      | 8/20 [00:12<00:19,  1.63s/it]
Task 4, Epoch 9/20 => Loss 0.118, Train_accy 95.20:  40%|████      | 8/20 [00:14<00:19,  1.63s/it]
Task 4, Epoch 9/20 => Loss 0.118, Train_accy 95.20:  45%|████▌     | 9/20 [00:14<00:17,  1.63s/it]
Task 4, Epoch 10/20 => Loss 0.088, Train_accy 97.20:  45%|████▌     | 9/20 [00:16<00:17,  1.63s/it]
Task 4, Epoch 10/20 => Loss 0.088, Train_accy 97.20:  50%|█████     | 10/20 [00:16<00:16,  1.62s/it]
Task 4, Epoch 11/20 => Loss 0.047, Train_accy 98.00:  50%|█████     | 10/20 [00:17<00:16,  1.62s/it]
Task 4, Epoch 11/20 => Loss 0.047, Train_accy 98.00:  55%|█████▌    | 11/20 [00:17<00:14,  1.62s/it]
Task 4, Epoch 12/20 => Loss 0.082, Train_accy 96.80:  55%|█████▌    | 11/20 [00:19<00:14,  1.62s/it]
Task 4, Epoch 12/20 => Loss 0.082, Train_accy 96.80:  60%|██████    | 12/20 [00:19<00:13,  1.63s/it]
Task 4, Epoch 13/20 => Loss 0.065, Train_accy 97.60:  60%|██████    | 12/20 [00:21<00:13,  1.63s/it]
Task 4, Epoch 13/20 => Loss 0.065, Train_accy 97.60:  65%|██████▌   | 13/20 [00:21<00:11,  1.63s/it]
Task 4, Epoch 14/20 => Loss 0.028, Train_accy 99.20:  65%|██████▌   | 13/20 [00:22<00:11,  1.63s/it]
Task 4, Epoch 14/20 => Loss 0.028, Train_accy 99.20:  70%|███████   | 14/20 [00:22<00:09,  1.62s/it]
Task 4, Epoch 15/20 => Loss 0.029, Train_accy 98.80:  70%|███████   | 14/20 [00:24<00:09,  1.62s/it]
Task 4, Epoch 15/20 => Loss 0.029, Train_accy 98.80:  75%|███████▌  | 15/20 [00:24<00:08,  1.62s/it]
Task 4, Epoch 16/20 => Loss 0.042, Train_accy 99.20:  75%|███████▌  | 15/20 [00:25<00:08,  1.62s/it]
Task 4, Epoch 16/20 => Loss 0.042, Train_accy 99.20:  80%|████████  | 16/20 [00:25<00:06,  1.61s/it]
Task 4, Epoch 17/20 => Loss 0.071, Train_accy 97.20:  80%|████████  | 16/20 [00:27<00:06,  1.61s/it]
Task 4, Epoch 17/20 => Loss 0.071, Train_accy 97.20:  85%|████████▌ | 17/20 [00:27<00:04,  1.64s/it]
Task 4, Epoch 18/20 => Loss 0.065, Train_accy 97.60:  85%|████████▌ | 17/20 [00:29<00:04,  1.64s/it]
Task 4, Epoch 18/20 => Loss 0.065, Train_accy 97.60:  90%|█████████ | 18/20 [00:29<00:03,  1.64s/it]
Task 4, Epoch 19/20 => Loss 0.041, Train_accy 98.40:  90%|█████████ | 18/20 [00:30<00:03,  1.64s/it]
Task 4, Epoch 19/20 => Loss 0.041, Train_accy 98.40:  95%|█████████▌| 19/20 [00:30<00:01,  1.64s/it]
Task 4, Epoch 20/20 => Loss 0.050, Train_accy 97.60:  95%|█████████▌| 19/20 [00:32<00:01,  1.64s/it]
Task 4, Epoch 20/20 => Loss 0.050, Train_accy 97.60: 100%|██████████| 20/20 [00:32<00:00,  1.64s/it]
Task 4, Epoch 20/20 => Loss 0.050, Train_accy 97.60: 100%|██████████| 20/20 [00:32<00:00,  1.62s/it]
2024-08-12 02:46:22,439 [inflora.py] => Task 4, Epoch 20/20 => Loss 0.050, Train_accy 97.60
Threshold:  0.97
Skip Updating DualGPM for layer: 1
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 14/768 type remove
Layer 3 : 22/768 type remove
Layer 4 : 30/768 type remove
Layer 5 : 41/768 type remove
Layer 6 : 48/768 type remove
Layer 7 : 60/768 type remove
Layer 8 : 90/768 type remove
Layer 9 : 101/768 type remove
Layer 10 : 75/768 type remove
Layer 11 : 42/768 type remove
Layer 12 : 83/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:46:30,194 [trainer.py] => Time:44.13408303260803
4102 4102
4102 4102
2024-08-12 02:46:38,046 [trainer.py] => Time:7.851304531097412
2024-08-12 02:46:38,047 [inflora.py] => Exemplar size: 0
2024-08-12 02:46:38,047 [trainer.py] => CNN: {'total': 85.06, '00-04': 93.36, '05-09': 88.52, '10-14': 94.61, '15-19': 80.2, '20-24': 82.47, 'old': 86.2, 'new': 82.47}
2024-08-12 02:46:38,047 [trainer.py] => CNN top1 curve: [97.9, 95.76, 95.48, 88.58, 85.06]
2024-08-12 02:46:38,047 [trainer.py] => CNN top1 with task curve: [97.9, 99.15, 99.37, 98.24, 98.24]
2024-08-12 02:46:38,047 [trainer.py] => CNN top1 task curve: [1.0, 0.9661016949152542, 0.9582753824756607, 0.8935721812434141, 0.8537298878595807]
2024-08-12 02:46:38,520 [trainer.py] => All params: 108905911
2024-08-12 02:46:38,521 [trainer.py] => Trainable params: 77573
2024-08-12 02:46:38,521 [inflora.py] => Learning on 25-30
Parameters to be updated: {'image_encoder.blocks.8.attn.lora_B_v.5.weight', 'image_encoder.blocks.2.attn.lora_B_k.5.weight', 'image_encoder.blocks.8.attn.lora_B_k.5.weight', 'image_encoder.blocks.9.attn.lora_B_v.5.weight', 'classifier_pool.5.weight', 'image_encoder.blocks.6.attn.lora_B_k.5.weight', 'image_encoder.blocks.3.attn.lora_B_k.5.weight', 'image_encoder.blocks.5.attn.lora_B_k.5.weight', 'image_encoder.blocks.9.attn.lora_B_k.5.weight', 'image_encoder.blocks.3.attn.lora_B_v.5.weight', 'image_encoder.blocks.10.attn.lora_B_v.5.weight', 'image_encoder.blocks.11.attn.lora_B_v.5.weight', 'image_encoder.blocks.7.attn.lora_B_v.5.weight', 'image_encoder.blocks.7.attn.lora_B_k.5.weight', 'image_encoder.blocks.6.attn.lora_B_v.5.weight', 'image_encoder.blocks.0.attn.lora_B_v.5.weight', 'image_encoder.blocks.11.attn.lora_B_k.5.weight', 'image_encoder.blocks.2.attn.lora_B_v.5.weight', 'image_encoder.blocks.4.attn.lora_B_v.5.weight', 'image_encoder.blocks.1.attn.lora_B_k.5.weight', 'image_encoder.blocks.4.attn.lora_B_k.5.weight', 'image_encoder.blocks.1.attn.lora_B_v.5.weight', 'image_encoder.blocks.10.attn.lora_B_k.5.weight', 'classifier_pool.5.bias', 'image_encoder.blocks.5.attn.lora_B_v.5.weight', 'image_encoder.blocks.0.attn.lora_B_k.5.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 5, Epoch 1/20 => Loss 1.406, Train_accy 42.61:   0%|          | 0/20 [00:01<?, ?it/s]
Task 5, Epoch 1/20 => Loss 1.406, Train_accy 42.61:   5%|▌         | 1/20 [00:01<00:23,  1.23s/it]
Task 5, Epoch 2/20 => Loss 0.646, Train_accy 71.30:   5%|▌         | 1/20 [00:02<00:23,  1.23s/it]
Task 5, Epoch 2/20 => Loss 0.646, Train_accy 71.30:  10%|█         | 2/20 [00:02<00:21,  1.22s/it]
Task 5, Epoch 3/20 => Loss 0.314, Train_accy 91.30:  10%|█         | 2/20 [00:03<00:21,  1.22s/it]
Task 5, Epoch 3/20 => Loss 0.314, Train_accy 91.30:  15%|█▌        | 3/20 [00:03<00:20,  1.22s/it]
Task 5, Epoch 4/20 => Loss 0.126, Train_accy 99.13:  15%|█▌        | 3/20 [00:04<00:20,  1.22s/it]
Task 5, Epoch 4/20 => Loss 0.126, Train_accy 99.13:  20%|██        | 4/20 [00:04<00:19,  1.23s/it]
Task 5, Epoch 5/20 => Loss 0.088, Train_accy 99.13:  20%|██        | 4/20 [00:06<00:19,  1.23s/it]
Task 5, Epoch 5/20 => Loss 0.088, Train_accy 99.13:  25%|██▌       | 5/20 [00:06<00:18,  1.25s/it]
Task 5, Epoch 6/20 => Loss 0.065, Train_accy 99.13:  25%|██▌       | 5/20 [00:07<00:18,  1.25s/it]
Task 5, Epoch 6/20 => Loss 0.065, Train_accy 99.13:  30%|███       | 6/20 [00:07<00:17,  1.26s/it]
Task 5, Epoch 7/20 => Loss 0.103, Train_accy 99.13:  30%|███       | 6/20 [00:08<00:17,  1.26s/it]
Task 5, Epoch 7/20 => Loss 0.103, Train_accy 99.13:  35%|███▌      | 7/20 [00:08<00:16,  1.27s/it]
Task 5, Epoch 8/20 => Loss 0.045, Train_accy 98.26:  35%|███▌      | 7/20 [00:10<00:16,  1.27s/it]
Task 5, Epoch 8/20 => Loss 0.045, Train_accy 98.26:  40%|████      | 8/20 [00:10<00:15,  1.26s/it]
Task 5, Epoch 9/20 => Loss 0.080, Train_accy 98.26:  40%|████      | 8/20 [00:11<00:15,  1.26s/it]
Task 5, Epoch 9/20 => Loss 0.080, Train_accy 98.26:  45%|████▌     | 9/20 [00:11<00:13,  1.26s/it]
Task 5, Epoch 10/20 => Loss 0.020, Train_accy 99.13:  45%|████▌     | 9/20 [00:12<00:13,  1.26s/it]
Task 5, Epoch 10/20 => Loss 0.020, Train_accy 99.13:  50%|█████     | 10/20 [00:12<00:12,  1.26s/it]
Task 5, Epoch 11/20 => Loss 0.017, Train_accy 100.00:  50%|█████     | 10/20 [00:13<00:12,  1.26s/it]
Task 5, Epoch 11/20 => Loss 0.017, Train_accy 100.00:  55%|█████▌    | 11/20 [00:13<00:11,  1.28s/it]
Task 5, Epoch 12/20 => Loss 0.021, Train_accy 100.00:  55%|█████▌    | 11/20 [00:15<00:11,  1.28s/it]
Task 5, Epoch 12/20 => Loss 0.021, Train_accy 100.00:  60%|██████    | 12/20 [00:15<00:10,  1.28s/it]
Task 5, Epoch 13/20 => Loss 0.019, Train_accy 99.13:  60%|██████    | 12/20 [00:16<00:10,  1.28s/it] 
Task 5, Epoch 13/20 => Loss 0.019, Train_accy 99.13:  65%|██████▌   | 13/20 [00:16<00:08,  1.29s/it]
Task 5, Epoch 14/20 => Loss 0.021, Train_accy 100.00:  65%|██████▌   | 13/20 [00:17<00:08,  1.29s/it]
Task 5, Epoch 14/20 => Loss 0.021, Train_accy 100.00:  70%|███████   | 14/20 [00:17<00:07,  1.28s/it]
Task 5, Epoch 15/20 => Loss 0.033, Train_accy 99.13:  70%|███████   | 14/20 [00:18<00:07,  1.28s/it] 
Task 5, Epoch 15/20 => Loss 0.033, Train_accy 99.13:  75%|███████▌  | 15/20 [00:18<00:06,  1.29s/it]
Task 5, Epoch 16/20 => Loss 0.061, Train_accy 98.26:  75%|███████▌  | 15/20 [00:20<00:06,  1.29s/it]
Task 5, Epoch 16/20 => Loss 0.061, Train_accy 98.26:  80%|████████  | 16/20 [00:20<00:05,  1.28s/it]
Task 5, Epoch 17/20 => Loss 0.036, Train_accy 98.26:  80%|████████  | 16/20 [00:21<00:05,  1.28s/it]
Task 5, Epoch 17/20 => Loss 0.036, Train_accy 98.26:  85%|████████▌ | 17/20 [00:21<00:03,  1.28s/it]
Task 5, Epoch 18/20 => Loss 0.024, Train_accy 100.00:  85%|████████▌ | 17/20 [00:22<00:03,  1.28s/it]
Task 5, Epoch 18/20 => Loss 0.024, Train_accy 100.00:  90%|█████████ | 18/20 [00:22<00:02,  1.29s/it]
Task 5, Epoch 19/20 => Loss 0.021, Train_accy 99.13:  90%|█████████ | 18/20 [00:24<00:02,  1.29s/it] 
Task 5, Epoch 19/20 => Loss 0.021, Train_accy 99.13:  95%|█████████▌| 19/20 [00:24<00:01,  1.32s/it]
Task 5, Epoch 20/20 => Loss 0.034, Train_accy 99.13:  95%|█████████▌| 19/20 [00:25<00:01,  1.32s/it]
Task 5, Epoch 20/20 => Loss 0.034, Train_accy 99.13: 100%|██████████| 20/20 [00:25<00:00,  1.30s/it]
Task 5, Epoch 20/20 => Loss 0.034, Train_accy 99.13: 100%|██████████| 20/20 [00:25<00:00,  1.28s/it]
2024-08-12 02:47:07,748 [inflora.py] => Task 5, Epoch 20/20 => Loss 0.034, Train_accy 99.13
Threshold:  0.975
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 7/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 29/768 type remove
Layer 4 : 38/768 type remove
Layer 5 : 55/768 type remove
Layer 6 : 63/768 type remove
Layer 7 : 75/768 type remove
Layer 8 : 108/768 type remove
Layer 9 : 126/768 type remove
Layer 10 : 102/768 type remove
Layer 11 : 61/768 type remove
Layer 12 : 96/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:47:15,526 [trainer.py] => Time:37.0042200088501
4480 4480
4480 4480
2024-08-12 02:47:23,836 [trainer.py] => Time:8.309513092041016
2024-08-12 02:47:23,836 [inflora.py] => Exemplar size: 0
2024-08-12 02:47:23,836 [trainer.py] => CNN: {'total': 82.19, '00-04': 91.96, '05-09': 87.96, '10-14': 95.1, '15-19': 79.7, '20-24': 81.35, '25-29': 57.67, 'old': 84.45, 'new': 57.67}
2024-08-12 02:47:23,836 [trainer.py] => CNN top1 curve: [97.9, 95.76, 95.48, 88.58, 85.06, 82.19]
2024-08-12 02:47:23,836 [trainer.py] => CNN top1 with task curve: [97.9, 99.15, 99.37, 98.24, 98.24, 98.46]
2024-08-12 02:47:23,836 [trainer.py] => CNN top1 task curve: [1.0, 0.9661016949152542, 0.9582753824756607, 0.8935721812434141, 0.8537298878595807, 0.8247767857142857]
2024-08-12 02:47:24,292 [trainer.py] => All params: 108905911
2024-08-12 02:47:24,294 [trainer.py] => Trainable params: 77573
2024-08-12 02:47:24,294 [inflora.py] => Learning on 30-35
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.6.weight', 'image_encoder.blocks.6.attn.lora_B_k.6.weight', 'image_encoder.blocks.9.attn.lora_B_k.6.weight', 'image_encoder.blocks.5.attn.lora_B_k.6.weight', 'classifier_pool.6.weight', 'image_encoder.blocks.0.attn.lora_B_v.6.weight', 'image_encoder.blocks.10.attn.lora_B_v.6.weight', 'image_encoder.blocks.1.attn.lora_B_k.6.weight', 'image_encoder.blocks.7.attn.lora_B_v.6.weight', 'image_encoder.blocks.0.attn.lora_B_k.6.weight', 'image_encoder.blocks.3.attn.lora_B_v.6.weight', 'image_encoder.blocks.10.attn.lora_B_k.6.weight', 'image_encoder.blocks.11.attn.lora_B_v.6.weight', 'classifier_pool.6.bias', 'image_encoder.blocks.7.attn.lora_B_k.6.weight', 'image_encoder.blocks.9.attn.lora_B_v.6.weight', 'image_encoder.blocks.8.attn.lora_B_v.6.weight', 'image_encoder.blocks.8.attn.lora_B_k.6.weight', 'image_encoder.blocks.4.attn.lora_B_v.6.weight', 'image_encoder.blocks.11.attn.lora_B_k.6.weight', 'image_encoder.blocks.1.attn.lora_B_v.6.weight', 'image_encoder.blocks.2.attn.lora_B_v.6.weight', 'image_encoder.blocks.4.attn.lora_B_k.6.weight', 'image_encoder.blocks.6.attn.lora_B_v.6.weight', 'image_encoder.blocks.2.attn.lora_B_k.6.weight', 'image_encoder.blocks.3.attn.lora_B_k.6.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 6, Epoch 1/20 => Loss 1.133, Train_accy 58.05:   0%|          | 0/20 [00:01<?, ?it/s]
Task 6, Epoch 1/20 => Loss 1.133, Train_accy 58.05:   5%|▌         | 1/20 [00:01<00:27,  1.45s/it]
Task 6, Epoch 2/20 => Loss 0.405, Train_accy 87.93:   5%|▌         | 1/20 [00:02<00:27,  1.45s/it]
Task 6, Epoch 2/20 => Loss 0.405, Train_accy 87.93:  10%|█         | 2/20 [00:02<00:25,  1.43s/it]
Task 6, Epoch 3/20 => Loss 0.144, Train_accy 95.98:  10%|█         | 2/20 [00:04<00:25,  1.43s/it]
Task 6, Epoch 3/20 => Loss 0.144, Train_accy 95.98:  15%|█▌        | 3/20 [00:04<00:24,  1.45s/it]
Task 6, Epoch 4/20 => Loss 0.070, Train_accy 98.85:  15%|█▌        | 3/20 [00:05<00:24,  1.45s/it]
Task 6, Epoch 4/20 => Loss 0.070, Train_accy 98.85:  20%|██        | 4/20 [00:05<00:23,  1.45s/it]
Task 6, Epoch 5/20 => Loss 0.052, Train_accy 98.85:  20%|██        | 4/20 [00:07<00:23,  1.45s/it]
Task 6, Epoch 5/20 => Loss 0.052, Train_accy 98.85:  25%|██▌       | 5/20 [00:07<00:21,  1.47s/it]
Task 6, Epoch 6/20 => Loss 0.021, Train_accy 100.00:  25%|██▌       | 5/20 [00:08<00:21,  1.47s/it]
Task 6, Epoch 6/20 => Loss 0.021, Train_accy 100.00:  30%|███       | 6/20 [00:08<00:20,  1.47s/it]
Task 6, Epoch 7/20 => Loss 0.017, Train_accy 99.43:  30%|███       | 6/20 [00:10<00:20,  1.47s/it] 
Task 6, Epoch 7/20 => Loss 0.017, Train_accy 99.43:  35%|███▌      | 7/20 [00:10<00:18,  1.45s/it]
Task 6, Epoch 8/20 => Loss 0.017, Train_accy 99.43:  35%|███▌      | 7/20 [00:11<00:18,  1.45s/it]
Task 6, Epoch 8/20 => Loss 0.017, Train_accy 99.43:  40%|████      | 8/20 [00:11<00:17,  1.45s/it]
Task 6, Epoch 9/20 => Loss 0.009, Train_accy 100.00:  40%|████      | 8/20 [00:13<00:17,  1.45s/it]
Task 6, Epoch 9/20 => Loss 0.009, Train_accy 100.00:  45%|████▌     | 9/20 [00:13<00:15,  1.45s/it]
Task 6, Epoch 10/20 => Loss 0.022, Train_accy 99.43:  45%|████▌     | 9/20 [00:14<00:15,  1.45s/it]
Task 6, Epoch 10/20 => Loss 0.022, Train_accy 99.43:  50%|█████     | 10/20 [00:14<00:14,  1.46s/it]
Task 6, Epoch 11/20 => Loss 0.007, Train_accy 100.00:  50%|█████     | 10/20 [00:15<00:14,  1.46s/it]
Task 6, Epoch 11/20 => Loss 0.007, Train_accy 100.00:  55%|█████▌    | 11/20 [00:15<00:13,  1.45s/it]
Task 6, Epoch 12/20 => Loss 0.005, Train_accy 100.00:  55%|█████▌    | 11/20 [00:17<00:13,  1.45s/it]
Task 6, Epoch 12/20 => Loss 0.005, Train_accy 100.00:  60%|██████    | 12/20 [00:17<00:11,  1.45s/it]
Task 6, Epoch 13/20 => Loss 0.012, Train_accy 99.43:  60%|██████    | 12/20 [00:18<00:11,  1.45s/it] 
Task 6, Epoch 13/20 => Loss 0.012, Train_accy 99.43:  65%|██████▌   | 13/20 [00:18<00:10,  1.44s/it]
Task 6, Epoch 14/20 => Loss 0.028, Train_accy 99.43:  65%|██████▌   | 13/20 [00:20<00:10,  1.44s/it]
Task 6, Epoch 14/20 => Loss 0.028, Train_accy 99.43:  70%|███████   | 14/20 [00:20<00:08,  1.44s/it]
Task 6, Epoch 15/20 => Loss 0.007, Train_accy 100.00:  70%|███████   | 14/20 [00:21<00:08,  1.44s/it]
Task 6, Epoch 15/20 => Loss 0.007, Train_accy 100.00:  75%|███████▌  | 15/20 [00:21<00:07,  1.45s/it]
Task 6, Epoch 16/20 => Loss 0.028, Train_accy 98.85:  75%|███████▌  | 15/20 [00:23<00:07,  1.45s/it] 
Task 6, Epoch 16/20 => Loss 0.028, Train_accy 98.85:  80%|████████  | 16/20 [00:23<00:05,  1.44s/it]
Task 6, Epoch 17/20 => Loss 0.022, Train_accy 99.43:  80%|████████  | 16/20 [00:24<00:05,  1.44s/it]
Task 6, Epoch 17/20 => Loss 0.022, Train_accy 99.43:  85%|████████▌ | 17/20 [00:24<00:04,  1.49s/it]
Task 6, Epoch 18/20 => Loss 0.005, Train_accy 100.00:  85%|████████▌ | 17/20 [00:26<00:04,  1.49s/it]
Task 6, Epoch 18/20 => Loss 0.005, Train_accy 100.00:  90%|█████████ | 18/20 [00:26<00:02,  1.48s/it]
Task 6, Epoch 19/20 => Loss 0.016, Train_accy 99.43:  90%|█████████ | 18/20 [00:27<00:02,  1.48s/it] 
Task 6, Epoch 19/20 => Loss 0.016, Train_accy 99.43:  95%|█████████▌| 19/20 [00:27<00:01,  1.46s/it]
Task 6, Epoch 20/20 => Loss 0.010, Train_accy 100.00:  95%|█████████▌| 19/20 [00:29<00:01,  1.46s/it]
Task 6, Epoch 20/20 => Loss 0.010, Train_accy 100.00: 100%|██████████| 20/20 [00:29<00:00,  1.46s/it]
Task 6, Epoch 20/20 => Loss 0.010, Train_accy 100.00: 100%|██████████| 20/20 [00:29<00:00,  1.46s/it]
2024-08-12 02:47:57,275 [inflora.py] => Task 6, Epoch 20/20 => Loss 0.010, Train_accy 100.00
Threshold:  0.98
Skip Updating DualGPM for layer: 2
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 16/768 type remove
Layer 3 : 30/768 type remove
Layer 4 : 43/768 type remove
Layer 5 : 64/768 type remove
Layer 6 : 76/768 type remove
Layer 7 : 91/768 type remove
Layer 8 : 136/768 type remove
Layer 9 : 156/768 type remove
Layer 10 : 122/768 type remove
Layer 11 : 75/768 type remove
Layer 12 : 135/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:48:04,855 [trainer.py] => Time:40.56067657470703
5291 5291
5291 5291
2024-08-12 02:48:14,447 [trainer.py] => Time:9.591750383377075
2024-08-12 02:48:14,449 [inflora.py] => Exemplar size: 0
2024-08-12 02:48:14,449 [trainer.py] => CNN: {'total': 80.42, '00-04': 90.56, '05-09': 88.15, '10-14': 91.01, '15-19': 76.3, '20-24': 73.07, '25-29': 54.5, '30-34': 94.33, 'old': 77.9, 'new': 94.33}
2024-08-12 02:48:14,449 [trainer.py] => CNN top1 curve: [97.9, 95.76, 95.48, 88.58, 85.06, 82.19, 80.42]
2024-08-12 02:48:14,449 [trainer.py] => CNN top1 with task curve: [97.9, 99.15, 99.37, 98.24, 98.24, 98.46, 98.47]
2024-08-12 02:48:14,449 [trainer.py] => CNN top1 task curve: [1.0, 0.9661016949152542, 0.9582753824756607, 0.8935721812434141, 0.8537298878595807, 0.8247767857142857, 0.8058968058968059]
2024-08-12 02:48:14,906 [trainer.py] => All params: 108905911
2024-08-12 02:48:14,908 [trainer.py] => Trainable params: 77573
2024-08-12 02:48:14,908 [inflora.py] => Learning on 35-40
Parameters to be updated: {'image_encoder.blocks.5.attn.lora_B_v.7.weight', 'image_encoder.blocks.1.attn.lora_B_k.7.weight', 'image_encoder.blocks.8.attn.lora_B_v.7.weight', 'image_encoder.blocks.3.attn.lora_B_v.7.weight', 'image_encoder.blocks.10.attn.lora_B_k.7.weight', 'classifier_pool.7.bias', 'image_encoder.blocks.7.attn.lora_B_k.7.weight', 'image_encoder.blocks.3.attn.lora_B_k.7.weight', 'image_encoder.blocks.7.attn.lora_B_v.7.weight', 'image_encoder.blocks.9.attn.lora_B_v.7.weight', 'image_encoder.blocks.2.attn.lora_B_k.7.weight', 'image_encoder.blocks.2.attn.lora_B_v.7.weight', 'image_encoder.blocks.1.attn.lora_B_v.7.weight', 'image_encoder.blocks.5.attn.lora_B_k.7.weight', 'image_encoder.blocks.10.attn.lora_B_v.7.weight', 'image_encoder.blocks.6.attn.lora_B_k.7.weight', 'image_encoder.blocks.8.attn.lora_B_k.7.weight', 'image_encoder.blocks.0.attn.lora_B_v.7.weight', 'image_encoder.blocks.4.attn.lora_B_v.7.weight', 'image_encoder.blocks.11.attn.lora_B_v.7.weight', 'image_encoder.blocks.11.attn.lora_B_k.7.weight', 'image_encoder.blocks.6.attn.lora_B_v.7.weight', 'classifier_pool.7.weight', 'image_encoder.blocks.0.attn.lora_B_k.7.weight', 'image_encoder.blocks.4.attn.lora_B_k.7.weight', 'image_encoder.blocks.9.attn.lora_B_k.7.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 7, Epoch 1/20 => Loss 1.639, Train_accy 33.33:   0%|          | 0/20 [00:01<?, ?it/s]
Task 7, Epoch 1/20 => Loss 1.639, Train_accy 33.33:   5%|▌         | 1/20 [00:01<00:25,  1.32s/it]
Task 7, Epoch 2/20 => Loss 0.858, Train_accy 65.12:   5%|▌         | 1/20 [00:02<00:25,  1.32s/it]
Task 7, Epoch 2/20 => Loss 0.858, Train_accy 65.12:  10%|█         | 2/20 [00:02<00:23,  1.32s/it]
Task 7, Epoch 3/20 => Loss 0.362, Train_accy 88.37:  10%|█         | 2/20 [00:03<00:23,  1.32s/it]
Task 7, Epoch 3/20 => Loss 0.362, Train_accy 88.37:  15%|█▌        | 3/20 [00:03<00:22,  1.32s/it]
Task 7, Epoch 4/20 => Loss 0.281, Train_accy 91.47:  15%|█▌        | 3/20 [00:05<00:22,  1.32s/it]
Task 7, Epoch 4/20 => Loss 0.281, Train_accy 91.47:  20%|██        | 4/20 [00:05<00:20,  1.31s/it]
Task 7, Epoch 5/20 => Loss 0.109, Train_accy 98.45:  20%|██        | 4/20 [00:06<00:20,  1.31s/it]
Task 7, Epoch 5/20 => Loss 0.109, Train_accy 98.45:  25%|██▌       | 5/20 [00:06<00:19,  1.32s/it]
Task 7, Epoch 6/20 => Loss 0.100, Train_accy 96.90:  25%|██▌       | 5/20 [00:07<00:19,  1.32s/it]
Task 7, Epoch 6/20 => Loss 0.100, Train_accy 96.90:  30%|███       | 6/20 [00:07<00:18,  1.31s/it]
Task 7, Epoch 7/20 => Loss 0.075, Train_accy 98.45:  30%|███       | 6/20 [00:09<00:18,  1.31s/it]
Task 7, Epoch 7/20 => Loss 0.075, Train_accy 98.45:  35%|███▌      | 7/20 [00:09<00:17,  1.31s/it]
Task 7, Epoch 8/20 => Loss 0.066, Train_accy 97.67:  35%|███▌      | 7/20 [00:10<00:17,  1.31s/it]
Task 7, Epoch 8/20 => Loss 0.066, Train_accy 97.67:  40%|████      | 8/20 [00:10<00:15,  1.32s/it]
Task 7, Epoch 9/20 => Loss 0.062, Train_accy 97.67:  40%|████      | 8/20 [00:11<00:15,  1.32s/it]
Task 7, Epoch 9/20 => Loss 0.062, Train_accy 97.67:  45%|████▌     | 9/20 [00:11<00:14,  1.32s/it]
Task 7, Epoch 10/20 => Loss 0.027, Train_accy 99.22:  45%|████▌     | 9/20 [00:13<00:14,  1.32s/it]
Task 7, Epoch 10/20 => Loss 0.027, Train_accy 99.22:  50%|█████     | 10/20 [00:13<00:13,  1.33s/it]
Task 7, Epoch 11/20 => Loss 0.069, Train_accy 99.22:  50%|█████     | 10/20 [00:14<00:13,  1.33s/it]
Task 7, Epoch 11/20 => Loss 0.069, Train_accy 99.22:  55%|█████▌    | 11/20 [00:14<00:12,  1.34s/it]
Task 7, Epoch 12/20 => Loss 0.061, Train_accy 98.45:  55%|█████▌    | 11/20 [00:15<00:12,  1.34s/it]
Task 7, Epoch 12/20 => Loss 0.061, Train_accy 98.45:  60%|██████    | 12/20 [00:15<00:10,  1.33s/it]
Task 7, Epoch 13/20 => Loss 0.053, Train_accy 98.45:  60%|██████    | 12/20 [00:17<00:10,  1.33s/it]
Task 7, Epoch 13/20 => Loss 0.053, Train_accy 98.45:  65%|██████▌   | 13/20 [00:17<00:09,  1.32s/it]
Task 7, Epoch 14/20 => Loss 0.055, Train_accy 98.45:  65%|██████▌   | 13/20 [00:18<00:09,  1.32s/it]
Task 7, Epoch 14/20 => Loss 0.055, Train_accy 98.45:  70%|███████   | 14/20 [00:18<00:07,  1.32s/it]
Task 7, Epoch 15/20 => Loss 0.039, Train_accy 98.45:  70%|███████   | 14/20 [00:19<00:07,  1.32s/it]
Task 7, Epoch 15/20 => Loss 0.039, Train_accy 98.45:  75%|███████▌  | 15/20 [00:19<00:06,  1.32s/it]
Task 7, Epoch 16/20 => Loss 0.061, Train_accy 98.45:  75%|███████▌  | 15/20 [00:21<00:06,  1.32s/it]
Task 7, Epoch 16/20 => Loss 0.061, Train_accy 98.45:  80%|████████  | 16/20 [00:21<00:05,  1.31s/it]
Task 7, Epoch 17/20 => Loss 0.103, Train_accy 98.45:  80%|████████  | 16/20 [00:22<00:05,  1.31s/it]
Task 7, Epoch 17/20 => Loss 0.103, Train_accy 98.45:  85%|████████▌ | 17/20 [00:22<00:03,  1.32s/it]
Task 7, Epoch 18/20 => Loss 0.047, Train_accy 98.45:  85%|████████▌ | 17/20 [00:23<00:03,  1.32s/it]
Task 7, Epoch 18/20 => Loss 0.047, Train_accy 98.45:  90%|█████████ | 18/20 [00:23<00:02,  1.32s/it]
Task 7, Epoch 19/20 => Loss 0.028, Train_accy 99.22:  90%|█████████ | 18/20 [00:25<00:02,  1.32s/it]
Task 7, Epoch 19/20 => Loss 0.028, Train_accy 99.22:  95%|█████████▌| 19/20 [00:25<00:01,  1.35s/it]
Task 7, Epoch 20/20 => Loss 0.019, Train_accy 100.00:  95%|█████████▌| 19/20 [00:26<00:01,  1.35s/it]
Task 7, Epoch 20/20 => Loss 0.019, Train_accy 100.00: 100%|██████████| 20/20 [00:26<00:00,  1.34s/it]
Task 7, Epoch 20/20 => Loss 0.019, Train_accy 100.00: 100%|██████████| 20/20 [00:26<00:00,  1.32s/it]
2024-08-12 02:48:45,649 [inflora.py] => Task 7, Epoch 20/20 => Loss 0.019, Train_accy 100.00
Threshold:  0.985
Skip Updating DualGPM for layer: 1
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 8/768 type remove
Layer 2 : 20/768 type remove
Layer 3 : 43/768 type remove
Layer 4 : 61/768 type remove
Layer 5 : 91/768 type remove
Layer 6 : 106/768 type remove
Layer 7 : 124/768 type remove
Layer 8 : 184/768 type remove
Layer 9 : 238/768 type remove
Layer 10 : 198/768 type remove
Layer 11 : 108/768 type remove
Layer 12 : 152/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:48:53,278 [trainer.py] => Time:38.36935305595398
5730 5730
5730 5730
2024-08-12 02:49:03,984 [trainer.py] => Time:10.706528902053833
2024-08-12 02:49:03,985 [inflora.py] => Exemplar size: 0
2024-08-12 02:49:03,985 [trainer.py] => CNN: {'total': 77.38, '00-04': 89.16, '05-09': 84.07, '10-14': 90.69, '15-19': 70.26, '20-24': 72.19, '25-29': 55.03, '30-34': 94.33, '35-39': 68.56, 'old': 78.11, 'new': 68.56}
2024-08-12 02:49:03,985 [trainer.py] => CNN top1 curve: [97.9, 95.76, 95.48, 88.58, 85.06, 82.19, 80.42, 77.38]
2024-08-12 02:49:03,985 [trainer.py] => CNN top1 with task curve: [97.9, 99.15, 99.37, 98.24, 98.24, 98.46, 98.47, 98.57]
2024-08-12 02:49:03,985 [trainer.py] => CNN top1 task curve: [1.0, 0.9661016949152542, 0.9582753824756607, 0.8935721812434141, 0.8537298878595807, 0.8247767857142857, 0.8058968058968059, 0.77521815008726]
2024-08-12 02:49:04,444 [trainer.py] => All params: 108905911
2024-08-12 02:49:04,446 [trainer.py] => Trainable params: 77573
2024-08-12 02:49:04,446 [inflora.py] => Learning on 40-45
Parameters to be updated: {'image_encoder.blocks.1.attn.lora_B_k.8.weight', 'image_encoder.blocks.11.attn.lora_B_v.8.weight', 'image_encoder.blocks.4.attn.lora_B_k.8.weight', 'image_encoder.blocks.8.attn.lora_B_v.8.weight', 'classifier_pool.8.weight', 'image_encoder.blocks.10.attn.lora_B_v.8.weight', 'image_encoder.blocks.2.attn.lora_B_v.8.weight', 'image_encoder.blocks.2.attn.lora_B_k.8.weight', 'image_encoder.blocks.9.attn.lora_B_k.8.weight', 'image_encoder.blocks.3.attn.lora_B_k.8.weight', 'image_encoder.blocks.0.attn.lora_B_k.8.weight', 'image_encoder.blocks.7.attn.lora_B_k.8.weight', 'image_encoder.blocks.4.attn.lora_B_v.8.weight', 'image_encoder.blocks.5.attn.lora_B_k.8.weight', 'image_encoder.blocks.6.attn.lora_B_k.8.weight', 'image_encoder.blocks.3.attn.lora_B_v.8.weight', 'image_encoder.blocks.11.attn.lora_B_k.8.weight', 'classifier_pool.8.bias', 'image_encoder.blocks.9.attn.lora_B_v.8.weight', 'image_encoder.blocks.5.attn.lora_B_v.8.weight', 'image_encoder.blocks.8.attn.lora_B_k.8.weight', 'image_encoder.blocks.1.attn.lora_B_v.8.weight', 'image_encoder.blocks.6.attn.lora_B_v.8.weight', 'image_encoder.blocks.7.attn.lora_B_v.8.weight', 'image_encoder.blocks.10.attn.lora_B_k.8.weight', 'image_encoder.blocks.0.attn.lora_B_v.8.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 8, Epoch 1/20 => Loss 1.444, Train_accy 41.58:   0%|          | 0/20 [00:01<?, ?it/s]
Task 8, Epoch 1/20 => Loss 1.444, Train_accy 41.58:   5%|▌         | 1/20 [00:01<00:36,  1.94s/it]
Task 8, Epoch 2/20 => Loss 0.535, Train_accy 80.41:   5%|▌         | 1/20 [00:03<00:36,  1.94s/it]
Task 8, Epoch 2/20 => Loss 0.535, Train_accy 80.41:  10%|█         | 2/20 [00:03<00:34,  1.93s/it]
Task 8, Epoch 3/20 => Loss 0.289, Train_accy 91.75:  10%|█         | 2/20 [00:05<00:34,  1.93s/it]
Task 8, Epoch 3/20 => Loss 0.289, Train_accy 91.75:  15%|█▌        | 3/20 [00:05<00:32,  1.90s/it]
Task 8, Epoch 4/20 => Loss 0.150, Train_accy 95.88:  15%|█▌        | 3/20 [00:07<00:32,  1.90s/it]
Task 8, Epoch 4/20 => Loss 0.150, Train_accy 95.88:  20%|██        | 4/20 [00:07<00:30,  1.89s/it]
Task 8, Epoch 5/20 => Loss 0.107, Train_accy 97.59:  20%|██        | 4/20 [00:09<00:30,  1.89s/it]
Task 8, Epoch 5/20 => Loss 0.107, Train_accy 97.59:  25%|██▌       | 5/20 [00:09<00:28,  1.90s/it]
Task 8, Epoch 6/20 => Loss 0.173, Train_accy 96.22:  25%|██▌       | 5/20 [00:11<00:28,  1.90s/it]
Task 8, Epoch 6/20 => Loss 0.173, Train_accy 96.22:  30%|███       | 6/20 [00:11<00:26,  1.90s/it]
Task 8, Epoch 7/20 => Loss 0.058, Train_accy 97.94:  30%|███       | 6/20 [00:13<00:26,  1.90s/it]
Task 8, Epoch 7/20 => Loss 0.058, Train_accy 97.94:  35%|███▌      | 7/20 [00:13<00:24,  1.92s/it]
Task 8, Epoch 8/20 => Loss 0.173, Train_accy 97.94:  35%|███▌      | 7/20 [00:15<00:24,  1.92s/it]
Task 8, Epoch 8/20 => Loss 0.173, Train_accy 97.94:  40%|████      | 8/20 [00:15<00:22,  1.91s/it]
Task 8, Epoch 9/20 => Loss 0.159, Train_accy 98.28:  40%|████      | 8/20 [00:17<00:22,  1.91s/it]
Task 8, Epoch 9/20 => Loss 0.159, Train_accy 98.28:  45%|████▌     | 9/20 [00:17<00:21,  1.91s/it]
Task 8, Epoch 10/20 => Loss 0.097, Train_accy 96.56:  45%|████▌     | 9/20 [00:19<00:21,  1.91s/it]
Task 8, Epoch 10/20 => Loss 0.097, Train_accy 96.56:  50%|█████     | 10/20 [00:19<00:19,  1.90s/it]
Task 8, Epoch 11/20 => Loss 0.084, Train_accy 97.59:  50%|█████     | 10/20 [00:20<00:19,  1.90s/it]
Task 8, Epoch 11/20 => Loss 0.084, Train_accy 97.59:  55%|█████▌    | 11/20 [00:20<00:17,  1.90s/it]
Task 8, Epoch 12/20 => Loss 0.067, Train_accy 97.25:  55%|█████▌    | 11/20 [00:22<00:17,  1.90s/it]
Task 8, Epoch 12/20 => Loss 0.067, Train_accy 97.25:  60%|██████    | 12/20 [00:22<00:15,  1.90s/it]
Task 8, Epoch 13/20 => Loss 0.053, Train_accy 99.31:  60%|██████    | 12/20 [00:24<00:15,  1.90s/it]
Task 8, Epoch 13/20 => Loss 0.053, Train_accy 99.31:  65%|██████▌   | 13/20 [00:24<00:13,  1.90s/it]
Task 8, Epoch 14/20 => Loss 0.057, Train_accy 98.63:  65%|██████▌   | 13/20 [00:26<00:13,  1.90s/it]
Task 8, Epoch 14/20 => Loss 0.057, Train_accy 98.63:  70%|███████   | 14/20 [00:26<00:11,  1.91s/it]
Task 8, Epoch 15/20 => Loss 0.045, Train_accy 98.28:  70%|███████   | 14/20 [00:28<00:11,  1.91s/it]
Task 8, Epoch 15/20 => Loss 0.045, Train_accy 98.28:  75%|███████▌  | 15/20 [00:28<00:09,  1.92s/it]
Task 8, Epoch 16/20 => Loss 0.032, Train_accy 99.31:  75%|███████▌  | 15/20 [00:30<00:09,  1.92s/it]
Task 8, Epoch 16/20 => Loss 0.032, Train_accy 99.31:  80%|████████  | 16/20 [00:30<00:07,  1.92s/it]
Task 8, Epoch 17/20 => Loss 0.024, Train_accy 99.31:  80%|████████  | 16/20 [00:32<00:07,  1.92s/it]
Task 8, Epoch 17/20 => Loss 0.024, Train_accy 99.31:  85%|████████▌ | 17/20 [00:32<00:05,  1.92s/it]
Task 8, Epoch 18/20 => Loss 0.047, Train_accy 99.31:  85%|████████▌ | 17/20 [00:34<00:05,  1.92s/it]
Task 8, Epoch 18/20 => Loss 0.047, Train_accy 99.31:  90%|█████████ | 18/20 [00:34<00:03,  1.93s/it]
Task 8, Epoch 19/20 => Loss 0.047, Train_accy 97.94:  90%|█████████ | 18/20 [00:36<00:03,  1.93s/it]
Task 8, Epoch 19/20 => Loss 0.047, Train_accy 97.94:  95%|█████████▌| 19/20 [00:36<00:01,  1.92s/it]
Task 8, Epoch 20/20 => Loss 0.049, Train_accy 98.28:  95%|█████████▌| 19/20 [00:38<00:01,  1.92s/it]
Task 8, Epoch 20/20 => Loss 0.049, Train_accy 98.28: 100%|██████████| 20/20 [00:38<00:00,  1.93s/it]
Task 8, Epoch 20/20 => Loss 0.049, Train_accy 98.28: 100%|██████████| 20/20 [00:38<00:00,  1.91s/it]
2024-08-12 02:49:46,954 [inflora.py] => Task 8, Epoch 20/20 => Loss 0.049, Train_accy 98.28
Threshold:  0.99
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 9/768 type remove
Layer 2 : 21/768 type remove
Layer 3 : 44/768 type remove
Layer 4 : 65/768 type remove
Layer 5 : 95/768 type remove
Layer 6 : 111/768 type remove
Layer 7 : 133/768 type remove
Layer 8 : 190/768 type remove
Layer 9 : 241/768 type remove
Layer 10 : 202/768 type remove
Layer 11 : 112/768 type remove
Layer 12 : 168/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:49:55,011 [trainer.py] => Time:50.56412363052368
7251 7251
7251 7251
2024-08-12 02:50:08,239 [trainer.py] => Time:13.227879524230957
2024-08-12 02:50:08,239 [inflora.py] => Exemplar size: 0
2024-08-12 02:50:08,240 [trainer.py] => CNN: {'total': 75.16, '00-04': 88.81, '05-09': 83.89, '10-14': 88.56, '15-19': 69.84, '20-24': 73.39, '25-29': 54.5, '30-34': 92.48, '35-39': 69.93, '40-44': 67.92, 'old': 77.09, 'new': 67.92}
2024-08-12 02:50:08,240 [trainer.py] => CNN top1 curve: [97.9, 95.76, 95.48, 88.58, 85.06, 82.19, 80.42, 77.38, 75.16]
2024-08-12 02:50:08,240 [trainer.py] => CNN top1 with task curve: [97.9, 99.15, 99.37, 98.24, 98.24, 98.46, 98.47, 98.57, 98.39]
2024-08-12 02:50:08,240 [trainer.py] => CNN top1 task curve: [1.0, 0.9661016949152542, 0.9582753824756607, 0.8935721812434141, 0.8537298878595807, 0.8247767857142857, 0.8058968058968059, 0.77521815008726, 0.7531374982760999]
2024-08-12 02:50:08,709 [trainer.py] => All params: 108905911
2024-08-12 02:50:08,711 [trainer.py] => Trainable params: 77573
2024-08-12 02:50:08,711 [inflora.py] => Learning on 45-50
Parameters to be updated: {'image_encoder.blocks.7.attn.lora_B_k.9.weight', 'image_encoder.blocks.10.attn.lora_B_k.9.weight', 'image_encoder.blocks.8.attn.lora_B_v.9.weight', 'image_encoder.blocks.4.attn.lora_B_k.9.weight', 'image_encoder.blocks.6.attn.lora_B_v.9.weight', 'image_encoder.blocks.3.attn.lora_B_v.9.weight', 'image_encoder.blocks.7.attn.lora_B_v.9.weight', 'classifier_pool.9.weight', 'image_encoder.blocks.6.attn.lora_B_k.9.weight', 'image_encoder.blocks.1.attn.lora_B_k.9.weight', 'image_encoder.blocks.9.attn.lora_B_k.9.weight', 'image_encoder.blocks.0.attn.lora_B_k.9.weight', 'image_encoder.blocks.2.attn.lora_B_v.9.weight', 'image_encoder.blocks.1.attn.lora_B_v.9.weight', 'image_encoder.blocks.9.attn.lora_B_v.9.weight', 'classifier_pool.9.bias', 'image_encoder.blocks.5.attn.lora_B_v.9.weight', 'image_encoder.blocks.4.attn.lora_B_v.9.weight', 'image_encoder.blocks.5.attn.lora_B_k.9.weight', 'image_encoder.blocks.0.attn.lora_B_v.9.weight', 'image_encoder.blocks.11.attn.lora_B_v.9.weight', 'image_encoder.blocks.2.attn.lora_B_k.9.weight', 'image_encoder.blocks.3.attn.lora_B_k.9.weight', 'image_encoder.blocks.8.attn.lora_B_k.9.weight', 'image_encoder.blocks.10.attn.lora_B_v.9.weight', 'image_encoder.blocks.11.attn.lora_B_k.9.weight'}

  0%|          | 0/20 [00:00<?, ?it/s]
Task 9, Epoch 1/20 => Loss 1.265, Train_accy 54.96:   0%|          | 0/20 [00:01<?, ?it/s]
Task 9, Epoch 1/20 => Loss 1.265, Train_accy 54.96:   5%|▌         | 1/20 [00:01<00:34,  1.80s/it]
Task 9, Epoch 2/20 => Loss 0.279, Train_accy 91.22:   5%|▌         | 1/20 [00:03<00:34,  1.80s/it]
Task 9, Epoch 2/20 => Loss 0.279, Train_accy 91.22:  10%|█         | 2/20 [00:03<00:32,  1.81s/it]
Task 9, Epoch 3/20 => Loss 0.129, Train_accy 96.18:  10%|█         | 2/20 [00:05<00:32,  1.81s/it]
Task 9, Epoch 3/20 => Loss 0.129, Train_accy 96.18:  15%|█▌        | 3/20 [00:05<00:30,  1.81s/it]
Task 9, Epoch 4/20 => Loss 0.065, Train_accy 98.85:  15%|█▌        | 3/20 [00:07<00:30,  1.81s/it]
Task 9, Epoch 4/20 => Loss 0.065, Train_accy 98.85:  20%|██        | 4/20 [00:07<00:28,  1.81s/it]
Task 9, Epoch 5/20 => Loss 0.060, Train_accy 98.09:  20%|██        | 4/20 [00:09<00:28,  1.81s/it]
Task 9, Epoch 5/20 => Loss 0.060, Train_accy 98.09:  25%|██▌       | 5/20 [00:09<00:27,  1.80s/it]
Task 9, Epoch 6/20 => Loss 0.040, Train_accy 98.85:  25%|██▌       | 5/20 [00:10<00:27,  1.80s/it]
Task 9, Epoch 6/20 => Loss 0.040, Train_accy 98.85:  30%|███       | 6/20 [00:10<00:25,  1.82s/it]
Task 9, Epoch 7/20 => Loss 0.034, Train_accy 99.24:  30%|███       | 6/20 [00:12<00:25,  1.82s/it]
Task 9, Epoch 7/20 => Loss 0.034, Train_accy 99.24:  35%|███▌      | 7/20 [00:12<00:23,  1.80s/it]
Task 9, Epoch 8/20 => Loss 0.042, Train_accy 98.47:  35%|███▌      | 7/20 [00:14<00:23,  1.80s/it]
Task 9, Epoch 8/20 => Loss 0.042, Train_accy 98.47:  40%|████      | 8/20 [00:14<00:21,  1.81s/it]
Task 9, Epoch 9/20 => Loss 0.025, Train_accy 99.62:  40%|████      | 8/20 [00:16<00:21,  1.81s/it]
Task 9, Epoch 9/20 => Loss 0.025, Train_accy 99.62:  45%|████▌     | 9/20 [00:16<00:19,  1.80s/it]
Task 9, Epoch 10/20 => Loss 0.026, Train_accy 99.62:  45%|████▌     | 9/20 [00:18<00:19,  1.80s/it]
Task 9, Epoch 10/20 => Loss 0.026, Train_accy 99.62:  50%|█████     | 10/20 [00:18<00:17,  1.80s/it]
Task 9, Epoch 11/20 => Loss 0.020, Train_accy 99.24:  50%|█████     | 10/20 [00:19<00:17,  1.80s/it]
Task 9, Epoch 11/20 => Loss 0.020, Train_accy 99.24:  55%|█████▌    | 11/20 [00:19<00:16,  1.79s/it]
Task 9, Epoch 12/20 => Loss 0.061, Train_accy 99.24:  55%|█████▌    | 11/20 [00:21<00:16,  1.79s/it]
Task 9, Epoch 12/20 => Loss 0.061, Train_accy 99.24:  60%|██████    | 12/20 [00:21<00:14,  1.80s/it]
Task 9, Epoch 13/20 => Loss 0.044, Train_accy 98.47:  60%|██████    | 12/20 [00:23<00:14,  1.80s/it]
Task 9, Epoch 13/20 => Loss 0.044, Train_accy 98.47:  65%|██████▌   | 13/20 [00:23<00:12,  1.79s/it]
Task 9, Epoch 14/20 => Loss 0.057, Train_accy 97.71:  65%|██████▌   | 13/20 [00:25<00:12,  1.79s/it]
Task 9, Epoch 14/20 => Loss 0.057, Train_accy 97.71:  70%|███████   | 14/20 [00:25<00:10,  1.80s/it]
Task 9, Epoch 15/20 => Loss 0.034, Train_accy 99.24:  70%|███████   | 14/20 [00:27<00:10,  1.80s/it]
Task 9, Epoch 15/20 => Loss 0.034, Train_accy 99.24:  75%|███████▌  | 15/20 [00:27<00:08,  1.80s/it]
Task 9, Epoch 16/20 => Loss 0.021, Train_accy 99.62:  75%|███████▌  | 15/20 [00:28<00:08,  1.80s/it]
Task 9, Epoch 16/20 => Loss 0.021, Train_accy 99.62:  80%|████████  | 16/20 [00:28<00:07,  1.81s/it]
Task 9, Epoch 17/20 => Loss 0.024, Train_accy 99.24:  80%|████████  | 16/20 [00:30<00:07,  1.81s/it]
Task 9, Epoch 17/20 => Loss 0.024, Train_accy 99.24:  85%|████████▌ | 17/20 [00:30<00:05,  1.79s/it]
Task 9, Epoch 18/20 => Loss 0.033, Train_accy 98.85:  85%|████████▌ | 17/20 [00:32<00:05,  1.79s/it]
Task 9, Epoch 18/20 => Loss 0.033, Train_accy 98.85:  90%|█████████ | 18/20 [00:32<00:03,  1.78s/it]
Task 9, Epoch 19/20 => Loss 0.067, Train_accy 98.47:  90%|█████████ | 18/20 [00:34<00:03,  1.78s/it]
Task 9, Epoch 19/20 => Loss 0.067, Train_accy 98.47:  95%|█████████▌| 19/20 [00:34<00:01,  1.78s/it]
Task 9, Epoch 20/20 => Loss 0.040, Train_accy 98.47:  95%|█████████▌| 19/20 [00:35<00:01,  1.78s/it]
Task 9, Epoch 20/20 => Loss 0.040, Train_accy 98.47: 100%|██████████| 20/20 [00:35<00:00,  1.78s/it]
Task 9, Epoch 20/20 => Loss 0.040, Train_accy 98.47: 100%|██████████| 20/20 [00:35<00:00,  1.80s/it]
2024-08-12 02:50:48,596 [inflora.py] => Task 9, Epoch 20/20 => Loss 0.040, Train_accy 98.47
Threshold:  0.995
----------------------------------------
Gradient Constraints Summary
----------------------------------------
Layer 1 : 10/768 type remove
Layer 2 : 22/768 type remove
Layer 3 : 52/768 type remove
Layer 4 : 81/768 type remove
Layer 5 : 113/768 type remove
Layer 6 : 133/768 type remove
Layer 7 : 171/768 type remove
Layer 8 : 227/768 type remove
Layer 9 : 257/768 type remove
Layer 10 : 217/768 type remove
Layer 11 : 129/768 type remove
Layer 12 : 215/768 type remove
----------------------------------------
Layer 1 - Projection Matrix shape: torch.Size([768, 768])
Layer 2 - Projection Matrix shape: torch.Size([768, 768])
Layer 3 - Projection Matrix shape: torch.Size([768, 768])
Layer 4 - Projection Matrix shape: torch.Size([768, 768])
Layer 5 - Projection Matrix shape: torch.Size([768, 768])
Layer 6 - Projection Matrix shape: torch.Size([768, 768])
Layer 7 - Projection Matrix shape: torch.Size([768, 768])
Layer 8 - Projection Matrix shape: torch.Size([768, 768])
Layer 9 - Projection Matrix shape: torch.Size([768, 768])
Layer 10 - Projection Matrix shape: torch.Size([768, 768])
Layer 11 - Projection Matrix shape: torch.Size([768, 768])
Layer 12 - Projection Matrix shape: torch.Size([768, 768])
2024-08-12 02:50:56,822 [trainer.py] => Time:48.11085867881775
8619 8619
8619 8619
2024-08-12 02:51:12,072 [trainer.py] => Time:15.250428915023804
2024-08-12 02:51:12,073 [inflora.py] => Exemplar size: 0
2024-08-12 02:51:12,073 [trainer.py] => CNN: {'total': 71.06, '00-04': 88.46, '05-09': 83.15, '10-14': 83.99, '15-19': 57.35, '20-24': 70.76, '25-29': 53.97, '30-34': 91.62, '35-39': 54.44, '40-44': 69.56, '45-49': 70.83, 'old': 71.11, 'new': 70.83}
2024-08-12 02:51:12,073 [trainer.py] => CNN top1 curve: [97.9, 95.76, 95.48, 88.58, 85.06, 82.19, 80.42, 77.38, 75.16, 71.06]
2024-08-12 02:51:12,073 [trainer.py] => CNN top1 with task curve: [97.9, 99.15, 99.37, 98.24, 98.24, 98.46, 98.47, 98.57, 98.39, 98.45]
2024-08-12 02:51:12,073 [trainer.py] => CNN top1 task curve: [1.0, 0.9661016949152542, 0.9582753824756607, 0.8935721812434141, 0.8537298878595807, 0.8247767857142857, 0.8058968058968059, 0.77521815008726, 0.7531374982760999, 0.7111033762617474]
Traceback (most recent call last):
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/main.py", line 33, in <module>
    main()
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/main.py", line 11, in main
    train(args)
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/trainer.py", line 28, in train
    _set_random(args["seed"])
  File "/mnt/mydisk/ruoheng.li/lrh/Code/Research/CIL/InfLoRA-main/trainer.py", line 101, in _set_random
    torch.manual_seed(args['seed'])
TypeError: 'int' object is not subscriptable
